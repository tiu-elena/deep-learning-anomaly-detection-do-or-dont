{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "123c1709-aadd-4c93-a80a-0a0c44290983",
   "metadata": {},
   "source": [
    "# AnoGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "2520ef24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e4854b-87bd-490e-88e4-9a64631dc5bb",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Libraries import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "feba20a1-5d20-4127-888b-b0e4c72f4757",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report, precision_recall_curve\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import  OneHotEncoder\n",
    "import time\n",
    "\n",
    "import torchvision.utils as vutils\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9814f465",
   "metadata": {},
   "source": [
    "## Train dataset loader for AnoGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "0dfde448",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTrainDataset(Dataset):\n",
    " \n",
    "      def __init__(self,file):\n",
    "            price_df=file\n",
    " \n",
    "            x=file.values\n",
    "            y=torch.ones(len(x),1)\n",
    " \n",
    "            self.x_train=torch.tensor(x,dtype=torch.float32)\n",
    "            self.y_train=y\n",
    " \n",
    "      def __len__(self):\n",
    "            return len(self.y_train)\n",
    "   \n",
    "      def __getitem__(self,idx):\n",
    "            return self.x_train[idx],self.y_train[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9bd3341",
   "metadata": {},
   "source": [
    "## Test dataset loader for AnoGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "9c886630",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTestDataset(Dataset):\n",
    " \n",
    "      def __init__(self,file):\n",
    "            price_df=file\n",
    " \n",
    "            x=file[file.columns[:-1]].values\n",
    "            y=file[file.columns[-1:]].values\n",
    " \n",
    "            self.x_train=torch.tensor(x,dtype=torch.float32)\n",
    "            self.y_train=torch.tensor(y,dtype=torch.float32)\n",
    " \n",
    " \n",
    "      def __len__(self):\n",
    "            return len(self.y_train)\n",
    "   \n",
    "      def __getitem__(self,idx):\n",
    "            return self.x_train[idx],self.y_train[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75ff7c4",
   "metadata": {},
   "source": [
    "## AnoGAN Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "5805334a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator Code\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "                        nn.Linear(in_features = input_dim, out_features = 512), \n",
    "                        nn.modules.BatchNorm1d(512),\n",
    "                        nn.LeakyReLU()\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "                        nn.Linear(in_features = 512,out_features = 256),  \n",
    "                        nn.modules.BatchNorm1d(256),\n",
    "                        nn.LeakyReLU(),\n",
    "        )\n",
    "        self.layer3 = nn.Sequential(\n",
    "                        nn.Linear(in_features = 256,out_features = 128), \n",
    "                        nn.modules.BatchNorm1d(128),\n",
    "                        nn.LeakyReLU(),\n",
    "        )\n",
    "        self.layer4 = nn.Sequential(\n",
    "                        nn.Linear(in_features = 128,out_features = input_dim),\n",
    "                        nn.Tanh()\n",
    "        )\n",
    "        \n",
    "       \n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        h0 = self.layer1(input)\n",
    "        h1 = self.layer2(h0)\n",
    "        h2 = self.layer3(h1)\n",
    "        h3 = self.layer4(h2)\n",
    "        out = h3\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624825aa",
   "metadata": {},
   "source": [
    "## AnoGAN Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "a8bb2e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "                    nn.Linear(in_features = input_dim, out_features = 512),  \n",
    "                    nn.modules.BatchNorm1d(512),\n",
    "                    nn.LeakyReLU()\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "                    nn.Linear(in_features = 512,out_features = 256),  \n",
    "                    nn.modules.BatchNorm1d(256),\n",
    "                    nn.LeakyReLU()\n",
    "        )\n",
    "        self.layer3 = nn.Sequential(\n",
    "                    nn.Linear(in_features = 256,out_features = 128),\n",
    "                    nn.modules.BatchNorm1d(128),\n",
    "                    nn.LeakyReLU()\n",
    "        )\n",
    "        self.layer4 = nn.Sequential(\n",
    "                    nn.Linear(in_features = 128,out_features = 1),\n",
    "                    nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean=0.0, std=1.0)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "    \n",
    "        \n",
    "\n",
    "    def forward(self, input):\n",
    "        h0 = self.layer1(input)\n",
    "        h1 = self.layer2(h0)\n",
    "        h2 = self.layer3(h1)\n",
    "        feature = h2\n",
    "        h3 = self.layer4(h2)\n",
    "        out = h3\n",
    "        return out, feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b30c5c",
   "metadata": {},
   "source": [
    "## Anomaly score for AnoGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "9184dce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Anomaly_score(x,G_z,Lambda=0.1):\n",
    "    discriminator.eval()\n",
    "    _,x_feature = discriminator(x)\n",
    "    x_feature = x_feature.view(-1)\n",
    "    _,G_z_feature = discriminator(G_z)\n",
    "    \n",
    "    residual_loss = torch.sum(torch.abs(x-G_z))\n",
    "    discrimination_loss = torch.sum(torch.abs(x_feature-G_z_feature))\n",
    "    \n",
    "    total_loss = (1-Lambda)*residual_loss + Lambda*discrimination_loss\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7db27ee-405e-4faa-a7e9-6b8cdf41d437",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Arrhythmia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e215f74",
   "metadata": {},
   "source": [
    "**Dataset source**: http://odds.cs.stonybrook.edu/arrhythmia-dataset/ (data is transformed from .mat to .csv format)\n",
    "\n",
    "Shebuti Rayana (2016). ODDS Library [http://odds.cs.stonybrook.edu]. Stony Brook, NY: Stony Brook University, Department of Computer Science.\n",
    "\n",
    "**Additional sources**:\n",
    "\n",
    "Liu, Fei Tony, Kai Ming Ting, and Zhi-Hua Zhou. “Isolation forest.” 2008 Eighth IEEE International Conference on Data Mining. IEEE, 2008.\n",
    "\n",
    "K. M. Ting, J. T. S. Chuan, and F. T. Liu. “Mass: A New Ranking Measure for Anomaly Detection.“, IEEE Transactions on Knowledge and Data Engineering, 2009.\n",
    "\n",
    "F. Keller, E. Muller, K. Bohm.“HiCS: High-contrast subspaces for density-based outlier ranking.” ICDE, 2012."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "a433c409-f51f-402b-8581-759c81670d7b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('./arrhythmia.csv', sep = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "9f34ed3a-f570-4f95-b4f0-d1dfed0f46e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping columns that consist only of 0's\n",
    "data = data.drop(columns = ['Col15', 'Col63', 'Col65', 'Col79', 'Col127', 'Col128','Col135', 'Col137', 'Col139','Col141',\n",
    "'Col147', 'Col152', 'Col153','Col160','Col200', 'Col260', 'Col270'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "a2922c1c-284f-420b-954d-ecf924697f54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Col1</th>\n",
       "      <th>Col2</th>\n",
       "      <th>Col3</th>\n",
       "      <th>Col4</th>\n",
       "      <th>Col5</th>\n",
       "      <th>Col6</th>\n",
       "      <th>Col7</th>\n",
       "      <th>Col8</th>\n",
       "      <th>Col9</th>\n",
       "      <th>Col10</th>\n",
       "      <th>...</th>\n",
       "      <th>Col265</th>\n",
       "      <th>Col266</th>\n",
       "      <th>Col267</th>\n",
       "      <th>Col268</th>\n",
       "      <th>Col269</th>\n",
       "      <th>Col271</th>\n",
       "      <th>Col272</th>\n",
       "      <th>Col273</th>\n",
       "      <th>Col274</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>75.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>193.0</td>\n",
       "      <td>371.0</td>\n",
       "      <td>174.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>-16.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>-0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>2.9</td>\n",
       "      <td>23.3</td>\n",
       "      <td>49.4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>56.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>174.0</td>\n",
       "      <td>401.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2.1</td>\n",
       "      <td>20.4</td>\n",
       "      <td>38.8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>54.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>172.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>138.0</td>\n",
       "      <td>163.0</td>\n",
       "      <td>386.0</td>\n",
       "      <td>185.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.5</td>\n",
       "      <td>-2.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>3.4</td>\n",
       "      <td>12.3</td>\n",
       "      <td>49.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>55.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>175.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>202.0</td>\n",
       "      <td>380.0</td>\n",
       "      <td>179.0</td>\n",
       "      <td>143.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.2</td>\n",
       "      <td>-2.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2.6</td>\n",
       "      <td>34.6</td>\n",
       "      <td>61.6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>75.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>181.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>-16.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.1</td>\n",
       "      <td>-3.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>3.9</td>\n",
       "      <td>25.4</td>\n",
       "      <td>62.8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 258 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Col1  Col2   Col3  Col4   Col5   Col6   Col7   Col8   Col9  Col10  ...  \\\n",
       "0  75.0   0.0  190.0  80.0   91.0  193.0  371.0  174.0  121.0  -16.0  ...   \n",
       "1  56.0   1.0  165.0  64.0   81.0  174.0  401.0  149.0   39.0   25.0  ...   \n",
       "2  54.0   0.0  172.0  95.0  138.0  163.0  386.0  185.0  102.0   96.0  ...   \n",
       "3  55.0   0.0  175.0  94.0  100.0  202.0  380.0  179.0  143.0   28.0  ...   \n",
       "4  75.0   0.0  190.0  80.0   88.0  181.0  360.0  177.0  103.0  -16.0  ...   \n",
       "\n",
       "   Col265  Col266  Col267  Col268  Col269  Col271  Col272  Col273  Col274  y  \n",
       "0    -0.3     0.0     9.0    -0.9     0.0     0.9     2.9    23.3    49.4  1  \n",
       "1    -0.5     0.0     8.5     0.0     0.0     0.2     2.1    20.4    38.8  0  \n",
       "2     0.9     0.0     9.5    -2.4     0.0     0.3     3.4    12.3    49.0  0  \n",
       "3     0.1     0.0    12.2    -2.2     0.0     0.4     2.6    34.6    61.6  0  \n",
       "4    -0.4     0.0    13.1    -3.6     0.0    -0.1     3.9    25.4    62.8  1  \n",
       "\n",
       "[5 rows x 258 columns]"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "7a470d6c-c28d-4798-aec0-2361530d8bf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(452, 258)"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "937686a7-6d97-4ed0-890a-4575bd4403f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Col1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Col1\n",
       "y      \n",
       "0   386\n",
       "1    66"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.pivot_table(data,\n",
    "             values = 'Col1',\n",
    "               index = 'y', \n",
    "              aggfunc = 'count')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3071e4fe",
   "metadata": {},
   "source": [
    "## AnoGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "b51d7e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "batch_size = 64\n",
    "learning_rate = 0.0002\n",
    "input_dim = data.shape[1]-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "5b90f251",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    x_train,\n",
    "    x_test,\n",
    "    y_train,\n",
    "    y_test,\n",
    "    indices_train,\n",
    "    indices_test,\n",
    ") = train_test_split(data[data.columns[:-1]].copy(), data['y'].copy(), data.copy().index, test_size=0.3,stratify=data[['y']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "041c4e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = pd.DataFrame(y_train)\n",
    "x_train = x_train.loc[y_train[y_train.y == 0].index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "1196e8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "myDs=MyTrainDataset(x_train)\n",
    "train_loader=torch.utils.data.DataLoader(myDs,batch_size=batch_size,shuffle=False, drop_last = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "a1b2da76",
   "metadata": {},
   "outputs": [],
   "source": [
    "netG = Generator()\n",
    "netD = Discriminator()\n",
    "\n",
    "# Initialize BCELoss function\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Establish convention for real and fake labels during training\n",
    "real_label = 1.\n",
    "fake_label = 0.\n",
    "\n",
    "# Setup Adam optimizers for both G and D\n",
    "optimizerD = torch.optim.Adam(netD.parameters(), lr=learning_rate, betas=(0.5, 0.999))\n",
    "optimizerG = torch.optim.Adam(netG.parameters(), lr=learning_rate, betas=(0.5, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "be5485e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training Loop...\n",
      "0\n",
      "[0/20][0/4]\tLoss_D: 20.5923\tLoss_G: 0.0117\tD(x): 0.9631\tD(G(z)): 0.9898 / 0.9890\n",
      "1\n",
      "[1/20][0/4]\tLoss_D: 23.5759\tLoss_G: 0.0861\tD(x): 0.9639\tD(G(z)): 0.9657 / 0.9647\n",
      "2\n",
      "[2/20][0/4]\tLoss_D: 23.2540\tLoss_G: 0.0167\tD(x): 0.9641\tD(G(z)): 0.9862 / 0.9855\n",
      "3\n",
      "[3/20][0/4]\tLoss_D: 24.5077\tLoss_G: 0.0177\tD(x): 0.9645\tD(G(z)): 0.9866 / 0.9863\n",
      "4\n",
      "[4/20][0/4]\tLoss_D: 21.5883\tLoss_G: 0.0899\tD(x): 0.9647\tD(G(z)): 0.9558 / 0.9556\n",
      "5\n",
      "[5/20][0/4]\tLoss_D: 15.4044\tLoss_G: 0.0713\tD(x): 0.9651\tD(G(z)): 0.9542 / 0.9545\n",
      "6\n",
      "[6/20][0/4]\tLoss_D: 22.8782\tLoss_G: 0.0523\tD(x): 0.9655\tD(G(z)): 0.9722 / 0.9716\n",
      "7\n",
      "[7/20][0/4]\tLoss_D: 22.6653\tLoss_G: 0.0395\tD(x): 0.9658\tD(G(z)): 0.9723 / 0.9718\n",
      "8\n",
      "[8/20][0/4]\tLoss_D: 17.3109\tLoss_G: 0.0225\tD(x): 0.9660\tD(G(z)): 0.9821 / 0.9816\n",
      "9\n",
      "[9/20][0/4]\tLoss_D: 18.2154\tLoss_G: 0.0450\tD(x): 0.9664\tD(G(z)): 0.9678 / 0.9670\n",
      "10\n",
      "[10/20][0/4]\tLoss_D: 16.9895\tLoss_G: 0.0460\tD(x): 0.9664\tD(G(z)): 0.9664 / 0.9656\n",
      "11\n",
      "[11/20][0/4]\tLoss_D: 17.3723\tLoss_G: 0.1419\tD(x): 0.9665\tD(G(z)): 0.9549 / 0.9545\n",
      "12\n",
      "[12/20][0/4]\tLoss_D: 16.9558\tLoss_G: 0.0429\tD(x): 0.9664\tD(G(z)): 0.9703 / 0.9697\n",
      "13\n",
      "[13/20][0/4]\tLoss_D: 13.1571\tLoss_G: 0.0330\tD(x): 0.9661\tD(G(z)): 0.9751 / 0.9750\n",
      "14\n",
      "[14/20][0/4]\tLoss_D: 16.9604\tLoss_G: 0.0409\tD(x): 0.9657\tD(G(z)): 0.9772 / 0.9767\n",
      "15\n",
      "[15/20][0/4]\tLoss_D: 14.2352\tLoss_G: 0.0783\tD(x): 0.9652\tD(G(z)): 0.9663 / 0.9660\n",
      "16\n",
      "[16/20][0/4]\tLoss_D: 11.5227\tLoss_G: 0.0492\tD(x): 0.9647\tD(G(z)): 0.9699 / 0.9694\n",
      "17\n",
      "[17/20][0/4]\tLoss_D: 15.0067\tLoss_G: 0.0187\tD(x): 0.9638\tD(G(z)): 0.9839 / 0.9829\n",
      "18\n",
      "[18/20][0/4]\tLoss_D: 11.3832\tLoss_G: 0.0851\tD(x): 0.9625\tD(G(z)): 0.9653 / 0.9649\n",
      "19\n",
      "[19/20][0/4]\tLoss_D: 15.0991\tLoss_G: 0.0410\tD(x): 0.9613\tD(G(z)): 0.9760 / 0.9755\n",
      "18.5\n"
     ]
    }
   ],
   "source": [
    "G_losses = []\n",
    "D_losses = []\n",
    "iters = 0\n",
    "\n",
    "start = time.process_time()\n",
    "print(\"Starting Training Loop...\")\n",
    "# For each epoch\n",
    "for epoch in range(num_epochs):\n",
    "    print(epoch)\n",
    "    # For each batch in the dataloader\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "\n",
    "        ############################\n",
    "        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "        ###########################\n",
    "        ## Train with all-real batch\n",
    "        netD.zero_grad()\n",
    "        \n",
    "        real_cpu = data[0]\n",
    "        #print(real_cpu)\n",
    "        b_size = real_cpu.size(0)\n",
    "        # Format batch\n",
    "        label = torch.full((b_size,), real_label, dtype=torch.float)\n",
    "        # Forward pass real batch through D\n",
    "        output, _ = netD(real_cpu)\n",
    "        output = output.view(-1)\n",
    "        # Calculate loss on all-real batch\n",
    "        errD_real = criterion(output, label)\n",
    "        # Calculate gradients for D in backward pass\n",
    "        errD_real.backward()\n",
    "        D_x = output.mean().item()\n",
    "\n",
    "        ## Train with all-fake batch\n",
    "        # Generate batch of latent vectors\n",
    "        noise = torch.randn(b_size, input_dim)\n",
    "        # Generate fake image batch with G\n",
    "        fake = netG(noise)\n",
    "        label.fill_(fake_label)\n",
    "        # Classify all fake batch with D\n",
    "        output, _ = netD(fake.detach())\n",
    "        output = output.view(-1)\n",
    "        # Calculate D's loss on the all-fake batch\n",
    "        errD_fake = criterion(output, label)\n",
    "        #print(errD_fake)\n",
    "        # Calculate the gradients for this batch, accumulated (summed) with previous gradients\n",
    "        errD_fake.backward()\n",
    "        D_G_z1 = output.mean().item()\n",
    "        # Compute error of D as sum over the fake and the real batches\n",
    "        errD = errD_real + errD_fake\n",
    "        # Update D\n",
    "        optimizerD.step()\n",
    "\n",
    "        ############################\n",
    "        # (2) Update G network: maximize log(D(G(z)))\n",
    "        ###########################\n",
    "        netG.zero_grad()\n",
    "        label.fill_(real_label)  # fake labels are real for generator cost\n",
    "        # Since we just updated D, perform another forward pass of all-fake batch through D\n",
    "        output, _ = netD(fake)\n",
    "        output = output.view(-1)\n",
    "        \n",
    "        # Calculate G's loss based on this output\n",
    "        errG = criterion(output, label)\n",
    "        # Calculate gradients for G\n",
    "        errG.backward()\n",
    "        D_G_z2 = output.mean().item()\n",
    "        # Update G\n",
    "        optimizerG.step()\n",
    "\n",
    "        # Output training stats\n",
    "        if i % 50 == 0:\n",
    "            print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n",
    "                  % (epoch, num_epochs, i, len(train_loader),\n",
    "                     errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
    "\n",
    "        # Save Losses for plotting later\n",
    "        G_losses.append(errG.item())\n",
    "        D_losses.append(errD.item())\n",
    "\n",
    "        iters += 1\n",
    "\n",
    "end = time.process_time()\n",
    "arrhythmia_gan_train_time = end - start\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "c2249566",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test['y'] = y_test\n",
    "myDs=MyTestDataset(x_test)\n",
    "test_loader=torch.utils.data.DataLoader(myDs,batch_size=1,shuffle=False,drop_last = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "6f183670",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = netG\n",
    "discriminator = netD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "21f1f79e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.484375\n"
     ]
    }
   ],
   "source": [
    "start = time.process_time()\n",
    "losses = []\n",
    "for j,(image,label) in enumerate(test_loader):\n",
    "    z = init.normal_(torch.zeros(1,input_dim),mean=0,std=0.1)\n",
    "    z_optimizer = torch.optim.Adam([z],lr=1e-4)\n",
    "    \n",
    "    generator.eval()\n",
    "    gen_fake = generator(z)\n",
    "    loss = Anomaly_score(image,gen_fake)\n",
    "    losses.append(loss.detach().numpy().tolist())\n",
    "    \n",
    "threshold = np.mean(losses) + np.std(losses)\n",
    "anomaly_mask = pd.Series(losses) > threshold\n",
    "preds = anomaly_mask.map(lambda x: 1.0 if x == True else 0.0)\n",
    "preds = preds.tolist()\n",
    "\n",
    "x_test['y'] = y_test\n",
    "x_test['anomaly score'] = losses\n",
    "x_test['prediction'] = preds\n",
    "\n",
    "end = time.process_time()\n",
    "arrhythmia_gan_test_time = end - start\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "bf2aeb39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[106,  10],\n",
       "       [ 16,   4]], dtype=int64)"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(x_test['y'], x_test['prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "3b39c8be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.593103448275862"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fpr, tpr, _ = metrics.roc_curve(x_test['y'], x_test['anomaly score'])\n",
    "arrhythmia_gan_auc = metrics.auc(fpr, tpr)\n",
    "metrics.auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "6472b2a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.91      0.89       116\n",
      "           1       0.29      0.20      0.24        20\n",
      "\n",
      "    accuracy                           0.81       136\n",
      "   macro avg       0.58      0.56      0.56       136\n",
      "weighted avg       0.78      0.81      0.79       136\n",
      "\n"
     ]
    }
   ],
   "source": [
    "arrhythmia_gan_report = classification_report(x_test['y'], x_test['prediction'], target_names = ['0','1'], output_dict = True)\n",
    "print(classification_report(x_test['y'], x_test['prediction'], target_names = ['0','1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "e9f654d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2857142857142857\n",
      "0.2\n",
      "0.23529411764705882\n"
     ]
    }
   ],
   "source": [
    "print(arrhythmia_gan_report['1']['precision'])\n",
    "print(arrhythmia_gan_report['1']['recall'])\n",
    "print(arrhythmia_gan_report['1']['f1-score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "03176a4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.32238383264947656\n"
     ]
    }
   ],
   "source": [
    "precision, recall, thresholds = precision_recall_curve(x_test['y'], x_test['anomaly score'])\n",
    "arrhythmia_gan_auc_precision_recall = metrics.auc(recall, precision)\n",
    "print(arrhythmia_gan_auc_precision_recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88953c24",
   "metadata": {},
   "source": [
    "## Cardiocotography"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88add5c0",
   "metadata": {},
   "source": [
    "**Dataset source**: http://odds.cs.stonybrook.edu/cardiotocogrpahy-dataset/ (data is transformed from .mat to .csv format)\n",
    "\n",
    "Shebuti Rayana (2016). ODDS Library [http://odds.cs.stonybrook.edu]. Stony Brook, NY: Stony Brook University, Department of Computer Science.\n",
    "\n",
    "**Additional sources:**\n",
    "\n",
    "C. C. Aggarwal and S. Sathe, “Theoretical foundations and algorithms for outlier ensembles.” ACM SIGKDD Explorations Newsletter, vol. 17, no. 1, pp. 24–47, 2015.\n",
    "\n",
    "Saket Sathe and Charu C. Aggarwal. LODES: Local Density meets Spectral Outlier Detection. SIAM Conference on Data Mining, 2016."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "0804030a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./Cardiotocography.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "883785f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1831, 22)"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "84e74f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['y'] = data['y'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "2498cbbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Col1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>176</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Col1\n",
       "y      \n",
       "0  1655\n",
       "1   176"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.pivot_table(data,\n",
    "             values = 'Col1',\n",
    "               index = 'y', \n",
    "              aggfunc = 'count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "99b795b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Col1</th>\n",
       "      <th>Col2</th>\n",
       "      <th>Col3</th>\n",
       "      <th>Col4</th>\n",
       "      <th>Col5</th>\n",
       "      <th>Col6</th>\n",
       "      <th>Col7</th>\n",
       "      <th>Col8</th>\n",
       "      <th>Col9</th>\n",
       "      <th>Col10</th>\n",
       "      <th>...</th>\n",
       "      <th>Col13</th>\n",
       "      <th>Col14</th>\n",
       "      <th>Col15</th>\n",
       "      <th>Col16</th>\n",
       "      <th>Col17</th>\n",
       "      <th>Col18</th>\n",
       "      <th>Col19</th>\n",
       "      <th>Col20</th>\n",
       "      <th>Col21</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.004912</td>\n",
       "      <td>0.693191</td>\n",
       "      <td>-0.203640</td>\n",
       "      <td>0.595322</td>\n",
       "      <td>0.353190</td>\n",
       "      <td>-0.061401</td>\n",
       "      <td>-0.278295</td>\n",
       "      <td>-1.650444</td>\n",
       "      <td>0.759072</td>\n",
       "      <td>-0.420487</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.798376</td>\n",
       "      <td>1.854728</td>\n",
       "      <td>0.622631</td>\n",
       "      <td>0.963083</td>\n",
       "      <td>0.301464</td>\n",
       "      <td>0.193113</td>\n",
       "      <td>0.231498</td>\n",
       "      <td>-0.289786</td>\n",
       "      <td>-0.493294</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.110729</td>\n",
       "      <td>-0.079903</td>\n",
       "      <td>-0.203640</td>\n",
       "      <td>1.268942</td>\n",
       "      <td>0.396246</td>\n",
       "      <td>-0.061401</td>\n",
       "      <td>-0.278295</td>\n",
       "      <td>-1.710270</td>\n",
       "      <td>0.759072</td>\n",
       "      <td>-0.420487</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.798376</td>\n",
       "      <td>1.854728</td>\n",
       "      <td>0.278625</td>\n",
       "      <td>0.963083</td>\n",
       "      <td>0.301464</td>\n",
       "      <td>0.129265</td>\n",
       "      <td>0.093563</td>\n",
       "      <td>-0.256385</td>\n",
       "      <td>-0.493294</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.216546</td>\n",
       "      <td>-0.272445</td>\n",
       "      <td>-0.203640</td>\n",
       "      <td>1.050988</td>\n",
       "      <td>0.148753</td>\n",
       "      <td>-0.061401</td>\n",
       "      <td>-0.278295</td>\n",
       "      <td>-1.710270</td>\n",
       "      <td>1.106509</td>\n",
       "      <td>-0.420487</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.332931</td>\n",
       "      <td>0.314688</td>\n",
       "      <td>2.342663</td>\n",
       "      <td>-0.488279</td>\n",
       "      <td>0.061002</td>\n",
       "      <td>0.065417</td>\n",
       "      <td>0.024596</td>\n",
       "      <td>-0.256385</td>\n",
       "      <td>1.140018</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.004912</td>\n",
       "      <td>0.727346</td>\n",
       "      <td>-0.203640</td>\n",
       "      <td>1.212171</td>\n",
       "      <td>-0.683598</td>\n",
       "      <td>-0.061401</td>\n",
       "      <td>-0.278295</td>\n",
       "      <td>-1.710270</td>\n",
       "      <td>1.106509</td>\n",
       "      <td>-0.420487</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.332931</td>\n",
       "      <td>0.314688</td>\n",
       "      <td>1.654650</td>\n",
       "      <td>-0.488279</td>\n",
       "      <td>0.061002</td>\n",
       "      <td>0.193113</td>\n",
       "      <td>0.093563</td>\n",
       "      <td>-0.323186</td>\n",
       "      <td>1.140018</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.100905</td>\n",
       "      <td>0.363595</td>\n",
       "      <td>1.321366</td>\n",
       "      <td>1.027120</td>\n",
       "      <td>0.141359</td>\n",
       "      <td>-0.061401</td>\n",
       "      <td>-0.278295</td>\n",
       "      <td>-0.992364</td>\n",
       "      <td>-0.051613</td>\n",
       "      <td>-0.420487</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.085638</td>\n",
       "      <td>-0.565334</td>\n",
       "      <td>0.278625</td>\n",
       "      <td>-0.488279</td>\n",
       "      <td>-0.059229</td>\n",
       "      <td>0.065417</td>\n",
       "      <td>0.024596</td>\n",
       "      <td>-0.456787</td>\n",
       "      <td>1.140018</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Col1      Col2      Col3      Col4      Col5      Col6      Col7  \\\n",
       "0  0.004912  0.693191 -0.203640  0.595322  0.353190 -0.061401 -0.278295   \n",
       "1  0.110729 -0.079903 -0.203640  1.268942  0.396246 -0.061401 -0.278295   \n",
       "2  0.216546 -0.272445 -0.203640  1.050988  0.148753 -0.061401 -0.278295   \n",
       "3  0.004912  0.727346 -0.203640  1.212171 -0.683598 -0.061401 -0.278295   \n",
       "4 -0.100905  0.363595  1.321366  1.027120  0.141359 -0.061401 -0.278295   \n",
       "\n",
       "       Col8      Col9     Col10  ...     Col13     Col14     Col15     Col16  \\\n",
       "0 -1.650444  0.759072 -0.420487  ... -0.798376  1.854728  0.622631  0.963083   \n",
       "1 -1.710270  0.759072 -0.420487  ... -0.798376  1.854728  0.278625  0.963083   \n",
       "2 -1.710270  1.106509 -0.420487  ... -1.332931  0.314688  2.342663 -0.488279   \n",
       "3 -1.710270  1.106509 -0.420487  ... -1.332931  0.314688  1.654650 -0.488279   \n",
       "4 -0.992364 -0.051613 -0.420487  ... -0.085638 -0.565334  0.278625 -0.488279   \n",
       "\n",
       "      Col17     Col18     Col19     Col20     Col21  y  \n",
       "0  0.301464  0.193113  0.231498 -0.289786 -0.493294  0  \n",
       "1  0.301464  0.129265  0.093563 -0.256385 -0.493294  0  \n",
       "2  0.061002  0.065417  0.024596 -0.256385  1.140018  0  \n",
       "3  0.061002  0.193113  0.093563 -0.323186  1.140018  0  \n",
       "4 -0.059229  0.065417  0.024596 -0.456787  1.140018  0  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9404b0cf",
   "metadata": {},
   "source": [
    "## AnoGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "bd091b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "batch_size = 64\n",
    "learning_rate = 0.0002\n",
    "input_dim = data.shape[1]-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "063f5db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    x_train,\n",
    "    x_test,\n",
    "    y_train,\n",
    "    y_test,\n",
    "    indices_train,\n",
    "    indices_test,\n",
    ") = train_test_split(data[data.columns[:-1]], data['y'], data.index, test_size=0.3,stratify=data[['y']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "c3e21533",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = pd.DataFrame(y_train)\n",
    "x_train = x_train.loc[y_train[y_train.y == 0].index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "748b4ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "myDs=MyTrainDataset(x_train)\n",
    "train_loader=torch.utils.data.DataLoader(myDs,batch_size=batch_size,shuffle=False, drop_last = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "21c67d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "netG = Generator()\n",
    "netD = Discriminator()\n",
    "\n",
    "# Initialize BCELoss function\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Establish convention for real and fake labels during training\n",
    "real_label = 1.\n",
    "fake_label = 0.\n",
    "\n",
    "# Setup Adam optimizers for both G and D\n",
    "optimizerD = torch.optim.Adam(netD.parameters(), lr=learning_rate, betas=(0.5, 0.999))\n",
    "optimizerG = torch.optim.Adam(netG.parameters(), lr=learning_rate, betas=(0.5, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "848648de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training Loop...\n",
      "0\n",
      "[0/20][0/18]\tLoss_D: 5.1298\tLoss_G: 3.0200\tD(x): 0.4250\tD(G(z)): 0.4925 / 0.4916\n",
      "1\n",
      "[1/20][0/18]\tLoss_D: 4.1783\tLoss_G: 2.3902\tD(x): 0.4536\tD(G(z)): 0.4706 / 0.4707\n",
      "2\n",
      "[2/20][0/18]\tLoss_D: 3.4773\tLoss_G: 1.9850\tD(x): 0.4799\tD(G(z)): 0.4653 / 0.4650\n",
      "3\n",
      "[3/20][0/18]\tLoss_D: 3.3718\tLoss_G: 1.7044\tD(x): 0.5003\tD(G(z)): 0.5074 / 0.5057\n",
      "4\n",
      "[4/20][0/18]\tLoss_D: 3.1828\tLoss_G: 1.7382\tD(x): 0.5101\tD(G(z)): 0.4698 / 0.4693\n",
      "5\n",
      "[5/20][0/18]\tLoss_D: 2.8949\tLoss_G: 1.4979\tD(x): 0.5144\tD(G(z)): 0.4980 / 0.4952\n",
      "6\n",
      "[6/20][0/18]\tLoss_D: 2.9216\tLoss_G: 1.4844\tD(x): 0.5180\tD(G(z)): 0.5036 / 0.5017\n",
      "7\n",
      "[7/20][0/18]\tLoss_D: 3.0322\tLoss_G: 1.4285\tD(x): 0.5161\tD(G(z)): 0.5262 / 0.5242\n",
      "8\n",
      "[8/20][0/18]\tLoss_D: 2.5539\tLoss_G: 1.3055\tD(x): 0.5202\tD(G(z)): 0.4658 / 0.4646\n",
      "9\n",
      "[9/20][0/18]\tLoss_D: 2.5104\tLoss_G: 1.1440\tD(x): 0.5195\tD(G(z)): 0.4919 / 0.4906\n",
      "10\n",
      "[10/20][0/18]\tLoss_D: 2.6298\tLoss_G: 1.0531\tD(x): 0.5164\tD(G(z)): 0.5308 / 0.5284\n",
      "11\n",
      "[11/20][0/18]\tLoss_D: 2.5239\tLoss_G: 1.2785\tD(x): 0.5169\tD(G(z)): 0.5319 / 0.5287\n",
      "12\n",
      "[12/20][0/18]\tLoss_D: 2.2214\tLoss_G: 1.1339\tD(x): 0.5165\tD(G(z)): 0.4625 / 0.4611\n",
      "13\n",
      "[13/20][0/18]\tLoss_D: 2.2479\tLoss_G: 1.3603\tD(x): 0.5207\tD(G(z)): 0.4453 / 0.4436\n",
      "14\n",
      "[14/20][0/18]\tLoss_D: 2.1898\tLoss_G: 1.1835\tD(x): 0.5191\tD(G(z)): 0.4910 / 0.4884\n",
      "15\n",
      "[15/20][0/18]\tLoss_D: 2.2936\tLoss_G: 1.2286\tD(x): 0.5207\tD(G(z)): 0.5041 / 0.5013\n",
      "16\n",
      "[16/20][0/18]\tLoss_D: 2.0925\tLoss_G: 1.1877\tD(x): 0.5207\tD(G(z)): 0.4724 / 0.4699\n",
      "17\n",
      "[17/20][0/18]\tLoss_D: 2.0667\tLoss_G: 0.9969\tD(x): 0.5208\tD(G(z)): 0.5183 / 0.5144\n",
      "18\n",
      "[18/20][0/18]\tLoss_D: 1.9088\tLoss_G: 1.1981\tD(x): 0.5198\tD(G(z)): 0.4368 / 0.4354\n",
      "19\n",
      "[19/20][0/18]\tLoss_D: 1.8887\tLoss_G: 1.1262\tD(x): 0.5227\tD(G(z)): 0.4504 / 0.4482\n",
      "70.203125\n"
     ]
    }
   ],
   "source": [
    "img_list = []\n",
    "G_losses = []\n",
    "D_losses = []\n",
    "iters = 0\n",
    "\n",
    "start = time.process_time()\n",
    "print(\"Starting Training Loop...\")\n",
    "# For each epoch\n",
    "for epoch in range(num_epochs):\n",
    "    print(epoch)\n",
    "    # For each batch in the dataloader\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "\n",
    "        ############################\n",
    "        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "        ###########################\n",
    "        ## Train with all-real batch\n",
    "        netD.zero_grad()\n",
    "        \n",
    "        real_cpu = data[0]\n",
    "        #print(real_cpu)\n",
    "        b_size = real_cpu.size(0)\n",
    "        # Format batch\n",
    "        label = torch.full((b_size,), real_label, dtype=torch.float)\n",
    "        # Forward pass real batch through D\n",
    "        output, _ = netD(real_cpu)\n",
    "        output = output.view(-1)\n",
    "        # Calculate loss on all-real batch\n",
    "        errD_real = criterion(output, label)\n",
    "        # Calculate gradients for D in backward pass\n",
    "        errD_real.backward()\n",
    "        D_x = output.mean().item()\n",
    "\n",
    "        ## Train with all-fake batch\n",
    "        # Generate batch of latent vectors\n",
    "        noise = torch.randn(b_size, input_dim)\n",
    "        # Generate fake image batch with G\n",
    "        fake = netG(noise)\n",
    "        label.fill_(fake_label)\n",
    "        # Classify all fake batch with D\n",
    "        output, _ = netD(fake.detach())\n",
    "        output = output.view(-1)\n",
    "        # Calculate D's loss on the all-fake batch\n",
    "        errD_fake = criterion(output, label)\n",
    "        #print(errD_fake)\n",
    "        # Calculate the gradients for this batch, accumulated (summed) with previous gradients\n",
    "        errD_fake.backward()\n",
    "        D_G_z1 = output.mean().item()\n",
    "        # Compute error of D as sum over the fake and the real batches\n",
    "        errD = errD_real + errD_fake\n",
    "        # Update D\n",
    "        optimizerD.step()\n",
    "\n",
    "        ############################\n",
    "        # (2) Update G network: maximize log(D(G(z)))\n",
    "        ###########################\n",
    "        netG.zero_grad()\n",
    "        label.fill_(real_label)  # fake labels are real for generator cost\n",
    "        # Since we just updated D, perform another forward pass of all-fake batch through D\n",
    "        output, _ = netD(fake)\n",
    "        output = output.view(-1)\n",
    "        \n",
    "        # Calculate G's loss based on this output\n",
    "        errG = criterion(output, label)\n",
    "        # Calculate gradients for G\n",
    "        errG.backward()\n",
    "        D_G_z2 = output.mean().item()\n",
    "        # Update G\n",
    "        optimizerG.step()\n",
    "\n",
    "        # Output training stats\n",
    "        if i % 50 == 0:\n",
    "            print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n",
    "                  % (epoch, num_epochs, i, len(train_loader),\n",
    "                     errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
    "\n",
    "        # Save Losses for plotting later\n",
    "        G_losses.append(errG.item())\n",
    "        D_losses.append(errD.item())\n",
    "\n",
    "        iters += 1\n",
    "        \n",
    "end = time.process_time()\n",
    "cardio_gan_train_time = end - start\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "5cdc4f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test['y'] = y_test\n",
    "myDs=MyTestDataset(x_test)\n",
    "test_loader=torch.utils.data.DataLoader(myDs,batch_size=1,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "ee2d83a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = netG\n",
    "discriminator = netD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "b959c85c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.59375\n"
     ]
    }
   ],
   "source": [
    "start = time.process_time()\n",
    "losses = []\n",
    "for j,(image,label) in enumerate(test_loader):\n",
    "    z = init.normal_(torch.zeros(1,input_dim),mean=0,std=0.1)\n",
    "    z_optimizer = torch.optim.Adam([z],lr=1e-4)\n",
    "    \n",
    "    generator.eval()\n",
    "    gen_fake = generator(z)\n",
    "    loss = Anomaly_score(image,gen_fake)\n",
    "    losses.append(loss.detach().numpy().tolist())\n",
    "    \n",
    "threshold = np.mean(losses) + np.std(losses)\n",
    "anomaly_mask = pd.Series(losses) > threshold\n",
    "preds = anomaly_mask.map(lambda x: 1.0 if x == True else 0.0)\n",
    "preds = preds.tolist()\n",
    "\n",
    "x_test['y'] = y_test\n",
    "x_test['anomaly score'] = losses\n",
    "x_test['prediction'] = preds\n",
    "\n",
    "end = time.process_time()\n",
    "cardio_gan_test_time = end - start\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "e114e3f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[463,  34],\n",
       "       [ 14,  39]], dtype=int64)"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(x_test['y'], x_test['prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "918da7f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9532288068030828"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fpr, tpr, _ = metrics.roc_curve(x_test['y'], x_test['anomaly score'])\n",
    "cardio_gan_auc = metrics.auc(fpr, tpr)\n",
    "metrics.auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "3ae256a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.93      0.95       497\n",
      "           1       0.53      0.74      0.62        53\n",
      "\n",
      "    accuracy                           0.91       550\n",
      "   macro avg       0.75      0.83      0.78       550\n",
      "weighted avg       0.93      0.91      0.92       550\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cardio_gan_report = classification_report(x_test['y'], x_test['prediction'], target_names = ['0','1'], output_dict = True)\n",
    "print(classification_report(x_test['y'], x_test['prediction'], target_names = ['0','1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "caf1147c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5342465753424658\n",
      "0.7358490566037735\n",
      "0.6190476190476191\n"
     ]
    }
   ],
   "source": [
    "print(cardio_gan_report['1']['precision'])\n",
    "print(cardio_gan_report['1']['recall'])\n",
    "print(cardio_gan_report['1']['f1-score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "61e5da42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6824427867704227\n"
     ]
    }
   ],
   "source": [
    "precision, recall, thresholds = precision_recall_curve(x_test['y'], x_test['anomaly score'])\n",
    "cardio_gan_auc_precision_recall = metrics.auc(recall, precision)\n",
    "print(cardio_gan_auc_precision_recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1563ee67-06dc-4374-b602-f4d623f6ac25",
   "metadata": {
    "tags": []
   },
   "source": [
    "## ForestCover"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e616a6",
   "metadata": {},
   "source": [
    "**Dataset source**: http://odds.cs.stonybrook.edu/forestcovercovertype-dataset/ (data is transformed from .mat to .csv format)\n",
    "\n",
    "Shebuti Rayana (2016). ODDS Library [http://odds.cs.stonybrook.edu]. Stony Brook, NY: Stony Brook University, Department of Computer Science.\n",
    "\n",
    "**Additional sources:**\n",
    "\n",
    "Liu, Fei Tony, Kai Ming Ting, and Zhi-Hua Zhou. “Isolation forest.” 2008 Eighth IEEE International Conference on Data Mining. IEEE, 2008.\n",
    "\n",
    "K. M. Ting, J. T. S. Chuan, and F. T. Liu. “Mass: A New Ranking Measure for Anomaly Detection.“, IEEE Transactions on Knowledge and Data Engineering, 2009.\n",
    "\n",
    "Kai Ming Ting, Guang-Tong Zhou, Fei Tony Liu & Tan Swee Chuan. (2010). Mass Estimation and Its Applications. Proceedings of The 16th ACM SIGKDD Conference on Knowledge Discovery and Data Mining 2010. pp. 989-998.\n",
    "\n",
    "Swee Chuan Tan, Kai Ming Ting & Fei Tony Liu. (2011). Fast Anomaly Detection for Streaming Data. Proceedings of the International Joint Conference on Artificial Intelligence 2011. pp.1151-1156."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "bff11aeb-e777-439f-a989-dbdd142d1027",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./ForestCover.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "bc232f7a-8630-46bf-b1a0-b8b8dbee4688",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(286048, 11)"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "725ab3d0-b18f-4f66-8201-4d1cd3423964",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Col2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>283301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2747</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Col2\n",
       "y        \n",
       "0  283301\n",
       "1    2747"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.pivot_table(data,\n",
    "             values = 'Col2',\n",
    "               index = 'y', \n",
    "              aggfunc = 'count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "ba2e2eaf-a650-494b-ba31-fe2afc4c7d42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Col1</th>\n",
       "      <th>Col2</th>\n",
       "      <th>Col3</th>\n",
       "      <th>Col4</th>\n",
       "      <th>Col5</th>\n",
       "      <th>Col6</th>\n",
       "      <th>Col7</th>\n",
       "      <th>Col8</th>\n",
       "      <th>Col9</th>\n",
       "      <th>Col10</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2804</td>\n",
       "      <td>139</td>\n",
       "      <td>9</td>\n",
       "      <td>268</td>\n",
       "      <td>65</td>\n",
       "      <td>3180</td>\n",
       "      <td>234</td>\n",
       "      <td>238</td>\n",
       "      <td>135</td>\n",
       "      <td>6121</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2785</td>\n",
       "      <td>155</td>\n",
       "      <td>18</td>\n",
       "      <td>242</td>\n",
       "      <td>118</td>\n",
       "      <td>3090</td>\n",
       "      <td>238</td>\n",
       "      <td>238</td>\n",
       "      <td>122</td>\n",
       "      <td>6211</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2579</td>\n",
       "      <td>132</td>\n",
       "      <td>6</td>\n",
       "      <td>300</td>\n",
       "      <td>-15</td>\n",
       "      <td>67</td>\n",
       "      <td>230</td>\n",
       "      <td>237</td>\n",
       "      <td>140</td>\n",
       "      <td>6031</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2886</td>\n",
       "      <td>151</td>\n",
       "      <td>11</td>\n",
       "      <td>371</td>\n",
       "      <td>26</td>\n",
       "      <td>5253</td>\n",
       "      <td>234</td>\n",
       "      <td>240</td>\n",
       "      <td>136</td>\n",
       "      <td>4051</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2742</td>\n",
       "      <td>134</td>\n",
       "      <td>22</td>\n",
       "      <td>150</td>\n",
       "      <td>69</td>\n",
       "      <td>3215</td>\n",
       "      <td>248</td>\n",
       "      <td>224</td>\n",
       "      <td>92</td>\n",
       "      <td>6091</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Col1  Col2  Col3  Col4  Col5  Col6  Col7  Col8  Col9  Col10  y\n",
       "0  2804   139     9   268    65  3180   234   238   135   6121  0\n",
       "1  2785   155    18   242   118  3090   238   238   122   6211  0\n",
       "2  2579   132     6   300   -15    67   230   237   140   6031  0\n",
       "3  2886   151    11   371    26  5253   234   240   136   4051  0\n",
       "4  2742   134    22   150    69  3215   248   224    92   6091  0"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed156a3",
   "metadata": {},
   "source": [
    "## AnoGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "3cb29cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "batch_size = 512\n",
    "learning_rate = 0.0002\n",
    "input_dim = data.shape[1]-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "07c45cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    x_train,\n",
    "    x_test,\n",
    "    y_train,\n",
    "    y_test,\n",
    "    indices_train,\n",
    "    indices_test,\n",
    ") = train_test_split(data[data.columns[:-1]], data['y'], data.index, test_size=0.3,stratify=data[['y']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "d5e1d821",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = pd.DataFrame(y_train)\n",
    "x_train = x_train.loc[y_train[y_train.y == 0].index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "38110718",
   "metadata": {},
   "outputs": [],
   "source": [
    "myDs=MyTrainDataset(x_train)\n",
    "train_loader=torch.utils.data.DataLoader(myDs,batch_size=batch_size,shuffle=False, drop_last = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "fea49d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "netG = Generator()\n",
    "netD = Discriminator()\n",
    "\n",
    "# Initialize BCELoss function\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Establish convention for real and fake labels during training\n",
    "real_label = 1.\n",
    "fake_label = 0.\n",
    "\n",
    "# Setup Adam optimizers for both G and D\n",
    "optimizerD = torch.optim.Adam(netD.parameters(), lr=learning_rate, betas=(0.5, 0.999))\n",
    "optimizerG = torch.optim.Adam(netG.parameters(), lr=learning_rate, betas=(0.5, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "a7ae9912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training Loop...\n",
      "0\n",
      "[0/20][0/387]\tLoss_D: 8.6557\tLoss_G: 3.2158\tD(x): 0.4045\tD(G(z)): 0.4766 / 0.4772\n",
      "[0/20][50/387]\tLoss_D: 2.5428\tLoss_G: 1.0143\tD(x): 0.5085\tD(G(z)): 0.5623 / 0.5586\n",
      "[0/20][100/387]\tLoss_D: 1.8631\tLoss_G: 1.1452\tD(x): 0.5494\tD(G(z)): 0.4710 / 0.4675\n",
      "[0/20][150/387]\tLoss_D: 1.4539\tLoss_G: 1.1660\tD(x): 0.6280\tD(G(z)): 0.4501 / 0.4430\n",
      "[0/20][200/387]\tLoss_D: 1.2721\tLoss_G: 1.3452\tD(x): 0.6464\tD(G(z)): 0.3920 / 0.3875\n",
      "[0/20][250/387]\tLoss_D: 1.1415\tLoss_G: 1.5149\tD(x): 0.6810\tD(G(z)): 0.3577 / 0.3524\n",
      "[0/20][300/387]\tLoss_D: 0.9949\tLoss_G: 1.5678\tD(x): 0.7028\tD(G(z)): 0.3312 / 0.3245\n",
      "[0/20][350/387]\tLoss_D: 0.9296\tLoss_G: 1.7109\tD(x): 0.7034\tD(G(z)): 0.3050 / 0.2999\n",
      "1\n",
      "[1/20][0/387]\tLoss_D: 0.9975\tLoss_G: 1.6129\tD(x): 0.6858\tD(G(z)): 0.3211 / 0.3144\n",
      "[1/20][50/387]\tLoss_D: 0.8951\tLoss_G: 1.6448\tD(x): 0.7136\tD(G(z)): 0.3052 / 0.2969\n",
      "[1/20][100/387]\tLoss_D: 0.8786\tLoss_G: 1.9138\tD(x): 0.7157\tD(G(z)): 0.2821 / 0.2748\n",
      "[1/20][150/387]\tLoss_D: 0.6989\tLoss_G: 1.9402\tD(x): 0.7699\tD(G(z)): 0.2544 / 0.2477\n",
      "[1/20][200/387]\tLoss_D: 0.8137\tLoss_G: 1.9999\tD(x): 0.7593\tD(G(z)): 0.2701 / 0.2602\n",
      "[1/20][250/387]\tLoss_D: 0.7184\tLoss_G: 1.8925\tD(x): 0.7442\tD(G(z)): 0.2555 / 0.2476\n",
      "[1/20][300/387]\tLoss_D: 0.8621\tLoss_G: 1.7416\tD(x): 0.7317\tD(G(z)): 0.3269 / 0.3058\n",
      "[1/20][350/387]\tLoss_D: 1.6324\tLoss_G: 0.7892\tD(x): 0.5219\tD(G(z)): 0.5244 / 0.5125\n",
      "2\n",
      "[2/20][0/387]\tLoss_D: 1.6763\tLoss_G: 0.7558\tD(x): 0.4789\tD(G(z)): 0.5185 / 0.5167\n",
      "[2/20][50/387]\tLoss_D: 1.5650\tLoss_G: 0.8110\tD(x): 0.4832\tD(G(z)): 0.4937 / 0.4876\n",
      "[2/20][100/387]\tLoss_D: 1.6171\tLoss_G: 0.7155\tD(x): 0.5137\tD(G(z)): 0.5284 / 0.5210\n",
      "[2/20][150/387]\tLoss_D: 1.5077\tLoss_G: 0.8077\tD(x): 0.5054\tD(G(z)): 0.4915 / 0.4876\n",
      "[2/20][200/387]\tLoss_D: 1.5997\tLoss_G: 0.7278\tD(x): 0.4733\tD(G(z)): 0.5095 / 0.5081\n",
      "[2/20][250/387]\tLoss_D: 1.5783\tLoss_G: 0.7038\tD(x): 0.5111\tD(G(z)): 0.5232 / 0.5189\n",
      "[2/20][300/387]\tLoss_D: 1.5703\tLoss_G: 0.7432\tD(x): 0.4774\tD(G(z)): 0.5068 / 0.5055\n",
      "[2/20][350/387]\tLoss_D: 1.6052\tLoss_G: 0.6891\tD(x): 0.4757\tD(G(z)): 0.5258 / 0.5229\n",
      "3\n",
      "[3/20][0/387]\tLoss_D: 1.5310\tLoss_G: 0.7366\tD(x): 0.4847\tD(G(z)): 0.4999 / 0.4993\n",
      "[3/20][50/387]\tLoss_D: 1.4952\tLoss_G: 0.7301\tD(x): 0.4972\tD(G(z)): 0.5031 / 0.5015\n",
      "[3/20][100/387]\tLoss_D: 1.5026\tLoss_G: 0.7214\tD(x): 0.4968\tD(G(z)): 0.5075 / 0.5043\n",
      "[3/20][150/387]\tLoss_D: 1.4949\tLoss_G: 0.7248\tD(x): 0.4987\tD(G(z)): 0.5091 / 0.5059\n",
      "[3/20][200/387]\tLoss_D: 1.5208\tLoss_G: 0.7178\tD(x): 0.4846\tD(G(z)): 0.5058 / 0.5049\n",
      "[3/20][250/387]\tLoss_D: 1.4966\tLoss_G: 0.7019\tD(x): 0.5035\tD(G(z)): 0.5120 / 0.5097\n",
      "[3/20][300/387]\tLoss_D: 1.4824\tLoss_G: 0.7360\tD(x): 0.4892\tD(G(z)): 0.4976 / 0.4967\n",
      "[3/20][350/387]\tLoss_D: 1.5015\tLoss_G: 0.7125\tD(x): 0.4971\tD(G(z)): 0.5111 / 0.5085\n",
      "4\n",
      "[4/20][0/387]\tLoss_D: 1.4880\tLoss_G: 0.7401\tD(x): 0.4862\tD(G(z)): 0.4948 / 0.4944\n",
      "[4/20][50/387]\tLoss_D: 1.4381\tLoss_G: 0.7623\tD(x): 0.5016\tD(G(z)): 0.4876 / 0.4875\n",
      "[4/20][100/387]\tLoss_D: 1.4800\tLoss_G: 0.7294\tD(x): 0.4949\tD(G(z)): 0.5026 / 0.5007\n",
      "[4/20][150/387]\tLoss_D: 1.4540\tLoss_G: 0.7464\tD(x): 0.4997\tD(G(z)): 0.4957 / 0.4929\n",
      "[4/20][200/387]\tLoss_D: 1.4983\tLoss_G: 0.7481\tD(x): 0.4895\tD(G(z)): 0.4985 / 0.4957\n",
      "[4/20][250/387]\tLoss_D: 1.4439\tLoss_G: 0.7571\tD(x): 0.5037\tD(G(z)): 0.4904 / 0.4886\n",
      "[4/20][300/387]\tLoss_D: 1.4691\tLoss_G: 0.7314\tD(x): 0.4968\tD(G(z)): 0.5007 / 0.4979\n",
      "[4/20][350/387]\tLoss_D: 1.4442\tLoss_G: 0.7562\tD(x): 0.4945\tD(G(z)): 0.4870 / 0.4862\n",
      "5\n",
      "[5/20][0/387]\tLoss_D: 1.4198\tLoss_G: 0.7141\tD(x): 0.5194\tD(G(z)): 0.5066 / 0.5020\n",
      "[5/20][50/387]\tLoss_D: 1.4439\tLoss_G: 0.7127\tD(x): 0.5107\tD(G(z)): 0.5059 / 0.5033\n",
      "[5/20][100/387]\tLoss_D: 1.4527\tLoss_G: 0.7421\tD(x): 0.4960\tD(G(z)): 0.4967 / 0.4934\n",
      "[5/20][150/387]\tLoss_D: 1.4760\tLoss_G: 0.7082\tD(x): 0.4993\tD(G(z)): 0.5114 / 0.5081\n",
      "[5/20][200/387]\tLoss_D: 1.4520\tLoss_G: 0.7251\tD(x): 0.4927\tD(G(z)): 0.4981 / 0.4968\n",
      "[5/20][250/387]\tLoss_D: 1.3957\tLoss_G: 0.7473\tD(x): 0.5133\tD(G(z)): 0.4872 / 0.4869\n",
      "[5/20][300/387]\tLoss_D: 1.4549\tLoss_G: 0.7111\tD(x): 0.5072\tD(G(z)): 0.5086 / 0.5048\n",
      "[5/20][350/387]\tLoss_D: 1.4199\tLoss_G: 0.7322\tD(x): 0.5104\tD(G(z)): 0.4973 / 0.4954\n",
      "6\n",
      "[6/20][0/387]\tLoss_D: 1.4214\tLoss_G: 0.7338\tD(x): 0.5102\tD(G(z)): 0.4979 / 0.4951\n",
      "[6/20][50/387]\tLoss_D: 1.4468\tLoss_G: 0.7138\tD(x): 0.4972\tD(G(z)): 0.5020 / 0.5010\n",
      "[6/20][100/387]\tLoss_D: 1.4295\tLoss_G: 0.7241\tD(x): 0.5065\tD(G(z)): 0.5023 / 0.4983\n",
      "[6/20][150/387]\tLoss_D: 1.3818\tLoss_G: 0.7414\tD(x): 0.5149\tD(G(z)): 0.4889 / 0.4879\n",
      "[6/20][200/387]\tLoss_D: 1.4476\tLoss_G: 0.7001\tD(x): 0.5030\tD(G(z)): 0.5097 / 0.5057\n",
      "[6/20][250/387]\tLoss_D: 1.4228\tLoss_G: 0.7437\tD(x): 0.5046\tD(G(z)): 0.4949 / 0.4932\n",
      "[6/20][300/387]\tLoss_D: 1.4232\tLoss_G: 0.7357\tD(x): 0.4983\tD(G(z)): 0.4920 / 0.4903\n",
      "[6/20][350/387]\tLoss_D: 1.3988\tLoss_G: 0.7015\tD(x): 0.5267\tD(G(z)): 0.5100 / 0.5056\n",
      "7\n",
      "[7/20][0/387]\tLoss_D: 1.3882\tLoss_G: 0.7430\tD(x): 0.5129\tD(G(z)): 0.4904 / 0.4881\n",
      "[7/20][50/387]\tLoss_D: 1.4002\tLoss_G: 0.7376\tD(x): 0.5102\tD(G(z)): 0.4929 / 0.4900\n",
      "[7/20][100/387]\tLoss_D: 1.4196\tLoss_G: 0.7277\tD(x): 0.5062\tD(G(z)): 0.4998 / 0.4947\n",
      "[7/20][150/387]\tLoss_D: 1.3760\tLoss_G: 0.7746\tD(x): 0.5091\tD(G(z)): 0.4778 / 0.4767\n",
      "[7/20][200/387]\tLoss_D: 1.3953\tLoss_G: 0.7343\tD(x): 0.5143\tD(G(z)): 0.4949 / 0.4923\n",
      "[7/20][250/387]\tLoss_D: 1.3652\tLoss_G: 0.7717\tD(x): 0.5059\tD(G(z)): 0.4729 / 0.4720\n",
      "[7/20][300/387]\tLoss_D: 1.4248\tLoss_G: 0.7054\tD(x): 0.5114\tD(G(z)): 0.5068 / 0.5034\n",
      "[7/20][350/387]\tLoss_D: 1.3454\tLoss_G: 0.7780\tD(x): 0.5203\tD(G(z)): 0.4758 / 0.4727\n",
      "8\n",
      "[8/20][0/387]\tLoss_D: 1.3619\tLoss_G: 0.7888\tD(x): 0.5085\tD(G(z)): 0.4708 / 0.4687\n",
      "[8/20][50/387]\tLoss_D: 1.2691\tLoss_G: 0.8468\tD(x): 0.5263\tD(G(z)): 0.4430 / 0.4439\n",
      "[8/20][100/387]\tLoss_D: 1.3967\tLoss_G: 0.7256\tD(x): 0.5136\tD(G(z)): 0.4995 / 0.4925\n",
      "[8/20][150/387]\tLoss_D: 1.3934\tLoss_G: 0.7231\tD(x): 0.5168\tD(G(z)): 0.5010 / 0.4953\n",
      "[8/20][200/387]\tLoss_D: 1.3819\tLoss_G: 0.7394\tD(x): 0.5149\tD(G(z)): 0.4901 / 0.4880\n",
      "[8/20][250/387]\tLoss_D: 1.2776\tLoss_G: 0.8604\tD(x): 0.5226\tD(G(z)): 0.4408 / 0.4414\n",
      "[8/20][300/387]\tLoss_D: 1.3254\tLoss_G: 0.8069\tD(x): 0.5169\tD(G(z)): 0.4602 / 0.4608\n",
      "[8/20][350/387]\tLoss_D: 1.3336\tLoss_G: 0.7498\tD(x): 0.5394\tD(G(z)): 0.4886 / 0.4864\n",
      "9\n",
      "[9/20][0/387]\tLoss_D: 1.3537\tLoss_G: 0.7452\tD(x): 0.5300\tD(G(z)): 0.4895 / 0.4869\n",
      "[9/20][50/387]\tLoss_D: 1.2957\tLoss_G: 0.8308\tD(x): 0.5248\tD(G(z)): 0.4548 / 0.4522\n",
      "[9/20][100/387]\tLoss_D: 1.3072\tLoss_G: 0.8222\tD(x): 0.5211\tD(G(z)): 0.4561 / 0.4545\n",
      "[9/20][150/387]\tLoss_D: 1.3169\tLoss_G: 0.7774\tD(x): 0.5372\tD(G(z)): 0.4764 / 0.4735\n",
      "[9/20][200/387]\tLoss_D: 1.1579\tLoss_G: 1.0309\tD(x): 0.5211\tD(G(z)): 0.3674 / 0.3765\n",
      "[9/20][250/387]\tLoss_D: 1.2978\tLoss_G: 0.8218\tD(x): 0.5289\tD(G(z)): 0.4601 / 0.4540\n",
      "[9/20][300/387]\tLoss_D: 1.3348\tLoss_G: 0.7775\tD(x): 0.5246\tD(G(z)): 0.4746 / 0.4716\n",
      "[9/20][350/387]\tLoss_D: 1.2930\tLoss_G: 0.7890\tD(x): 0.5334\tD(G(z)): 0.4674 / 0.4636\n",
      "10\n",
      "[10/20][0/387]\tLoss_D: 1.2262\tLoss_G: 0.8552\tD(x): 0.5455\tD(G(z)): 0.4403 / 0.4397\n",
      "[10/20][50/387]\tLoss_D: 1.2839\tLoss_G: 0.8236\tD(x): 0.5258\tD(G(z)): 0.4511 / 0.4507\n",
      "[10/20][100/387]\tLoss_D: 1.3035\tLoss_G: 0.7945\tD(x): 0.5348\tD(G(z)): 0.4706 / 0.4653\n",
      "[10/20][150/387]\tLoss_D: 1.2554\tLoss_G: 0.8358\tD(x): 0.5331\tD(G(z)): 0.4450 / 0.4452\n",
      "[10/20][200/387]\tLoss_D: 1.2352\tLoss_G: 0.8791\tD(x): 0.5320\tD(G(z)): 0.4290 / 0.4282\n",
      "[10/20][250/387]\tLoss_D: 1.3694\tLoss_G: 0.7296\tD(x): 0.5330\tD(G(z)): 0.5007 / 0.4928\n",
      "[10/20][300/387]\tLoss_D: 1.3295\tLoss_G: 0.7381\tD(x): 0.5519\tD(G(z)): 0.4968 / 0.4890\n",
      "[10/20][350/387]\tLoss_D: 1.1731\tLoss_G: 0.9636\tD(x): 0.5464\tD(G(z)): 0.4046 / 0.4052\n",
      "11\n",
      "[11/20][0/387]\tLoss_D: 1.3045\tLoss_G: 0.8045\tD(x): 0.5384\tD(G(z)): 0.4713 / 0.4645\n",
      "[11/20][50/387]\tLoss_D: 1.2502\tLoss_G: 0.8726\tD(x): 0.5340\tD(G(z)): 0.4371 / 0.4328\n",
      "[11/20][100/387]\tLoss_D: 1.1784\tLoss_G: 0.9293\tD(x): 0.5630\tD(G(z)): 0.4259 / 0.4202\n",
      "[11/20][150/387]\tLoss_D: 1.2596\tLoss_G: 0.7925\tD(x): 0.5642\tD(G(z)): 0.4737 / 0.4679\n",
      "[11/20][200/387]\tLoss_D: 1.2933\tLoss_G: 0.8126\tD(x): 0.5426\tD(G(z)): 0.4687 / 0.4594\n",
      "[11/20][250/387]\tLoss_D: 1.0515\tLoss_G: 1.2131\tD(x): 0.5299\tD(G(z)): 0.3047 / 0.3176\n",
      "[11/20][300/387]\tLoss_D: 1.2474\tLoss_G: 0.8369\tD(x): 0.5601\tD(G(z)): 0.4586 / 0.4491\n",
      "[11/20][350/387]\tLoss_D: 1.2262\tLoss_G: 0.8473\tD(x): 0.5623\tD(G(z)): 0.4522 / 0.4449\n",
      "12\n",
      "[12/20][0/387]\tLoss_D: 1.3587\tLoss_G: 0.7251\tD(x): 0.5583\tD(G(z)): 0.5128 / 0.4997\n",
      "[12/20][50/387]\tLoss_D: 1.2280\tLoss_G: 0.9636\tD(x): 0.5209\tD(G(z)): 0.4101 / 0.4096\n",
      "[12/20][100/387]\tLoss_D: 1.2903\tLoss_G: 0.8084\tD(x): 0.5379\tD(G(z)): 0.4642 / 0.4596\n",
      "[12/20][150/387]\tLoss_D: 1.2822\tLoss_G: 0.7965\tD(x): 0.5434\tD(G(z)): 0.4668 / 0.4637\n",
      "[12/20][200/387]\tLoss_D: 1.3052\tLoss_G: 0.7591\tD(x): 0.5535\tD(G(z)): 0.4887 / 0.4804\n",
      "[12/20][250/387]\tLoss_D: 0.9754\tLoss_G: 1.2707\tD(x): 0.5581\tD(G(z)): 0.2953 / 0.3102\n",
      "[12/20][300/387]\tLoss_D: 1.1989\tLoss_G: 0.8442\tD(x): 0.5867\tD(G(z)): 0.4579 / 0.4481\n",
      "[12/20][350/387]\tLoss_D: 1.2093\tLoss_G: 0.9436\tD(x): 0.5413\tD(G(z)): 0.4175 / 0.4131\n",
      "13\n",
      "[13/20][0/387]\tLoss_D: 1.1452\tLoss_G: 0.9138\tD(x): 0.5723\tD(G(z)): 0.4153 / 0.4184\n",
      "[13/20][50/387]\tLoss_D: 1.2552\tLoss_G: 0.8460\tD(x): 0.5371\tD(G(z)): 0.4460 / 0.4439\n",
      "[13/20][100/387]\tLoss_D: 1.2302\tLoss_G: 0.8777\tD(x): 0.5393\tD(G(z)): 0.4325 / 0.4324\n",
      "[13/20][150/387]\tLoss_D: 1.3855\tLoss_G: 0.7499\tD(x): 0.5380\tD(G(z)): 0.5023 / 0.4893\n",
      "[13/20][200/387]\tLoss_D: 1.3101\tLoss_G: 0.7822\tD(x): 0.5425\tD(G(z)): 0.4778 / 0.4684\n",
      "[13/20][250/387]\tLoss_D: 1.2363\tLoss_G: 0.9256\tD(x): 0.5232\tD(G(z)): 0.4169 / 0.4176\n",
      "[13/20][300/387]\tLoss_D: 1.3326\tLoss_G: 0.7679\tD(x): 0.5363\tD(G(z)): 0.4832 / 0.4768\n",
      "[13/20][350/387]\tLoss_D: 1.3098\tLoss_G: 0.7954\tD(x): 0.5433\tD(G(z)): 0.4795 / 0.4642\n",
      "14\n",
      "[14/20][0/387]\tLoss_D: 1.3246\tLoss_G: 0.8178\tD(x): 0.5296\tD(G(z)): 0.4698 / 0.4581\n",
      "[14/20][50/387]\tLoss_D: 1.3599\tLoss_G: 0.7838\tD(x): 0.5187\tD(G(z)): 0.4775 / 0.4716\n",
      "[14/20][100/387]\tLoss_D: 1.2632\tLoss_G: 0.8869\tD(x): 0.5272\tD(G(z)): 0.4350 / 0.4310\n",
      "[14/20][150/387]\tLoss_D: 1.0358\tLoss_G: 1.1677\tD(x): 0.5754\tD(G(z)): 0.3379 / 0.3498\n",
      "[14/20][200/387]\tLoss_D: 1.3037\tLoss_G: 0.8528\tD(x): 0.5166\tD(G(z)): 0.4481 / 0.4426\n",
      "[14/20][250/387]\tLoss_D: 1.2880\tLoss_G: 0.8041\tD(x): 0.5423\tD(G(z)): 0.4685 / 0.4606\n",
      "[14/20][300/387]\tLoss_D: 1.3260\tLoss_G: 0.7781\tD(x): 0.5358\tD(G(z)): 0.4808 / 0.4709\n",
      "[14/20][350/387]\tLoss_D: 1.1189\tLoss_G: 0.9687\tD(x): 0.5579\tD(G(z)): 0.3891 / 0.3963\n",
      "15\n",
      "[15/20][0/387]\tLoss_D: 1.3584\tLoss_G: 0.7120\tD(x): 0.5513\tD(G(z)): 0.5123 / 0.5011\n",
      "[15/20][50/387]\tLoss_D: 1.3466\tLoss_G: 0.7870\tD(x): 0.5208\tD(G(z)): 0.4749 / 0.4684\n",
      "[15/20][100/387]\tLoss_D: 1.3417\tLoss_G: 0.7890\tD(x): 0.5168\tD(G(z)): 0.4695 / 0.4674\n",
      "[15/20][150/387]\tLoss_D: 1.3383\tLoss_G: 0.8106\tD(x): 0.5092\tD(G(z)): 0.4604 / 0.4581\n",
      "[15/20][200/387]\tLoss_D: 1.3276\tLoss_G: 0.7754\tD(x): 0.5538\tD(G(z)): 0.4950 / 0.4768\n",
      "[15/20][250/387]\tLoss_D: 1.3969\tLoss_G: 0.7141\tD(x): 0.5299\tD(G(z)): 0.5120 / 0.5014\n",
      "[15/20][300/387]\tLoss_D: 1.3670\tLoss_G: 0.7808\tD(x): 0.5173\tD(G(z)): 0.4836 / 0.4748\n",
      "[15/20][350/387]\tLoss_D: 1.3151\tLoss_G: 0.7954\tD(x): 0.5284\tD(G(z)): 0.4713 / 0.4628\n",
      "16\n",
      "[16/20][0/387]\tLoss_D: 1.2764\tLoss_G: 0.7903\tD(x): 0.5621\tD(G(z)): 0.4783 / 0.4694\n",
      "[16/20][50/387]\tLoss_D: 1.3179\tLoss_G: 0.8421\tD(x): 0.5179\tD(G(z)): 0.4540 / 0.4489\n",
      "[16/20][100/387]\tLoss_D: 1.3075\tLoss_G: 0.8469\tD(x): 0.5391\tD(G(z)): 0.4684 / 0.4482\n",
      "[16/20][150/387]\tLoss_D: 1.3692\tLoss_G: 0.7543\tD(x): 0.5195\tD(G(z)): 0.4866 / 0.4814\n",
      "[16/20][200/387]\tLoss_D: 1.3531\tLoss_G: 0.7823\tD(x): 0.5279\tD(G(z)): 0.4853 / 0.4734\n",
      "[16/20][250/387]\tLoss_D: 1.3740\tLoss_G: 0.7383\tD(x): 0.5297\tD(G(z)): 0.4992 / 0.4926\n",
      "[16/20][300/387]\tLoss_D: 1.3604\tLoss_G: 0.7557\tD(x): 0.5201\tD(G(z)): 0.4841 / 0.4807\n",
      "[16/20][350/387]\tLoss_D: 1.2908\tLoss_G: 0.7989\tD(x): 0.5415\tD(G(z)): 0.4681 / 0.4651\n",
      "17\n",
      "[17/20][0/387]\tLoss_D: 1.3475\tLoss_G: 0.8004\tD(x): 0.5297\tD(G(z)): 0.4823 / 0.4658\n",
      "[17/20][50/387]\tLoss_D: 1.1683\tLoss_G: 0.9639\tD(x): 0.5301\tD(G(z)): 0.3879 / 0.3976\n",
      "[17/20][100/387]\tLoss_D: 1.3514\tLoss_G: 0.7797\tD(x): 0.5164\tD(G(z)): 0.4760 / 0.4708\n",
      "[17/20][150/387]\tLoss_D: 1.4001\tLoss_G: 0.7359\tD(x): 0.5188\tD(G(z)): 0.5007 / 0.4927\n",
      "[17/20][200/387]\tLoss_D: 1.3625\tLoss_G: 0.7841\tD(x): 0.5138\tD(G(z)): 0.4771 / 0.4697\n",
      "[17/20][250/387]\tLoss_D: 1.3636\tLoss_G: 0.7676\tD(x): 0.5171\tD(G(z)): 0.4813 / 0.4750\n",
      "[17/20][300/387]\tLoss_D: 1.3667\tLoss_G: 0.7562\tD(x): 0.5216\tD(G(z)): 0.4869 / 0.4817\n",
      "[17/20][350/387]\tLoss_D: 1.3216\tLoss_G: 0.7750\tD(x): 0.5272\tD(G(z)): 0.4739 / 0.4705\n",
      "18\n",
      "[18/20][0/387]\tLoss_D: 1.3089\tLoss_G: 0.8211\tD(x): 0.5238\tD(G(z)): 0.4610 / 0.4527\n",
      "[18/20][50/387]\tLoss_D: 1.3335\tLoss_G: 0.8095\tD(x): 0.5184\tD(G(z)): 0.4616 / 0.4596\n",
      "[18/20][100/387]\tLoss_D: 1.3061\tLoss_G: 0.8141\tD(x): 0.5243\tD(G(z)): 0.4602 / 0.4566\n",
      "[18/20][150/387]\tLoss_D: 1.3388\tLoss_G: 0.7899\tD(x): 0.5190\tD(G(z)): 0.4719 / 0.4681\n",
      "[18/20][200/387]\tLoss_D: 1.3661\tLoss_G: 0.7741\tD(x): 0.5129\tD(G(z)): 0.4775 / 0.4731\n",
      "[18/20][250/387]\tLoss_D: 1.2995\tLoss_G: 0.8383\tD(x): 0.5086\tD(G(z)): 0.4445 / 0.4433\n",
      "[18/20][300/387]\tLoss_D: 1.3818\tLoss_G: 0.7387\tD(x): 0.5198\tD(G(z)): 0.4946 / 0.4873\n",
      "[18/20][350/387]\tLoss_D: 1.3891\tLoss_G: 0.7493\tD(x): 0.5119\tD(G(z)): 0.4905 / 0.4855\n",
      "19\n",
      "[19/20][0/387]\tLoss_D: 1.3407\tLoss_G: 0.7398\tD(x): 0.5374\tD(G(z)): 0.4925 / 0.4877\n",
      "[19/20][50/387]\tLoss_D: 1.3515\tLoss_G: 0.7830\tD(x): 0.5118\tD(G(z)): 0.4726 / 0.4692\n",
      "[19/20][100/387]\tLoss_D: 1.3006\tLoss_G: 0.8454\tD(x): 0.5091\tD(G(z)): 0.4425 / 0.4452\n",
      "[19/20][150/387]\tLoss_D: 1.3436\tLoss_G: 0.7722\tD(x): 0.5234\tD(G(z)): 0.4783 / 0.4738\n",
      "[19/20][200/387]\tLoss_D: 1.3432\tLoss_G: 0.7705\tD(x): 0.5187\tD(G(z)): 0.4732 / 0.4732\n",
      "[19/20][250/387]\tLoss_D: 1.4092\tLoss_G: 0.7350\tD(x): 0.5097\tD(G(z)): 0.4971 / 0.4914\n",
      "[19/20][300/387]\tLoss_D: 1.3408\tLoss_G: 0.7637\tD(x): 0.5255\tD(G(z)): 0.4822 / 0.4748\n",
      "[19/20][350/387]\tLoss_D: 1.3780\tLoss_G: 0.7511\tD(x): 0.5177\tD(G(z)): 0.4904 / 0.4822\n",
      "3768.421875\n"
     ]
    }
   ],
   "source": [
    "img_list = []\n",
    "G_losses = []\n",
    "D_losses = []\n",
    "iters = 0\n",
    "\n",
    "start = time.process_time()\n",
    "print(\"Starting Training Loop...\")\n",
    "# For each epoch\n",
    "for epoch in range(num_epochs):\n",
    "    print(epoch)\n",
    "    # For each batch in the dataloader\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "\n",
    "        ############################\n",
    "        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "        ###########################\n",
    "        ## Train with all-real batch\n",
    "        netD.zero_grad()\n",
    "        \n",
    "        real_cpu = data[0]\n",
    "        #print(real_cpu)\n",
    "        b_size = real_cpu.size(0)\n",
    "        # Format batch\n",
    "        label = torch.full((b_size,), real_label, dtype=torch.float)\n",
    "        # Forward pass real batch through D\n",
    "        output, _ = netD(real_cpu)\n",
    "        output = output.view(-1)\n",
    "        # Calculate loss on all-real batch\n",
    "        errD_real = criterion(output, label)\n",
    "        # Calculate gradients for D in backward pass\n",
    "        errD_real.backward()\n",
    "        D_x = output.mean().item()\n",
    "\n",
    "        ## Train with all-fake batch\n",
    "        # Generate batch of latent vectors\n",
    "        noise = torch.randn(b_size, input_dim)\n",
    "        # Generate fake image batch with G\n",
    "        fake = netG(noise)\n",
    "        label.fill_(fake_label)\n",
    "        # Classify all fake batch with D\n",
    "        output, _ = netD(fake.detach())\n",
    "        output = output.view(-1)\n",
    "        # Calculate D's loss on the all-fake batch\n",
    "        errD_fake = criterion(output, label)\n",
    "        #print(errD_fake)\n",
    "        # Calculate the gradients for this batch, accumulated (summed) with previous gradients\n",
    "        errD_fake.backward()\n",
    "        D_G_z1 = output.mean().item()\n",
    "        # Compute error of D as sum over the fake and the real batches\n",
    "        errD = errD_real + errD_fake\n",
    "        # Update D\n",
    "        optimizerD.step()\n",
    "\n",
    "        ############################\n",
    "        # (2) Update G network: maximize log(D(G(z)))\n",
    "        ###########################\n",
    "        netG.zero_grad()\n",
    "        label.fill_(real_label)  # fake labels are real for generator cost\n",
    "        # Since we just updated D, perform another forward pass of all-fake batch through D\n",
    "        output, _ = netD(fake)\n",
    "        output = output.view(-1)\n",
    "        \n",
    "        # Calculate G's loss based on this output\n",
    "        errG = criterion(output, label)\n",
    "        # Calculate gradients for G\n",
    "        errG.backward()\n",
    "        D_G_z2 = output.mean().item()\n",
    "        # Update G\n",
    "        optimizerG.step()\n",
    "\n",
    "        # Output training stats\n",
    "        if i % 50 == 0:\n",
    "            print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n",
    "                  % (epoch, num_epochs, i, len(train_loader),\n",
    "                     errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
    "\n",
    "        # Save Losses for plotting later\n",
    "        G_losses.append(errG.item())\n",
    "        D_losses.append(errD.item())\n",
    "    \n",
    "        iters += 1\n",
    "        \n",
    "end = time.process_time()\n",
    "forestcover_gan_train_time = end - start\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "e3f5eb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test['y'] = y_test\n",
    "myDs=MyTestDataset(x_test)\n",
    "test_loader=torch.utils.data.DataLoader(myDs,batch_size=1,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "1f4b57d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = netG\n",
    "discriminator = netD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "10dc125f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1458.296875\n"
     ]
    }
   ],
   "source": [
    "start = time.process_time()\n",
    "losses = []\n",
    "for j,(image,label) in enumerate(test_loader):\n",
    "    z = init.normal_(torch.zeros(1,input_dim),mean=0,std=0.1)\n",
    "    z_optimizer = torch.optim.Adam([z],lr=1e-4)\n",
    "    generator.eval()\n",
    "    gen_fake = generator(z)\n",
    "    loss = Anomaly_score(image,gen_fake)\n",
    "    losses.append(loss.detach().numpy().tolist())\n",
    "    \n",
    "threshold = np.mean(losses) + np.std(losses)\n",
    "anomaly_mask = pd.Series(losses) > threshold\n",
    "preds = anomaly_mask.map(lambda x: 1.0 if x == True else 0.0)\n",
    "preds = preds.tolist()\n",
    "\n",
    "x_test['y'] = y_test\n",
    "x_test['anomaly score'] = losses\n",
    "x_test['prediction'] = preds\n",
    "\n",
    "end = time.process_time()\n",
    "forestcover_gan_test_time = end - start\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "63225d42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[70773, 14218],\n",
       "       [  824,     0]], dtype=int64)"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(x_test['y'], x_test['prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "0cf5cc21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04266760312599632"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fpr, tpr, _ = metrics.roc_curve(x_test['y'], x_test['anomaly score'])\n",
    "forestcover_gan_auc = metrics.auc(fpr, tpr)\n",
    "metrics.auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "111da7c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.83      0.90     84991\n",
      "           1       0.00      0.00      0.00       824\n",
      "\n",
      "    accuracy                           0.82     85815\n",
      "   macro avg       0.49      0.42      0.45     85815\n",
      "weighted avg       0.98      0.82      0.90     85815\n",
      "\n"
     ]
    }
   ],
   "source": [
    "forestcover_gan_report = classification_report(x_test['y'], x_test['prediction'], target_names = ['0','1'], output_dict = True)\n",
    "print(classification_report(x_test['y'], x_test['prediction'], target_names = ['0','1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "d9f9c1b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(forestcover_gan_report['1']['precision'])\n",
    "print(forestcover_gan_report['1']['recall'])\n",
    "print(forestcover_gan_report['1']['f1-score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "1e2da0b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0049248904148862125\n"
     ]
    }
   ],
   "source": [
    "precision, recall, thresholds = precision_recall_curve(x_test['y'], x_test['anomaly score'])\n",
    "forestcover_gan_auc_precision_recall = metrics.auc(recall, precision)\n",
    "print(forestcover_gan_auc_precision_recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e7ab52-cd86-4205-829f-82354fd330c0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Annthyroid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84bc2c60",
   "metadata": {},
   "source": [
    "**Dataset source**: http://odds.cs.stonybrook.edu/annthyroid-dataset/ (data is transformed from .mat to .csv format)\n",
    "\n",
    "Shebuti Rayana (2016). ODDS Library [http://odds.cs.stonybrook.edu]. Stony Brook, NY: Stony Brook University, Department of Computer Science.\n",
    "\n",
    "**Aditional sources:**\n",
    "\n",
    "Abe, Naoki, Bianca Zadrozny, and John Langford. “Outlier detection by active learning.” Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 2006.\n",
    "\n",
    "Liu, Fei Tony, Kai Ming Ting, and Zhi-Hua Zhou. “Isolation forest.” 2008 Eighth IEEE International Conference on Data Mining. IEEE, 2008.\n",
    "\n",
    "K. M. Ting, J. T. S. Chuan, and F. T. Liu. “Mass: A New Ranking Measure for Anomaly Detection.“, IEEE Transactions on Knowledge and Data Engineering, 2009."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "74880b01-a430-41cc-988f-647cb4ca4ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./annthyroid.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "be075faf-1275-4bc7-bffb-874175a5fb4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Col1</th>\n",
       "      <th>Col2</th>\n",
       "      <th>Col3</th>\n",
       "      <th>Col4</th>\n",
       "      <th>Col5</th>\n",
       "      <th>Col6</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.73</td>\n",
       "      <td>0.00060</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.082</td>\n",
       "      <td>0.146</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.24</td>\n",
       "      <td>0.00025</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.108</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.47</td>\n",
       "      <td>0.00190</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.131</td>\n",
       "      <td>0.078</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.64</td>\n",
       "      <td>0.00090</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.085</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.23</td>\n",
       "      <td>0.00025</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.139</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.153</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Col1     Col2   Col3   Col4   Col5   Col6  y\n",
       "0  0.73  0.00060  0.015  0.120  0.082  0.146  0\n",
       "1  0.24  0.00025  0.030  0.143  0.133  0.108  0\n",
       "2  0.47  0.00190  0.024  0.102  0.131  0.078  0\n",
       "3  0.64  0.00090  0.017  0.077  0.090  0.085  0\n",
       "4  0.23  0.00025  0.026  0.139  0.090  0.153  0"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "f83c1dd7-907d-4135-a293-180dd4327d11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7200, 7)"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "1d67fa9a-8257-4681-8366-9a701af73c7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Col1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>534</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Col1\n",
       "y      \n",
       "0  6666\n",
       "1   534"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.pivot_table(data,\n",
    "             values = 'Col1',\n",
    "               index = 'y', \n",
    "              aggfunc = 'count')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed066d2",
   "metadata": {},
   "source": [
    "## AnoGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "e49967ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "batch_size = 512\n",
    "learning_rate = 0.0002\n",
    "input_dim = data.shape[1]-1\n",
    "num_epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "ae26250a",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    x_train,\n",
    "    x_test,\n",
    "    y_train,\n",
    "    y_test,\n",
    "    indices_train,\n",
    "    indices_test,\n",
    ") = train_test_split(data[data.columns[:-1]], data['y'], data.index, test_size=0.3,stratify=data[['y']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "ea04e675",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = pd.DataFrame(y_train)\n",
    "x_train = x_train.loc[y_train[y_train.y == 0].index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "444fe104",
   "metadata": {},
   "outputs": [],
   "source": [
    "myDs=MyTrainDataset(x_train)\n",
    "train_loader=torch.utils.data.DataLoader(myDs,batch_size=batch_size,shuffle=False, drop_last = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "18a24551",
   "metadata": {},
   "outputs": [],
   "source": [
    "netG = Generator()\n",
    "netD = Discriminator()\n",
    "\n",
    "# Initialize BCELoss function\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "\n",
    "# Establish convention for real and fake labels during training\n",
    "real_label = 1.\n",
    "fake_label = 0.\n",
    "\n",
    "# Setup Adam optimizers for both G and D\n",
    "optimizerD = torch.optim.Adam(netD.parameters(), lr=learning_rate, betas=(0.5, 0.999))\n",
    "optimizerG = torch.optim.Adam(netG.parameters(), lr=learning_rate, betas=(0.5, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "f08e1b4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training Loop...\n",
      "0\n",
      "[0/20][0/9]\tLoss_D: 6.5559\tLoss_G: 5.4583\tD(x): 0.1997\tD(G(z)): 0.2840 / 0.2830\n",
      "1\n",
      "[1/20][0/9]\tLoss_D: 4.5263\tLoss_G: 3.4387\tD(x): 0.1788\tD(G(z)): 0.2131 / 0.2137\n",
      "2\n",
      "[2/20][0/9]\tLoss_D: 3.9052\tLoss_G: 2.6849\tD(x): 0.1883\tD(G(z)): 0.2512 / 0.2520\n",
      "3\n",
      "[3/20][0/9]\tLoss_D: 3.5160\tLoss_G: 2.0896\tD(x): 0.2213\tD(G(z)): 0.3107 / 0.3108\n",
      "4\n",
      "[4/20][0/9]\tLoss_D: 3.1858\tLoss_G: 1.7220\tD(x): 0.2624\tD(G(z)): 0.3736 / 0.3725\n",
      "5\n",
      "[5/20][0/9]\tLoss_D: 2.8148\tLoss_G: 1.6934\tD(x): 0.3066\tD(G(z)): 0.3709 / 0.3689\n",
      "6\n",
      "[6/20][0/9]\tLoss_D: 2.5353\tLoss_G: 1.4784\tD(x): 0.3520\tD(G(z)): 0.3978 / 0.3934\n",
      "7\n",
      "[7/20][0/9]\tLoss_D: 2.2631\tLoss_G: 1.3035\tD(x): 0.3958\tD(G(z)): 0.4057 / 0.4029\n",
      "8\n",
      "[8/20][0/9]\tLoss_D: 2.1711\tLoss_G: 1.3343\tD(x): 0.4374\tD(G(z)): 0.4354 / 0.4310\n",
      "9\n",
      "[9/20][0/9]\tLoss_D: 1.9993\tLoss_G: 1.3002\tD(x): 0.4759\tD(G(z)): 0.4343 / 0.4298\n",
      "10\n",
      "[10/20][0/9]\tLoss_D: 1.7478\tLoss_G: 1.3309\tD(x): 0.5101\tD(G(z)): 0.3936 / 0.3889\n",
      "11\n",
      "[11/20][0/9]\tLoss_D: 1.6563\tLoss_G: 1.2532\tD(x): 0.5433\tD(G(z)): 0.4114 / 0.4055\n",
      "12\n",
      "[12/20][0/9]\tLoss_D: 1.5297\tLoss_G: 1.3509\tD(x): 0.5722\tD(G(z)): 0.3849 / 0.3787\n",
      "13\n",
      "[13/20][0/9]\tLoss_D: 1.5452\tLoss_G: 1.2771\tD(x): 0.5972\tD(G(z)): 0.4268 / 0.4206\n",
      "14\n",
      "[14/20][0/9]\tLoss_D: 1.5653\tLoss_G: 1.3124\tD(x): 0.6145\tD(G(z)): 0.4292 / 0.4237\n",
      "15\n",
      "[15/20][0/9]\tLoss_D: 1.3755\tLoss_G: 1.3137\tD(x): 0.6303\tD(G(z)): 0.3950 / 0.3893\n",
      "16\n",
      "[16/20][0/9]\tLoss_D: 1.2753\tLoss_G: 1.3742\tD(x): 0.6445\tD(G(z)): 0.3769 / 0.3716\n",
      "17\n",
      "[17/20][0/9]\tLoss_D: 1.2989\tLoss_G: 1.4205\tD(x): 0.6547\tD(G(z)): 0.3715 / 0.3672\n",
      "18\n",
      "[18/20][0/9]\tLoss_D: 1.3710\tLoss_G: 1.3678\tD(x): 0.6652\tD(G(z)): 0.3903 / 0.3867\n",
      "19\n",
      "[19/20][0/9]\tLoss_D: 1.2818\tLoss_G: 1.1327\tD(x): 0.6722\tD(G(z)): 0.4312 / 0.4227\n",
      "95.546875\n"
     ]
    }
   ],
   "source": [
    "img_list = []\n",
    "G_losses = []\n",
    "D_losses = []\n",
    "iters = 0\n",
    "\n",
    "start = time.process_time()\n",
    "print(\"Starting Training Loop...\")\n",
    "# For each epoch\n",
    "for epoch in range(num_epochs):\n",
    "    print(epoch)\n",
    "    # For each batch in the dataloader\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "\n",
    "        ############################\n",
    "        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "        ###########################\n",
    "        ## Train with all-real batch\n",
    "        netD.zero_grad()\n",
    "        \n",
    "        real_cpu = data[0]\n",
    "        #print(real_cpu)\n",
    "        b_size = real_cpu.size(0)\n",
    "        # Format batch\n",
    "        label = torch.full((b_size,), real_label, dtype=torch.float)\n",
    "        # Forward pass real batch through D\n",
    "        output, _ = netD(real_cpu)\n",
    "        output = output.view(-1)\n",
    "        # Calculate loss on all-real batch\n",
    "        errD_real = criterion(output, label)\n",
    "        # Calculate gradients for D in backward pass\n",
    "        errD_real.backward()\n",
    "        D_x = output.mean().item()\n",
    "\n",
    "        ## Train with all-fake batch\n",
    "        # Generate batch of latent vectors\n",
    "        noise = torch.randn(b_size, input_dim)\n",
    "        # Generate fake image batch with G\n",
    "        fake = netG(noise)\n",
    "        label.fill_(fake_label)\n",
    "        # Classify all fake batch with D\n",
    "        output, _ = netD(fake.detach())\n",
    "        output = output.view(-1)\n",
    "        # Calculate D's loss on the all-fake batch\n",
    "        errD_fake = criterion(output, label)\n",
    "        #print(errD_fake)\n",
    "        # Calculate the gradients for this batch, accumulated (summed) with previous gradients\n",
    "        errD_fake.backward()\n",
    "        D_G_z1 = output.mean().item()\n",
    "        # Compute error of D as sum over the fake and the real batches\n",
    "        errD = errD_real + errD_fake\n",
    "        # Update D\n",
    "        optimizerD.step()\n",
    "\n",
    "        ############################\n",
    "        # (2) Update G network: maximize log(D(G(z)))\n",
    "        ###########################\n",
    "        netG.zero_grad()\n",
    "        label.fill_(real_label)  # fake labels are real for generator cost\n",
    "        # Since we just updated D, perform another forward pass of all-fake batch through D\n",
    "        output, _ = netD(fake)\n",
    "        output = output.view(-1)\n",
    "        \n",
    "        # Calculate G's loss based on this output\n",
    "        errG = criterion(output, label)\n",
    "        # Calculate gradients for G\n",
    "        errG.backward()\n",
    "        D_G_z2 = output.mean().item()\n",
    "        # Update G\n",
    "        optimizerG.step()\n",
    "\n",
    "        # Output training stats\n",
    "        if i % 50 == 0:\n",
    "            print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n",
    "                  % (epoch, num_epochs, i, len(train_loader),\n",
    "                     errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
    "\n",
    "        # Save Losses for plotting later\n",
    "        G_losses.append(errG.item())\n",
    "        D_losses.append(errD.item())\n",
    "\n",
    "        iters += 1\n",
    "        \n",
    "end = time.process_time()\n",
    "annthyroid_gan_train_time = end - start\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "32157f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test['y'] = y_test\n",
    "myDs=MyTestDataset(x_test)\n",
    "test_loader=torch.utils.data.DataLoader(myDs,batch_size=1,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "cb79ba94",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = netG\n",
    "discriminator = netD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "148d8fb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42.453125\n"
     ]
    }
   ],
   "source": [
    "start = time.process_time()\n",
    "losses = []\n",
    "for j,(image,label) in enumerate(test_loader):\n",
    "    z = init.normal_(torch.zeros(1,input_dim),mean=0,std=0.1)\n",
    "    z_optimizer = torch.optim.Adam([z],lr=1e-4)\n",
    "    \n",
    "    generator.eval()\n",
    "    gen_fake = generator(z)\n",
    "    loss = Anomaly_score(image,gen_fake)\n",
    "    losses.append(loss.detach().numpy().tolist())\n",
    "    \n",
    "threshold = np.mean(losses) + np.std(losses)\n",
    "anomaly_mask = pd.Series(losses) > threshold\n",
    "preds = anomaly_mask.map(lambda x: 1.0 if x == True else 0.0)\n",
    "preds = preds.tolist()\n",
    "\n",
    "x_test['y'] = y_test\n",
    "x_test['anomaly score'] = losses\n",
    "x_test['prediction'] = preds\n",
    "\n",
    "end = time.process_time()\n",
    "annthyroid_gan_test_time = end - start\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "99d3d4ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1621,  379],\n",
       "       [ 123,   37]], dtype=int64)"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(x_test['y'], x_test['prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "fcbd8d6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.51930625"
      ]
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fpr, tpr, _ = metrics.roc_curve(x_test['y'], x_test['anomaly score'])\n",
    "annthyroid_gan_auc = metrics.auc(fpr, tpr)\n",
    "metrics.auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "71370c6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.81      0.87      2000\n",
      "           1       0.09      0.23      0.13       160\n",
      "\n",
      "    accuracy                           0.77      2160\n",
      "   macro avg       0.51      0.52      0.50      2160\n",
      "weighted avg       0.87      0.77      0.81      2160\n",
      "\n"
     ]
    }
   ],
   "source": [
    "annthyroid_gan_report = classification_report(x_test['y'], x_test['prediction'], target_names = ['0','1'], output_dict = True)\n",
    "print(classification_report(x_test['y'], x_test['prediction'], target_names = ['0','1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "35c36751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0889423076923077\n",
      "0.23125\n",
      "0.12847222222222224\n"
     ]
    }
   ],
   "source": [
    "print(annthyroid_gan_report['1']['precision'])\n",
    "print(annthyroid_gan_report['1']['recall'])\n",
    "print(annthyroid_gan_report['1']['f1-score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "a04abb79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08312800128402117\n"
     ]
    }
   ],
   "source": [
    "precision, recall, thresholds = precision_recall_curve(x_test['y'], x_test['anomaly score'])\n",
    "annthyroid_gan_auc_precision_recall = metrics.auc(recall, precision)\n",
    "print(annthyroid_gan_auc_precision_recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c02c6cb-54b6-4c76-af0d-27bdcdfe0c35",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Credit card"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36c733b",
   "metadata": {},
   "source": [
    "**Dataset source**: https://www.kaggle.com/mlg-ulb/creditcardfraud\n",
    "\n",
    "**Additional sources:**\n",
    "\n",
    "Andrea Dal Pozzolo, Olivier Caelen, Reid A. Johnson and Gianluca Bontempi. Calibrating Probability with Undersampling for Unbalanced Classification. In Symposium on Computational Intelligence and Data Mining (CIDM), IEEE, 2015\n",
    "\n",
    "Dal Pozzolo, Andrea; Caelen, Olivier; Le Borgne, Yann-Ael; Waterschoot, Serge; Bontempi, Gianluca. Learned lessons in credit card fraud detection from a practitioner perspective, Expert systems with applications,41,10,4915-4928,2014, Pergamon\n",
    "\n",
    "Dal Pozzolo, Andrea; Boracchi, Giacomo; Caelen, Olivier; Alippi, Cesare; Bontempi, Gianluca. Credit card fraud detection: a realistic modeling and a novel learning strategy, IEEE transactions on neural networks and learning systems,29,8,3784-3797,2018,IEEE\n",
    "\n",
    "Dal Pozzolo, Andrea Adaptive Machine learning for credit card fraud detection ULB MLG PhD thesis (supervised by G. Bontempi)\n",
    "\n",
    "Carcillo, Fabrizio; Dal Pozzolo, Andrea; Le Borgne, Yann-Aël; Caelen, Olivier; Mazzer, Yannis; Bontempi, Gianluca. Scarff: a scalable framework for streaming credit card fraud detection with Spark, Information fusion,41, 182-194,2018,Elsevier\n",
    "\n",
    "Carcillo, Fabrizio; Le Borgne, Yann-Aël; Caelen, Olivier; Bontempi, Gianluca. Streaming active learning strategies for real-life credit card fraud detection: assessment and visualization, International Journal of Data Science and Analytics, 5,4,285-300,2018,Springer International Publishing\n",
    "\n",
    "Bertrand Lebichot, Yann-Aël Le Borgne, Liyun He, Frederic Oblé, Gianluca Bontempi Deep-Learning Domain Adaptation Techniques for Credit Cards Fraud Detection, INNSBDDL 2019: Recent Advances in Big Data and Deep Learning, pp 78-88, 2019\n",
    "\n",
    "Fabrizio Carcillo, Yann-Aël Le Borgne, Olivier Caelen, Frederic Oblé, Gianluca Bontempi Combining Unsupervised and Supervised Learning in Credit Card Fraud Detection Information Sciences, 2019\n",
    "\n",
    "Yann-Aël Le Borgne, Gianluca Bontempi Machine Learning for Credit Card Fraud Detection - Practical Handbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "abaad9a8-e22d-40a5-9e2b-1611697c48c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./creditcard.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "f805c4b2-f0cf-4b91-bd69-60bb6acf57d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(columns = ['Time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "c53f52a7-5385-452d-b93e-0526c104f5c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(284807, 30)"
      ]
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "64fd85a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>0.090794</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>-0.166974</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>0.207643</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>-0.054952</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>0.753074</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9       V10  ...       V21       V22       V23       V24  \\\n",
       "0  0.098698  0.363787  0.090794  ... -0.018307  0.277838 -0.110474  0.066928   \n",
       "1  0.085102 -0.255425 -0.166974  ... -0.225775 -0.638672  0.101288 -0.339846   \n",
       "2  0.247676 -1.514654  0.207643  ...  0.247998  0.771679  0.909412 -0.689281   \n",
       "3  0.377436 -1.387024 -0.054952  ... -0.108300  0.005274 -0.190321 -1.175575   \n",
       "4 -0.270533  0.817739  0.753074  ... -0.009431  0.798278 -0.137458  0.141267   \n",
       "\n",
       "        V25       V26       V27       V28  Amount  Class  \n",
       "0  0.128539 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1  0.167170  0.125895 -0.008983  0.014724    2.69      0  \n",
       "2 -0.327642 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "3  0.647376 -0.221929  0.062723  0.061458  123.50      0  \n",
       "4 -0.206010  0.502292  0.219422  0.215153   69.99      0  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "ff70cce9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>284315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>492</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           V1\n",
       "Class        \n",
       "0      284315\n",
       "1         492"
      ]
     },
     "execution_count": 365,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.pivot_table(data,\n",
    "             values = 'V1',\n",
    "               index = 'Class', \n",
    "              aggfunc = 'count')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8859c456",
   "metadata": {},
   "source": [
    "## AnoGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "f297d672",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "batch_size = 512\n",
    "learning_rate = 0.0002\n",
    "input_dim = data.shape[1]-1\n",
    "num_epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "7be311f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    x_train,\n",
    "    x_test,\n",
    "    y_train,\n",
    "    y_test,\n",
    "    indices_train,\n",
    "    indices_test,\n",
    ") = train_test_split(data[data.columns[:-1]], data['Class'], data.index, test_size=0.3,stratify=data[['Class']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "2681280a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = pd.DataFrame(y_train)\n",
    "x_train = x_train.loc[y_train[y_train.Class == 0].index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "id": "28de7b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "myDs=MyTrainDataset(x_train)\n",
    "train_loader=torch.utils.data.DataLoader(myDs,batch_size=batch_size,shuffle=False, drop_last = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "id": "e1fe501c",
   "metadata": {},
   "outputs": [],
   "source": [
    "netG = Generator()\n",
    "netD = Discriminator()\n",
    "\n",
    "# Initialize BCELoss function\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Create batch of latent vectors that we will use to visualize\n",
    "#  the progression of the generator\n",
    "fixed_noise = torch.randn(batch_size, input_dim)\n",
    "\n",
    "# Establish convention for real and fake labels during training\n",
    "real_label = 1.\n",
    "fake_label = 0.\n",
    "\n",
    "# Setup Adam optimizers for both G and D\n",
    "optimizerD = torch.optim.Adam(netD.parameters(), lr=learning_rate, betas=(0.5, 0.999))\n",
    "optimizerG = torch.optim.Adam(netG.parameters(), lr=learning_rate, betas=(0.5, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "id": "f0e1360a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training Loop...\n",
      "0\n",
      "[0/20][0/388]\tLoss_D: 10.7736\tLoss_G: 0.5119\tD(x): 0.5789\tD(G(z)): 0.8313 / 0.8304\n",
      "[0/20][50/388]\tLoss_D: 5.0215\tLoss_G: 0.2715\tD(x): 0.8593\tD(G(z)): 0.8609 / 0.8588\n",
      "[0/20][100/388]\tLoss_D: 3.4959\tLoss_G: 0.2883\tD(x): 0.8702\tD(G(z)): 0.8266 / 0.8235\n",
      "[0/20][150/388]\tLoss_D: 2.4650\tLoss_G: 0.3741\tD(x): 0.8472\tD(G(z)): 0.7630 / 0.7579\n",
      "[0/20][200/388]\tLoss_D: 1.7207\tLoss_G: 0.5730\tD(x): 0.8646\tD(G(z)): 0.6564 / 0.6494\n",
      "[0/20][250/388]\tLoss_D: 1.2850\tLoss_G: 0.8452\tD(x): 0.8346\tD(G(z)): 0.5365 / 0.5292\n",
      "[0/20][300/388]\tLoss_D: 1.0199\tLoss_G: 0.9314\tD(x): 0.8952\tD(G(z)): 0.4987 / 0.4868\n",
      "[0/20][350/388]\tLoss_D: 0.7815\tLoss_G: 1.1666\tD(x): 0.9440\tD(G(z)): 0.4231 / 0.4123\n",
      "1\n",
      "[1/20][0/388]\tLoss_D: 0.9238\tLoss_G: 1.2149\tD(x): 0.8017\tD(G(z)): 0.3999 / 0.3884\n",
      "[1/20][50/388]\tLoss_D: 0.7857\tLoss_G: 1.3266\tD(x): 0.8498\tD(G(z)): 0.3884 / 0.3730\n",
      "[1/20][100/388]\tLoss_D: 0.5978\tLoss_G: 1.5549\tD(x): 0.8773\tD(G(z)): 0.3111 / 0.2994\n",
      "[1/20][150/388]\tLoss_D: 0.7832\tLoss_G: 1.6711\tD(x): 0.7302\tD(G(z)): 0.2805 / 0.2709\n",
      "[1/20][200/388]\tLoss_D: 0.6001\tLoss_G: 1.6307\tD(x): 0.8735\tD(G(z)): 0.3092 / 0.2923\n",
      "[1/20][250/388]\tLoss_D: 0.6524\tLoss_G: 1.7439\tD(x): 0.7950\tD(G(z)): 0.2653 / 0.2565\n",
      "[1/20][300/388]\tLoss_D: 0.4561\tLoss_G: 1.8549\tD(x): 0.9115\tD(G(z)): 0.2461 / 0.2399\n",
      "[1/20][350/388]\tLoss_D: 0.5713\tLoss_G: 2.0498\tD(x): 0.9557\tD(G(z)): 0.2328 / 0.2228\n",
      "2\n",
      "[2/20][0/388]\tLoss_D: 0.6951\tLoss_G: 2.0492\tD(x): 0.7347\tD(G(z)): 0.2164 / 0.2105\n",
      "[2/20][50/388]\tLoss_D: 0.4750\tLoss_G: 2.1353\tD(x): 0.8233\tD(G(z)): 0.2108 / 0.2020\n",
      "[2/20][100/388]\tLoss_D: 0.3741\tLoss_G: 2.5056\tD(x): 0.8636\tD(G(z)): 0.1674 / 0.1599\n",
      "[2/20][150/388]\tLoss_D: 0.6822\tLoss_G: 2.2807\tD(x): 0.7082\tD(G(z)): 0.1865 / 0.1818\n",
      "[2/20][200/388]\tLoss_D: 0.4012\tLoss_G: 2.3475\tD(x): 0.8862\tD(G(z)): 0.1862 / 0.1763\n",
      "[2/20][250/388]\tLoss_D: 0.5251\tLoss_G: 2.4942\tD(x): 0.8258\tD(G(z)): 0.1789 / 0.1710\n",
      "[2/20][300/388]\tLoss_D: 0.2639\tLoss_G: 2.4564\tD(x): 0.9359\tD(G(z)): 0.1533 / 0.1450\n",
      "[2/20][350/388]\tLoss_D: 0.1991\tLoss_G: 2.6841\tD(x): 0.9668\tD(G(z)): 0.1216 / 0.1167\n",
      "3\n",
      "[3/20][0/388]\tLoss_D: 0.4664\tLoss_G: 2.5710\tD(x): 0.7712\tD(G(z)): 0.1399 / 0.1294\n",
      "[3/20][50/388]\tLoss_D: 0.3126\tLoss_G: 2.9460\tD(x): 0.8630\tD(G(z)): 0.1146 / 0.1085\n",
      "[3/20][100/388]\tLoss_D: 0.2539\tLoss_G: 2.9037\tD(x): 0.8979\tD(G(z)): 0.1239 / 0.1106\n",
      "[3/20][150/388]\tLoss_D: 0.5134\tLoss_G: 3.1082\tD(x): 0.7432\tD(G(z)): 0.1065 / 0.1016\n",
      "[3/20][200/388]\tLoss_D: 0.2638\tLoss_G: 3.0993\tD(x): 0.9168\tD(G(z)): 0.1131 / 0.1042\n",
      "[3/20][250/388]\tLoss_D: 0.2556\tLoss_G: 3.0503\tD(x): 0.8771\tD(G(z)): 0.0988 / 0.0873\n",
      "[3/20][300/388]\tLoss_D: 0.1857\tLoss_G: 3.2409\tD(x): 0.9620\tD(G(z)): 0.1119 / 0.0901\n",
      "[3/20][350/388]\tLoss_D: 0.1021\tLoss_G: 3.4002\tD(x): 0.9765\tD(G(z)): 0.0725 / 0.0656\n",
      "4\n",
      "[4/20][0/388]\tLoss_D: 0.3117\tLoss_G: 3.3018\tD(x): 0.8220\tD(G(z)): 0.0858 / 0.0779\n",
      "[4/20][50/388]\tLoss_D: 0.2405\tLoss_G: 3.0506\tD(x): 0.9001\tD(G(z)): 0.1165 / 0.1019\n",
      "[4/20][100/388]\tLoss_D: 0.1685\tLoss_G: 3.5040\tD(x): 0.9256\tD(G(z)): 0.0777 / 0.0698\n",
      "[4/20][150/388]\tLoss_D: 0.3603\tLoss_G: 3.3890\tD(x): 0.7919\tD(G(z)): 0.0765 / 0.0686\n",
      "[4/20][200/388]\tLoss_D: 0.1683\tLoss_G: 3.4044\tD(x): 0.9395\tD(G(z)): 0.0855 / 0.0781\n",
      "[4/20][250/388]\tLoss_D: 0.1907\tLoss_G: 3.4973\tD(x): 0.9033\tD(G(z)): 0.0710 / 0.0669\n",
      "[4/20][300/388]\tLoss_D: 0.1352\tLoss_G: 3.3335\tD(x): 0.9728\tD(G(z)): 0.0914 / 0.0840\n",
      "[4/20][350/388]\tLoss_D: 0.1018\tLoss_G: 3.4562\tD(x): 0.9826\tD(G(z)): 0.0747 / 0.0694\n",
      "5\n",
      "[5/20][0/388]\tLoss_D: 0.2583\tLoss_G: 3.2729\tD(x): 0.8616\tD(G(z)): 0.0859 / 0.0810\n",
      "[5/20][50/388]\tLoss_D: 0.1728\tLoss_G: 3.5628\tD(x): 0.9156\tD(G(z)): 0.0745 / 0.0699\n",
      "[5/20][100/388]\tLoss_D: 0.1556\tLoss_G: 3.6812\tD(x): 0.9324\tD(G(z)): 0.0774 / 0.0687\n",
      "[5/20][150/388]\tLoss_D: 0.2963\tLoss_G: 3.8458\tD(x): 0.8085\tD(G(z)): 0.0508 / 0.0497\n",
      "[5/20][200/388]\tLoss_D: 0.1707\tLoss_G: 3.2657\tD(x): 0.9464\tD(G(z)): 0.0920 / 0.0810\n",
      "[5/20][250/388]\tLoss_D: 0.1671\tLoss_G: 3.3731\tD(x): 0.9169\tD(G(z)): 0.0725 / 0.0678\n",
      "[5/20][300/388]\tLoss_D: 0.0790\tLoss_G: 3.7838\tD(x): 0.9750\tD(G(z)): 0.0509 / 0.0475\n",
      "[5/20][350/388]\tLoss_D: 0.1009\tLoss_G: 3.5275\tD(x): 0.9843\tD(G(z)): 0.0743 / 0.0672\n",
      "6\n",
      "[6/20][0/388]\tLoss_D: 0.2586\tLoss_G: 3.4499\tD(x): 0.8724\tD(G(z)): 0.0804 / 0.0710\n",
      "[6/20][50/388]\tLoss_D: 0.1944\tLoss_G: 4.0492\tD(x): 0.9259\tD(G(z)): 0.0955 / 0.0844\n",
      "[6/20][100/388]\tLoss_D: 0.1659\tLoss_G: 4.0544\tD(x): 0.9226\tD(G(z)): 0.0761 / 0.0679\n",
      "[6/20][150/388]\tLoss_D: 0.3626\tLoss_G: 3.5756\tD(x): 0.7949\tD(G(z)): 0.0955 / 0.0849\n",
      "[6/20][200/388]\tLoss_D: 0.1149\tLoss_G: 3.5791\tD(x): 0.9524\tD(G(z)): 0.0588 / 0.0546\n",
      "[6/20][250/388]\tLoss_D: 0.1146\tLoss_G: 3.6471\tD(x): 0.9306\tD(G(z)): 0.0395 / 0.0384\n",
      "[6/20][300/388]\tLoss_D: 0.0818\tLoss_G: 3.8454\tD(x): 0.9822\tD(G(z)): 0.0577 / 0.0503\n",
      "[6/20][350/388]\tLoss_D: 0.0833\tLoss_G: 3.7281\tD(x): 0.9878\tD(G(z)): 0.0569 / 0.0446\n",
      "7\n",
      "[7/20][0/388]\tLoss_D: 0.1609\tLoss_G: 3.6450\tD(x): 0.9030\tD(G(z)): 0.0493 / 0.0447\n",
      "[7/20][50/388]\tLoss_D: 0.1222\tLoss_G: 3.4822\tD(x): 0.9539\tD(G(z)): 0.0512 / 0.0483\n",
      "[7/20][100/388]\tLoss_D: 0.0890\tLoss_G: 3.6723\tD(x): 0.9600\tD(G(z)): 0.0453 / 0.0414\n",
      "[7/20][150/388]\tLoss_D: 0.1777\tLoss_G: 3.6358\tD(x): 0.8799\tD(G(z)): 0.0400 / 0.0376\n",
      "[7/20][200/388]\tLoss_D: 0.0766\tLoss_G: 3.6759\tD(x): 0.9741\tD(G(z)): 0.0478 / 0.0447\n",
      "[7/20][250/388]\tLoss_D: 0.0840\tLoss_G: 4.0200\tD(x): 0.9537\tD(G(z)): 0.0340 / 0.0309\n",
      "[7/20][300/388]\tLoss_D: 0.0575\tLoss_G: 3.9497\tD(x): 0.9873\tD(G(z)): 0.0417 / 0.0371\n",
      "[7/20][350/388]\tLoss_D: 0.0557\tLoss_G: 3.8424\tD(x): 0.9918\tD(G(z)): 0.0441 / 0.0383\n",
      "8\n",
      "[8/20][0/388]\tLoss_D: 0.1336\tLoss_G: 3.8786\tD(x): 0.9225\tD(G(z)): 0.0414 / 0.0398\n",
      "[8/20][50/388]\tLoss_D: 0.0745\tLoss_G: 3.7788\tD(x): 0.9644\tD(G(z)): 0.0348 / 0.0331\n",
      "[8/20][100/388]\tLoss_D: 0.0896\tLoss_G: 3.7855\tD(x): 0.9651\tD(G(z)): 0.0506 / 0.0444\n",
      "[8/20][150/388]\tLoss_D: 0.1935\tLoss_G: 3.8502\tD(x): 0.8704\tD(G(z)): 0.0433 / 0.0410\n",
      "[8/20][200/388]\tLoss_D: 0.0802\tLoss_G: 3.7983\tD(x): 0.9711\tD(G(z)): 0.0479 / 0.0419\n",
      "[8/20][250/388]\tLoss_D: 0.1460\tLoss_G: 3.3828\tD(x): 0.9387\tD(G(z)): 0.0710 / 0.0615\n",
      "[8/20][300/388]\tLoss_D: 0.0842\tLoss_G: 3.8816\tD(x): 0.9832\tD(G(z)): 0.0606 / 0.0451\n",
      "[8/20][350/388]\tLoss_D: 0.0711\tLoss_G: 3.5104\tD(x): 0.9896\tD(G(z)): 0.0577 / 0.0513\n",
      "9\n",
      "[9/20][0/388]\tLoss_D: 0.1637\tLoss_G: 3.8939\tD(x): 0.9040\tD(G(z)): 0.0543 / 0.0478\n",
      "[9/20][50/388]\tLoss_D: 0.1293\tLoss_G: 3.2738\tD(x): 0.9556\tD(G(z)): 0.0779 / 0.0704\n",
      "[9/20][100/388]\tLoss_D: 0.1098\tLoss_G: 3.7380\tD(x): 0.9561\tD(G(z)): 0.0611 / 0.0559\n",
      "[9/20][150/388]\tLoss_D: 0.2132\tLoss_G: 4.3614\tD(x): 0.8426\tD(G(z)): 0.0292 / 0.0283\n",
      "[9/20][200/388]\tLoss_D: 0.0745\tLoss_G: 4.3275\tD(x): 0.9674\tD(G(z)): 0.0391 / 0.0342\n",
      "[9/20][250/388]\tLoss_D: 0.0848\tLoss_G: 4.1620\tD(x): 0.9533\tD(G(z)): 0.0347 / 0.0317\n",
      "[9/20][300/388]\tLoss_D: 0.0490\tLoss_G: 3.9960\tD(x): 0.9882\tD(G(z)): 0.0355 / 0.0339\n",
      "[9/20][350/388]\tLoss_D: 0.0407\tLoss_G: 4.2389\tD(x): 0.9924\tD(G(z)): 0.0314 / 0.0294\n",
      "10\n",
      "[10/20][0/388]\tLoss_D: 0.1157\tLoss_G: 3.8215\tD(x): 0.9305\tD(G(z)): 0.0399 / 0.0374\n",
      "[10/20][50/388]\tLoss_D: 0.0658\tLoss_G: 3.9859\tD(x): 0.9713\tD(G(z)): 0.0354 / 0.0321\n",
      "[10/20][100/388]\tLoss_D: 0.0783\tLoss_G: 3.8375\tD(x): 0.9688\tD(G(z)): 0.0439 / 0.0404\n",
      "[10/20][150/388]\tLoss_D: 0.2236\tLoss_G: 4.3847\tD(x): 0.8771\tD(G(z)): 0.0684 / 0.0550\n",
      "[10/20][200/388]\tLoss_D: 0.0817\tLoss_G: 4.2624\tD(x): 0.9677\tD(G(z)): 0.0448 / 0.0388\n",
      "[10/20][250/388]\tLoss_D: 0.0925\tLoss_G: 4.7746\tD(x): 0.9587\tD(G(z)): 0.0372 / 0.0303\n",
      "[10/20][300/388]\tLoss_D: 0.0533\tLoss_G: 4.7051\tD(x): 0.9889\tD(G(z)): 0.0389 / 0.0335\n",
      "[10/20][350/388]\tLoss_D: 0.0454\tLoss_G: 4.6334\tD(x): 0.9903\tD(G(z)): 0.0342 / 0.0313\n",
      "11\n",
      "[11/20][0/388]\tLoss_D: 0.1064\tLoss_G: 4.4520\tD(x): 0.9324\tD(G(z)): 0.0318 / 0.0298\n",
      "[11/20][50/388]\tLoss_D: 0.0498\tLoss_G: 4.8025\tD(x): 0.9719\tD(G(z)): 0.0206 / 0.0189\n",
      "[11/20][100/388]\tLoss_D: 0.0544\tLoss_G: 4.3657\tD(x): 0.9728\tD(G(z)): 0.0261 / 0.0241\n",
      "[11/20][150/388]\tLoss_D: 0.1453\tLoss_G: 3.9080\tD(x): 0.9049\tD(G(z)): 0.0391 / 0.0349\n",
      "[11/20][200/388]\tLoss_D: 0.0356\tLoss_G: 4.5615\tD(x): 0.9825\tD(G(z)): 0.0174 / 0.0165\n",
      "[11/20][250/388]\tLoss_D: 0.0530\tLoss_G: 4.3616\tD(x): 0.9725\tD(G(z)): 0.0242 / 0.0231\n",
      "[11/20][300/388]\tLoss_D: 0.0391\tLoss_G: 3.9473\tD(x): 0.9921\tD(G(z)): 0.0305 / 0.0278\n",
      "[11/20][350/388]\tLoss_D: 0.0376\tLoss_G: 3.9997\tD(x): 0.9943\tD(G(z)): 0.0311 / 0.0287\n",
      "12\n",
      "[12/20][0/388]\tLoss_D: 0.1004\tLoss_G: 4.4356\tD(x): 0.9464\tD(G(z)): 0.0417 / 0.0371\n",
      "[12/20][50/388]\tLoss_D: 0.0581\tLoss_G: 4.7135\tD(x): 0.9801\tD(G(z)): 0.0351 / 0.0311\n",
      "[12/20][100/388]\tLoss_D: 0.0340\tLoss_G: 5.2689\tD(x): 0.9791\tD(G(z)): 0.0126 / 0.0123\n",
      "[12/20][150/388]\tLoss_D: 0.1207\tLoss_G: 4.6718\tD(x): 0.9258\tD(G(z)): 0.0380 / 0.0340\n",
      "[12/20][200/388]\tLoss_D: 0.0375\tLoss_G: 4.9849\tD(x): 0.9852\tD(G(z)): 0.0210 / 0.0185\n",
      "[12/20][250/388]\tLoss_D: 0.0445\tLoss_G: 4.7837\tD(x): 0.9759\tD(G(z)): 0.0195 / 0.0187\n",
      "[12/20][300/388]\tLoss_D: 0.0551\tLoss_G: 4.4756\tD(x): 0.9927\tD(G(z)): 0.0414 / 0.0344\n",
      "[12/20][350/388]\tLoss_D: 0.0462\tLoss_G: 5.5262\tD(x): 0.9953\tD(G(z)): 0.0249 / 0.0138\n",
      "13\n",
      "[13/20][0/388]\tLoss_D: 0.0943\tLoss_G: 4.6001\tD(x): 0.9562\tD(G(z)): 0.0387 / 0.0329\n",
      "[13/20][50/388]\tLoss_D: 0.0683\tLoss_G: 5.1555\tD(x): 0.9826\tD(G(z)): 0.0281 / 0.0214\n",
      "[13/20][100/388]\tLoss_D: 0.0350\tLoss_G: 4.9478\tD(x): 0.9825\tD(G(z)): 0.0169 / 0.0160\n",
      "[13/20][150/388]\tLoss_D: 0.1174\tLoss_G: 4.6603\tD(x): 0.9265\tD(G(z)): 0.0210 / 0.0204\n",
      "[13/20][200/388]\tLoss_D: 0.0888\tLoss_G: 4.5418\tD(x): 0.9807\tD(G(z)): 0.0461 / 0.0301\n",
      "[13/20][250/388]\tLoss_D: 0.1267\tLoss_G: 4.6160\tD(x): 0.9717\tD(G(z)): 0.0399 / 0.0331\n",
      "[13/20][300/388]\tLoss_D: 0.0289\tLoss_G: 5.1310\tD(x): 0.9920\tD(G(z)): 0.0203 / 0.0197\n",
      "[13/20][350/388]\tLoss_D: 0.0227\tLoss_G: 5.2878\tD(x): 0.9936\tD(G(z)): 0.0161 / 0.0155\n",
      "14\n",
      "[14/20][0/388]\tLoss_D: 0.1058\tLoss_G: 4.9157\tD(x): 0.9436\tD(G(z)): 0.0429 / 0.0363\n",
      "[14/20][50/388]\tLoss_D: 0.0698\tLoss_G: 4.8695\tD(x): 0.9804\tD(G(z)): 0.0445 / 0.0392\n",
      "[14/20][100/388]\tLoss_D: 0.1045\tLoss_G: 4.4175\tD(x): 0.9688\tD(G(z)): 0.0584 / 0.0437\n",
      "[14/20][150/388]\tLoss_D: 0.3205\tLoss_G: 3.8372\tD(x): 0.8011\tD(G(z)): 0.0712 / 0.0640\n",
      "[14/20][200/388]\tLoss_D: 0.0792\tLoss_G: 4.6342\tD(x): 0.9578\tD(G(z)): 0.0339 / 0.0320\n",
      "[14/20][250/388]\tLoss_D: 0.1396\tLoss_G: 3.5144\tD(x): 0.9384\tD(G(z)): 0.0686 / 0.0652\n",
      "[14/20][300/388]\tLoss_D: 0.1084\tLoss_G: 4.0462\tD(x): 0.9839\tD(G(z)): 0.0559 / 0.0505\n",
      "[14/20][350/388]\tLoss_D: 0.1309\tLoss_G: 3.0475\tD(x): 0.9902\tD(G(z)): 0.0993 / 0.0876\n",
      "15\n",
      "[15/20][0/388]\tLoss_D: 0.3037\tLoss_G: 3.1605\tD(x): 0.9041\tD(G(z)): 0.1689 / 0.1324\n",
      "[15/20][50/388]\tLoss_D: 0.1843\tLoss_G: 3.7297\tD(x): 0.9552\tD(G(z)): 0.0934 / 0.0842\n",
      "[15/20][100/388]\tLoss_D: 0.1148\tLoss_G: 4.3862\tD(x): 0.9654\tD(G(z)): 0.0661 / 0.0595\n",
      "[15/20][150/388]\tLoss_D: 0.2734\tLoss_G: 4.6718\tD(x): 0.8292\tD(G(z)): 0.0431 / 0.0425\n",
      "[15/20][200/388]\tLoss_D: 0.1526\tLoss_G: 3.6155\tD(x): 0.9471\tD(G(z)): 0.0780 / 0.0688\n",
      "[15/20][250/388]\tLoss_D: 0.2021\tLoss_G: 4.1154\tD(x): 0.9480\tD(G(z)): 0.0544 / 0.0521\n",
      "[15/20][300/388]\tLoss_D: 0.1168\tLoss_G: 3.1561\tD(x): 0.9861\tD(G(z)): 0.0915 / 0.0719\n",
      "[15/20][350/388]\tLoss_D: 0.1342\tLoss_G: 3.2030\tD(x): 0.9929\tD(G(z)): 0.1133 / 0.0834\n",
      "16\n",
      "[16/20][0/388]\tLoss_D: 0.1575\tLoss_G: 5.2087\tD(x): 0.8877\tD(G(z)): 0.0311 / 0.0314\n",
      "[16/20][50/388]\tLoss_D: 0.0629\tLoss_G: 5.0107\tD(x): 0.9603\tD(G(z)): 0.0205 / 0.0200\n",
      "[16/20][100/388]\tLoss_D: 0.1145\tLoss_G: 3.9325\tD(x): 0.9675\tD(G(z)): 0.0726 / 0.0612\n",
      "[16/20][150/388]\tLoss_D: 0.2993\tLoss_G: 5.0082\tD(x): 0.7934\tD(G(z)): 0.0299 / 0.0316\n",
      "[16/20][200/388]\tLoss_D: 0.3328\tLoss_G: 3.3609\tD(x): 0.9602\tD(G(z)): 0.0806 / 0.0722\n",
      "[16/20][250/388]\tLoss_D: 0.2253\tLoss_G: 2.8449\tD(x): 0.9270\tD(G(z)): 0.1154 / 0.0963\n",
      "[16/20][300/388]\tLoss_D: 0.1936\tLoss_G: 2.8962\tD(x): 0.9871\tD(G(z)): 0.1585 / 0.1193\n",
      "[16/20][350/388]\tLoss_D: 0.1102\tLoss_G: 3.1986\tD(x): 0.9955\tD(G(z)): 0.0894 / 0.0760\n",
      "17\n",
      "[17/20][0/388]\tLoss_D: 0.1426\tLoss_G: 3.9827\tD(x): 0.9090\tD(G(z)): 0.0406 / 0.0388\n",
      "[17/20][50/388]\tLoss_D: 0.0832\tLoss_G: 4.2687\tD(x): 0.9780\tD(G(z)): 0.0559 / 0.0479\n",
      "[17/20][100/388]\tLoss_D: 0.1133\tLoss_G: 3.5703\tD(x): 0.9643\tD(G(z)): 0.0674 / 0.0624\n",
      "[17/20][150/388]\tLoss_D: 0.5263\tLoss_G: 2.2969\tD(x): 0.7897\tD(G(z)): 0.1840 / 0.1520\n",
      "[17/20][200/388]\tLoss_D: 0.1137\tLoss_G: 4.5002\tD(x): 0.9578\tD(G(z)): 0.0462 / 0.0409\n",
      "[17/20][250/388]\tLoss_D: 0.1338\tLoss_G: 3.9851\tD(x): 0.9331\tD(G(z)): 0.0392 / 0.0362\n",
      "[17/20][300/388]\tLoss_D: 0.2033\tLoss_G: 2.5855\tD(x): 0.9888\tD(G(z)): 0.1585 / 0.1174\n",
      "[17/20][350/388]\tLoss_D: 0.0741\tLoss_G: 3.5587\tD(x): 0.9965\tD(G(z)): 0.0629 / 0.0564\n",
      "18\n",
      "[18/20][0/388]\tLoss_D: 0.1122\tLoss_G: 4.4601\tD(x): 0.9234\tD(G(z)): 0.0291 / 0.0283\n",
      "[18/20][50/388]\tLoss_D: 0.0997\tLoss_G: 3.9395\tD(x): 0.9660\tD(G(z)): 0.0431 / 0.0448\n",
      "[18/20][100/388]\tLoss_D: 0.1685\tLoss_G: 3.0812\tD(x): 0.9646\tD(G(z)): 0.1118 / 0.1019\n",
      "[18/20][150/388]\tLoss_D: 0.3349\tLoss_G: 3.7000\tD(x): 0.8061\tD(G(z)): 0.0553 / 0.0552\n",
      "[18/20][200/388]\tLoss_D: 0.0616\tLoss_G: 8.4209\tD(x): 0.9616\tD(G(z)): 0.0076 / 0.0058\n",
      "[18/20][250/388]\tLoss_D: 0.1169\tLoss_G: 3.4828\tD(x): 0.9493\tD(G(z)): 0.0605 / 0.0600\n",
      "[18/20][300/388]\tLoss_D: 0.0217\tLoss_G: 5.7477\tD(x): 0.9918\tD(G(z)): 0.0117 / 0.0106\n",
      "[18/20][350/388]\tLoss_D: 0.0224\tLoss_G: 5.2099\tD(x): 0.9967\tD(G(z)): 0.0183 / 0.0172\n",
      "19\n",
      "[19/20][0/388]\tLoss_D: 0.2799\tLoss_G: 3.9400\tD(x): 0.8079\tD(G(z)): 0.0510 / 0.0525\n",
      "[19/20][50/388]\tLoss_D: 0.0598\tLoss_G: 5.6141\tD(x): 0.9615\tD(G(z)): 0.0177 / 0.0193\n",
      "[19/20][100/388]\tLoss_D: 0.0757\tLoss_G: 4.3251\tD(x): 0.9757\tD(G(z)): 0.0426 / 0.0307\n",
      "[19/20][150/388]\tLoss_D: 0.4380\tLoss_G: 4.4213\tD(x): 0.7086\tD(G(z)): 0.0367 / 0.0277\n",
      "[19/20][200/388]\tLoss_D: 0.1371\tLoss_G: 3.4103\tD(x): 0.9662\tD(G(z)): 0.0898 / 0.0769\n",
      "[19/20][250/388]\tLoss_D: 0.1649\tLoss_G: 3.0986\tD(x): 0.9408\tD(G(z)): 0.0890 / 0.0852\n",
      "[19/20][300/388]\tLoss_D: 0.0702\tLoss_G: 3.6675\tD(x): 0.9905\tD(G(z)): 0.0475 / 0.0410\n",
      "[19/20][350/388]\tLoss_D: 0.0325\tLoss_G: 5.7577\tD(x): 0.9958\tD(G(z)): 0.0194 / 0.0102\n",
      "3683.5\n"
     ]
    }
   ],
   "source": [
    "img_list = []\n",
    "G_losses = []\n",
    "D_losses = []\n",
    "iters = 0\n",
    "\n",
    "start = time.process_time()\n",
    "print(\"Starting Training Loop...\")\n",
    "# For each epoch\n",
    "for epoch in range(num_epochs):\n",
    "    print(epoch)\n",
    "    # For each batch in the dataloader\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "\n",
    "        ############################\n",
    "        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "        ###########################\n",
    "        ## Train with all-real batch\n",
    "        netD.zero_grad()\n",
    "        \n",
    "        real_cpu = data[0]\n",
    "        #print(real_cpu)\n",
    "        b_size = real_cpu.size(0)\n",
    "        # Format batch\n",
    "        label = torch.full((b_size,), real_label, dtype=torch.float)\n",
    "        # Forward pass real batch through D\n",
    "        output, _ = netD(real_cpu)\n",
    "        output = output.view(-1)\n",
    "        # Calculate loss on all-real batch\n",
    "        errD_real = criterion(output, label)\n",
    "        # Calculate gradients for D in backward pass\n",
    "        errD_real.backward()\n",
    "        D_x = output.mean().item()\n",
    "\n",
    "        ## Train with all-fake batch\n",
    "        # Generate batch of latent vectors\n",
    "        noise = torch.randn(b_size, input_dim)\n",
    "        # Generate fake image batch with G\n",
    "        fake = netG(noise)\n",
    "        label.fill_(fake_label)\n",
    "        # Classify all fake batch with D\n",
    "        output, _ = netD(fake.detach())\n",
    "        output = output.view(-1)\n",
    "        # Calculate D's loss on the all-fake batch\n",
    "        errD_fake = criterion(output, label)\n",
    "        #print(errD_fake)\n",
    "        # Calculate the gradients for this batch, accumulated (summed) with previous gradients\n",
    "        errD_fake.backward()\n",
    "        D_G_z1 = output.mean().item()\n",
    "        # Compute error of D as sum over the fake and the real batches\n",
    "        errD = errD_real + errD_fake\n",
    "        # Update D\n",
    "        optimizerD.step()\n",
    "\n",
    "        ############################\n",
    "        # (2) Update G network: maximize log(D(G(z)))\n",
    "        ###########################\n",
    "        netG.zero_grad()\n",
    "        label.fill_(real_label)  # fake labels are real for generator cost\n",
    "        # Since we just updated D, perform another forward pass of all-fake batch through D\n",
    "        output, _ = netD(fake)\n",
    "        output = output.view(-1)\n",
    "        \n",
    "        # Calculate G's loss based on this output\n",
    "        errG = criterion(output, label)\n",
    "        # Calculate gradients for G\n",
    "        errG.backward()\n",
    "        D_G_z2 = output.mean().item()\n",
    "        # Update G\n",
    "        optimizerG.step()\n",
    "\n",
    "        # Output training stats\n",
    "        if i % 50 == 0:\n",
    "            print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n",
    "                  % (epoch, num_epochs, i, len(train_loader),\n",
    "                     errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
    "\n",
    "        # Save Losses for plotting later\n",
    "        G_losses.append(errG.item())\n",
    "        D_losses.append(errD.item())\n",
    "\n",
    "        # Check how the generator is doing by saving G's output on fixed_noise\n",
    "        if (iters % 500 == 0) or ((epoch == num_epochs-1) and (i == len(train_loader)-1)):\n",
    "            with torch.no_grad():\n",
    "                fake = netG(fixed_noise).detach().cpu()\n",
    "            img_list.append(vutils.make_grid(fake, padding=2, normalize=True))\n",
    "\n",
    "        iters += 1\n",
    "        \n",
    "end = time.process_time()\n",
    "creditcard_gan_train_time = end - start\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "id": "69195877",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test['y'] = y_test\n",
    "myDs=MyTestDataset(x_test)\n",
    "test_loader=torch.utils.data.DataLoader(myDs,batch_size=1,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "a73b1a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = netG\n",
    "discriminator = netD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "id": "b15319cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1392.734375\n"
     ]
    }
   ],
   "source": [
    "start = time.process_time()\n",
    "losses = []\n",
    "for j,(image,label) in enumerate(test_loader):\n",
    "    z = init.normal_(torch.zeros(1,input_dim),mean=0,std=0.1)\n",
    "    z_optimizer = torch.optim.Adam([z],lr=1e-4)\n",
    "    \n",
    "    generator.eval()\n",
    "    gen_fake = generator(z)\n",
    "    loss = Anomaly_score(image,gen_fake)\n",
    "    losses.append(loss.detach().numpy().tolist())\n",
    "    \n",
    "threshold = np.mean(losses) + np.std(losses)\n",
    "anomaly_mask = pd.Series(losses) > threshold\n",
    "preds = anomaly_mask.map(lambda x: 1.0 if x == True else 0.0)\n",
    "preds = preds.tolist()\n",
    "\n",
    "x_test['y'] = y_test\n",
    "x_test['anomaly score'] = losses\n",
    "x_test['prediction'] = preds\n",
    "\n",
    "end = time.process_time()\n",
    "creditcard_gan_test_time = end - start\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "id": "1ce2ef03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[80774,  4521],\n",
       "       [  127,    21]], dtype=int64)"
      ]
     },
     "execution_count": 375,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(x_test['y'], x_test['prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "id": "3cc7f809",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7769046377991803"
      ]
     },
     "execution_count": 376,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fpr, tpr, _ = metrics.roc_curve(x_test['y'], x_test['anomaly score'])\n",
    "creditcard_gan_auc = metrics.auc(fpr, tpr)\n",
    "metrics.auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "id": "4912700b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.95      0.97     85295\n",
      "           1       0.00      0.14      0.01       148\n",
      "\n",
      "    accuracy                           0.95     85443\n",
      "   macro avg       0.50      0.54      0.49     85443\n",
      "weighted avg       1.00      0.95      0.97     85443\n",
      "\n"
     ]
    }
   ],
   "source": [
    "creditcard_gan_report = classification_report(x_test['y'], x_test['prediction'], target_names = ['0','1'], output_dict = True)\n",
    "print(classification_report(x_test['y'], x_test['prediction'], target_names = ['0','1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "70a9a7d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.004623513870541612\n",
      "0.14189189189189189\n",
      "0.008955223880597017\n"
     ]
    }
   ],
   "source": [
    "print(creditcard_gan_report['1']['precision'])\n",
    "print(creditcard_gan_report['1']['recall'])\n",
    "print(creditcard_gan_report['1']['f1-score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "a8d2f21e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0046865041389160655\n"
     ]
    }
   ],
   "source": [
    "precision, recall, thresholds = precision_recall_curve(x_test['y'], x_test['anomaly score'])\n",
    "creditcard_gan_auc_precision_recall = metrics.auc(recall, precision)\n",
    "print(creditcard_gan_auc_precision_recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03cd6004",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Mammography"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c670a21",
   "metadata": {},
   "source": [
    "**Dataset source**: http://odds.cs.stonybrook.edu/mammography-dataset/ (data is transformed from .mat to .csv format)\n",
    "\n",
    "Shebuti Rayana (2016). ODDS Library [http://odds.cs.stonybrook.edu]. Stony Brook, NY: Stony Brook University, Department of Computer Science.\n",
    "\n",
    "**Additional sources:**\n",
    "\n",
    "Abe, Naoki, Bianca Zadrozny, and John Langford. “Outlier detection by active learning.” Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 2006.\n",
    "\n",
    "Liu, Fei Tony, Kai Ming Ting, and Zhi-Hua Zhou. “Isolation forest.” 2008 Eighth IEEE International Conference on Data Mining. IEEE, 2008.\n",
    "\n",
    "K. M. Ting, J. T. S. Chuan, and F. T. Liu. “Mass: A New Ranking Measure for Anomaly Detection.“, IEEE Transactions on Knowledge and Data Engineering, 2009."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "2fa86495-b3c3-420b-9869-e218b46ddddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./mammography.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "9903d321-6749-4ad3-a74f-5a36ad1922ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Col1</th>\n",
       "      <th>Col2</th>\n",
       "      <th>Col3</th>\n",
       "      <th>Col4</th>\n",
       "      <th>Col5</th>\n",
       "      <th>Col6</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.230020</td>\n",
       "      <td>5.072578</td>\n",
       "      <td>-0.276061</td>\n",
       "      <td>0.832444</td>\n",
       "      <td>-0.377866</td>\n",
       "      <td>0.480322</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.155491</td>\n",
       "      <td>-0.169390</td>\n",
       "      <td>0.670652</td>\n",
       "      <td>-0.859553</td>\n",
       "      <td>-0.377866</td>\n",
       "      <td>-0.945723</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.784415</td>\n",
       "      <td>-0.443654</td>\n",
       "      <td>5.674705</td>\n",
       "      <td>-0.859553</td>\n",
       "      <td>-0.377866</td>\n",
       "      <td>-0.945723</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.546088</td>\n",
       "      <td>0.131415</td>\n",
       "      <td>-0.456387</td>\n",
       "      <td>-0.859553</td>\n",
       "      <td>-0.377866</td>\n",
       "      <td>-0.945723</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.102987</td>\n",
       "      <td>-0.394994</td>\n",
       "      <td>-0.140816</td>\n",
       "      <td>0.979703</td>\n",
       "      <td>-0.377866</td>\n",
       "      <td>1.013566</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Col1      Col2      Col3      Col4      Col5      Col6  y\n",
       "0  0.230020  5.072578 -0.276061  0.832444 -0.377866  0.480322  0\n",
       "1  0.155491 -0.169390  0.670652 -0.859553 -0.377866 -0.945723  0\n",
       "2 -0.784415 -0.443654  5.674705 -0.859553 -0.377866 -0.945723  0\n",
       "3  0.546088  0.131415 -0.456387 -0.859553 -0.377866 -0.945723  0\n",
       "4 -0.102987 -0.394994 -0.140816  0.979703 -0.377866  1.013566  0"
      ]
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "8e43cae7-bae2-4513-8a6b-1d1fc1767200",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Col1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>260</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Col1\n",
       "y       \n",
       "0  10923\n",
       "1    260"
      ]
     },
     "execution_count": 382,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.pivot_table(data,\n",
    "             values = 'Col1',\n",
    "               index = 'y', \n",
    "              aggfunc = 'count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "id": "67a6e1ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11183, 7)"
      ]
     },
     "execution_count": 383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4745195",
   "metadata": {},
   "source": [
    "## AnoGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "id": "ccdd5d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "batch_size = 512\n",
    "learning_rate = 0.0002\n",
    "input_dim = data.shape[1]-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "id": "26891ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    x_train,\n",
    "    x_test,\n",
    "    y_train,\n",
    "    y_test,\n",
    "    indices_train,\n",
    "    indices_test,\n",
    ") = train_test_split(data[data.columns[:-1]], data['y'], data.index, test_size=0.3,stratify=data[['y']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "id": "53434590",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = pd.DataFrame(y_train)\n",
    "x_train = x_train.loc[y_train[y_train.y == 0].index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "b54123b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "myDs=MyTrainDataset(x_train)\n",
    "train_loader=torch.utils.data.DataLoader(myDs,batch_size=batch_size,shuffle=False, drop_last = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "d083263f",
   "metadata": {},
   "outputs": [],
   "source": [
    "netG = Generator()\n",
    "netD = Discriminator()\n",
    "\n",
    "# Initialize BCELoss function\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Create batch of latent vectors that we will use to visualize\n",
    "#  the progression of the generator\n",
    "fixed_noise = torch.randn(batch_size, input_dim)\n",
    "\n",
    "# Establish convention for real and fake labels during training\n",
    "real_label = 1.\n",
    "fake_label = 0.\n",
    "\n",
    "# Setup Adam optimizers for both G and D\n",
    "optimizerD = torch.optim.Adam(netD.parameters(), lr=learning_rate, betas=(0.5, 0.999))\n",
    "optimizerG = torch.optim.Adam(netG.parameters(), lr=learning_rate, betas=(0.5, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "efa3066a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training Loop...\n",
      "0\n",
      "[0/20][0/14]\tLoss_D: 4.5676\tLoss_G: 4.1149\tD(x): 0.1869\tD(G(z)): 0.2721 / 0.2719\n",
      "1\n",
      "[1/20][0/14]\tLoss_D: 3.2955\tLoss_G: 2.2728\tD(x): 0.3557\tD(G(z)): 0.3079 / 0.3072\n",
      "2\n",
      "[2/20][0/14]\tLoss_D: 2.7184\tLoss_G: 1.5456\tD(x): 0.4429\tD(G(z)): 0.3937 / 0.3916\n",
      "3\n",
      "[3/20][0/14]\tLoss_D: 2.2512\tLoss_G: 1.3203\tD(x): 0.5083\tD(G(z)): 0.4187 / 0.4142\n",
      "4\n",
      "[4/20][0/14]\tLoss_D: 1.8655\tLoss_G: 1.3071\tD(x): 0.5638\tD(G(z)): 0.3924 / 0.3891\n",
      "5\n",
      "[5/20][0/14]\tLoss_D: 1.6137\tLoss_G: 1.2628\tD(x): 0.6107\tD(G(z)): 0.4075 / 0.4021\n",
      "6\n",
      "[6/20][0/14]\tLoss_D: 1.4453\tLoss_G: 1.2475\tD(x): 0.6439\tD(G(z)): 0.4098 / 0.4043\n",
      "7\n",
      "[7/20][0/14]\tLoss_D: 1.3956\tLoss_G: 1.1668\tD(x): 0.6662\tD(G(z)): 0.4364 / 0.4273\n",
      "8\n",
      "[8/20][0/14]\tLoss_D: 1.2551\tLoss_G: 1.2207\tD(x): 0.6878\tD(G(z)): 0.4174 / 0.4096\n",
      "9\n",
      "[9/20][0/14]\tLoss_D: 1.2053\tLoss_G: 1.2156\tD(x): 0.6949\tD(G(z)): 0.4110 / 0.4023\n",
      "10\n",
      "[10/20][0/14]\tLoss_D: 1.1997\tLoss_G: 1.2575\tD(x): 0.6941\tD(G(z)): 0.4011 / 0.3920\n",
      "11\n",
      "[11/20][0/14]\tLoss_D: 1.1174\tLoss_G: 1.3472\tD(x): 0.7042\tD(G(z)): 0.3797 / 0.3699\n",
      "12\n",
      "[12/20][0/14]\tLoss_D: 1.1081\tLoss_G: 1.2722\tD(x): 0.7053\tD(G(z)): 0.3889 / 0.3808\n",
      "13\n",
      "[13/20][0/14]\tLoss_D: 1.0824\tLoss_G: 1.3850\tD(x): 0.6986\tD(G(z)): 0.3433 / 0.3392\n",
      "14\n",
      "[14/20][0/14]\tLoss_D: 1.2619\tLoss_G: 1.2490\tD(x): 0.6808\tD(G(z)): 0.4196 / 0.4147\n",
      "15\n",
      "[15/20][0/14]\tLoss_D: 1.4634\tLoss_G: 1.1490\tD(x): 0.6394\tD(G(z)): 0.4470 / 0.4447\n",
      "16\n",
      "[16/20][0/14]\tLoss_D: 1.4019\tLoss_G: 1.0264\tD(x): 0.6200\tD(G(z)): 0.4530 / 0.4498\n",
      "17\n",
      "[17/20][0/14]\tLoss_D: 1.5199\tLoss_G: 1.0243\tD(x): 0.6001\tD(G(z)): 0.4850 / 0.4817\n",
      "18\n",
      "[18/20][0/14]\tLoss_D: 1.3221\tLoss_G: 1.0764\tD(x): 0.5899\tD(G(z)): 0.4116 / 0.4108\n",
      "19\n",
      "[19/20][0/14]\tLoss_D: 1.4191\tLoss_G: 1.0168\tD(x): 0.5874\tD(G(z)): 0.4582 / 0.4521\n",
      "134.578125\n"
     ]
    }
   ],
   "source": [
    "img_list = []\n",
    "G_losses = []\n",
    "D_losses = []\n",
    "iters = 0\n",
    "\n",
    "start = time.process_time()\n",
    "print(\"Starting Training Loop...\")\n",
    "# For each epoch\n",
    "for epoch in range(num_epochs):\n",
    "    print(epoch)\n",
    "    # For each batch in the dataloader\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "\n",
    "        ############################\n",
    "        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "        ###########################\n",
    "        ## Train with all-real batch\n",
    "        netD.zero_grad()\n",
    "        \n",
    "        real_cpu = data[0]\n",
    "        #print(real_cpu)\n",
    "        b_size = real_cpu.size(0)\n",
    "        # Format batch\n",
    "        label = torch.full((b_size,), real_label, dtype=torch.float)\n",
    "        # Forward pass real batch through D\n",
    "        output, _ = netD(real_cpu)\n",
    "        output = output.view(-1)\n",
    "        # Calculate loss on all-real batch\n",
    "        errD_real = criterion(output, label)\n",
    "        # Calculate gradients for D in backward pass\n",
    "        errD_real.backward()\n",
    "        D_x = output.mean().item()\n",
    "\n",
    "        ## Train with all-fake batch\n",
    "        # Generate batch of latent vectors\n",
    "        noise = torch.randn(b_size, input_dim)\n",
    "        # Generate fake image batch with G\n",
    "        fake = netG(noise)\n",
    "        label.fill_(fake_label)\n",
    "        # Classify all fake batch with D\n",
    "        output, _ = netD(fake.detach())\n",
    "        output = output.view(-1)\n",
    "        # Calculate D's loss on the all-fake batch\n",
    "        errD_fake = criterion(output, label)\n",
    "        #print(errD_fake)\n",
    "        # Calculate the gradients for this batch, accumulated (summed) with previous gradients\n",
    "        errD_fake.backward()\n",
    "        D_G_z1 = output.mean().item()\n",
    "        # Compute error of D as sum over the fake and the real batches\n",
    "        errD = errD_real + errD_fake\n",
    "        # Update D\n",
    "        optimizerD.step()\n",
    "\n",
    "        ############################\n",
    "        # (2) Update G network: maximize log(D(G(z)))\n",
    "        ###########################\n",
    "        netG.zero_grad()\n",
    "        label.fill_(real_label)  # fake labels are real for generator cost\n",
    "        # Since we just updated D, perform another forward pass of all-fake batch through D\n",
    "        output, _ = netD(fake)\n",
    "        output = output.view(-1)\n",
    "        \n",
    "        # Calculate G's loss based on this output\n",
    "        errG = criterion(output, label)\n",
    "        # Calculate gradients for G\n",
    "        errG.backward()\n",
    "        D_G_z2 = output.mean().item()\n",
    "        # Update G\n",
    "        optimizerG.step()\n",
    "\n",
    "        # Output training stats\n",
    "        if i % 50 == 0:\n",
    "            print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n",
    "                  % (epoch, num_epochs, i, len(train_loader),\n",
    "                     errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
    "\n",
    "        # Save Losses for plotting later\n",
    "        G_losses.append(errG.item())\n",
    "        D_losses.append(errD.item())\n",
    "\n",
    "        # Check how the generator is doing by saving G's output on fixed_noise\n",
    "        if (iters % 500 == 0) or ((epoch == num_epochs-1) and (i == len(train_loader)-1)):\n",
    "            with torch.no_grad():\n",
    "                fake = netG(fixed_noise).detach().cpu()\n",
    "            img_list.append(vutils.make_grid(fake, padding=2, normalize=True))\n",
    "\n",
    "        iters += 1\n",
    "        \n",
    "end = time.process_time()\n",
    "mammography_gan_train_time = end - start\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "2048bf55",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test['y'] = y_test\n",
    "myDs=MyTestDataset(x_test)\n",
    "test_loader=torch.utils.data.DataLoader(myDs,batch_size=1,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "4f707bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = netG\n",
    "discriminator = netD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "id": "8d4522c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54.9375\n"
     ]
    }
   ],
   "source": [
    "start = time.process_time()\n",
    "losses = []\n",
    "for j,(image,label) in enumerate(test_loader):\n",
    "    z = init.normal_(torch.zeros(1,input_dim),mean=0,std=0.1)\n",
    "    z_optimizer = torch.optim.Adam([z],lr=1e-4)\n",
    "\n",
    "    generator.eval()\n",
    "    gen_fake = generator(z)\n",
    "    loss = Anomaly_score(image,gen_fake)\n",
    "    losses.append(loss.detach().numpy().tolist())\n",
    "    \n",
    "threshold = np.mean(losses) + np.std(losses)\n",
    "anomaly_mask = pd.Series(losses) > threshold\n",
    "preds = anomaly_mask.map(lambda x: 1.0 if x == True else 0.0)\n",
    "preds = preds.tolist()\n",
    "\n",
    "x_test['y'] = y_test\n",
    "x_test['anomaly score'] = losses\n",
    "x_test['prediction'] = preds\n",
    "\n",
    "end = time.process_time()\n",
    "mammography_gan_test_time = end - start\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "6ba7d02a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3005,  272],\n",
       "       [  36,   42]], dtype=int64)"
      ]
     },
     "execution_count": 393,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(x_test['y'], x_test['prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "id": "29fc3209",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.861313897169863"
      ]
     },
     "execution_count": 394,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fpr, tpr, _ = metrics.roc_curve(x_test['y'], x_test['anomaly score'])\n",
    "mammography_gan_auc = metrics.auc(fpr, tpr)\n",
    "metrics.auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "d36961de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.92      0.95      3277\n",
      "           1       0.13      0.54      0.21        78\n",
      "\n",
      "    accuracy                           0.91      3355\n",
      "   macro avg       0.56      0.73      0.58      3355\n",
      "weighted avg       0.97      0.91      0.93      3355\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mammography_gan_report = classification_report(x_test['y'], x_test['prediction'], target_names = ['0','1'], output_dict = True)\n",
    "print(classification_report(x_test['y'], x_test['prediction'], target_names = ['0','1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "id": "69403458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1337579617834395\n",
      "0.5384615384615384\n",
      "0.2142857142857143\n"
     ]
    }
   ],
   "source": [
    "print(mammography_gan_report['1']['precision'])\n",
    "print(mammography_gan_report['1']['recall'])\n",
    "print(mammography_gan_report['1']['f1-score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "id": "db629af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1424669105185536\n"
     ]
    }
   ],
   "source": [
    "precision, recall, thresholds = precision_recall_curve(x_test['y'], x_test['anomaly score'])\n",
    "mammography_gan_auc_precision_recall = metrics.auc(recall, precision)\n",
    "print(mammography_gan_auc_precision_recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74377696-aba0-4a6a-b9f5-620f26843c9d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Shuttle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057b3c41",
   "metadata": {},
   "source": [
    "**Dataset source**: http://odds.cs.stonybrook.edu/shuttle-dataset/ (data is transformed from .mat to .csv format)\n",
    "\n",
    "Shebuti Rayana (2016). ODDS Library [http://odds.cs.stonybrook.edu]. Stony Brook, NY: Stony Brook University, Department of Computer Science.\n",
    "\n",
    "**Additional sources:**\n",
    "\n",
    "Abe, Naoki, Bianca Zadrozny, and John Langford. “Outlier detection by active learning.” Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 2006.\n",
    "\n",
    "Liu, Fei Tony, Kai Ming Ting, and Zhi-Hua Zhou. “Isolation forest.” 2008 Eighth IEEE International Conference on Data Mining. IEEE, 2008.\n",
    "\n",
    "K. M. Ting, J. T. S. Chuan, and F. T. Liu. “Mass: A New Ranking Measure for Anomaly Detection.“, IEEE Transactions on Knowledge and Data Engineering, 2009.\n",
    "\n",
    "Kai Ming Ting, Guang-Tong Zhou, Fei Tony Liu & Tan Swee Chuan. (2010). Mass Estimation and Its Applications. Proceedings of The 16th ACM SIGKDD Conference on Knowledge Discovery and Data Mining 2010. pp. 989-998.\n",
    "\n",
    "Swee Chuan Tan, Kai Ming Ting & Fei Tony Liu. (2011). Fast Anomaly Detection for Streaming Data. Proceedings of the International Joint Conference on Artificial Intelligence 2011. pp.1151-1156."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "id": "a424b386-3e5e-49d5-bf08-d4da0ac643d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./shuttle.csv', sep = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "id": "b4ecddb9-b1c1-4601-a92b-8219448586d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Col1</th>\n",
       "      <th>Col2</th>\n",
       "      <th>Col3</th>\n",
       "      <th>Col4</th>\n",
       "      <th>Col5</th>\n",
       "      <th>Col6</th>\n",
       "      <th>Col7</th>\n",
       "      <th>Col8</th>\n",
       "      <th>Col9</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50</td>\n",
       "      <td>21</td>\n",
       "      <td>77</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>48</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>53</td>\n",
       "      <td>0</td>\n",
       "      <td>82</td>\n",
       "      <td>0</td>\n",
       "      <td>52</td>\n",
       "      <td>-5</td>\n",
       "      <td>29</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>76</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>18</td>\n",
       "      <td>40</td>\n",
       "      <td>48</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>79</td>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>-26</td>\n",
       "      <td>43</td>\n",
       "      <td>46</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>85</td>\n",
       "      <td>0</td>\n",
       "      <td>88</td>\n",
       "      <td>-4</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>83</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Col1  Col2  Col3  Col4  Col5  Col6  Col7  Col8  Col9  y\n",
       "0    50    21    77     0    28     0    27    48    22  1\n",
       "1    53     0    82     0    52    -5    29    30     2  0\n",
       "2    37     0    76     0    28    18    40    48     8  0\n",
       "3    37     0    79     0    34   -26    43    46     2  0\n",
       "4    85     0    88    -4     6     1     3    83    80  1"
      ]
     },
     "execution_count": 399,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "id": "94d4c917",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49097, 10)"
      ]
     },
     "execution_count": 400,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "id": "f09c4dcc-609d-4c38-b7ae-69187b451e02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Col1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>45586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3511</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Col1\n",
       "y       \n",
       "0  45586\n",
       "1   3511"
      ]
     },
     "execution_count": 401,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.pivot_table(data,\n",
    "             values = 'Col1',\n",
    "               index = 'y', \n",
    "              aggfunc = 'count')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc00f7e",
   "metadata": {},
   "source": [
    "## AnoGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "id": "39bfc0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "batch_size = 512\n",
    "learning_rate = 0.0002\n",
    "input_dim = data.shape[1]-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "id": "bd172c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    x_train,\n",
    "    x_test,\n",
    "    y_train,\n",
    "    y_test,\n",
    "    indices_train,\n",
    "    indices_test,\n",
    ") = train_test_split(data[data.columns[:-1]], data['y'], data.index, test_size=0.3,stratify=data[['y']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "id": "4d2a3bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = pd.DataFrame(y_train)\n",
    "x_train = x_train.loc[y_train[y_train.y == 0].index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "id": "d4f43a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "myDs=MyTrainDataset(x_train)\n",
    "train_loader=torch.utils.data.DataLoader(myDs,batch_size=batch_size,shuffle=False, drop_last = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "id": "d262cc40",
   "metadata": {},
   "outputs": [],
   "source": [
    "netG = Generator()\n",
    "netD = Discriminator()\n",
    "\n",
    "# Initialize BCELoss function\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Create batch of latent vectors that we will use to visualize\n",
    "#  the progression of the generator\n",
    "fixed_noise = torch.randn(batch_size, input_dim)\n",
    "\n",
    "# Establish convention for real and fake labels during training\n",
    "real_label = 1.\n",
    "fake_label = 0.\n",
    "\n",
    "# Setup Adam optimizers for both G and D\n",
    "optimizerD = torch.optim.Adam(netD.parameters(), lr=learning_rate, betas=(0.5, 0.999))\n",
    "optimizerG = torch.optim.Adam(netG.parameters(), lr=learning_rate, betas=(0.5, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "id": "46383b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training Loop...\n",
      "0\n",
      "[0/20][0/62]\tLoss_D: 2.9971\tLoss_G: 5.0737\tD(x): 0.2576\tD(G(z)): 0.1969 / 0.1956\n",
      "[0/20][50/62]\tLoss_D: 3.6019\tLoss_G: 1.8457\tD(x): 0.2313\tD(G(z)): 0.3437 / 0.3437\n",
      "1\n",
      "[1/20][0/62]\tLoss_D: 1.5418\tLoss_G: 1.5460\tD(x): 0.6006\tD(G(z)): 0.3688 / 0.3662\n",
      "[1/20][50/62]\tLoss_D: 2.7629\tLoss_G: 1.1340\tD(x): 0.3481\tD(G(z)): 0.4434 / 0.4400\n",
      "2\n",
      "[2/20][0/62]\tLoss_D: 1.3073\tLoss_G: 1.1106\tD(x): 0.7694\tD(G(z)): 0.4338 / 0.4268\n",
      "[2/20][50/62]\tLoss_D: 2.0825\tLoss_G: 1.2750\tD(x): 0.4375\tD(G(z)): 0.3832 / 0.3778\n",
      "3\n",
      "[3/20][0/62]\tLoss_D: 1.0625\tLoss_G: 1.2385\tD(x): 0.8386\tD(G(z)): 0.4005 / 0.3924\n",
      "[3/20][50/62]\tLoss_D: 1.7826\tLoss_G: 1.3059\tD(x): 0.5093\tD(G(z)): 0.3843 / 0.3794\n",
      "4\n",
      "[4/20][0/62]\tLoss_D: 1.0091\tLoss_G: 1.2673\tD(x): 0.8797\tD(G(z)): 0.3972 / 0.3868\n",
      "[4/20][50/62]\tLoss_D: 1.7798\tLoss_G: 1.1814\tD(x): 0.5264\tD(G(z)): 0.4372 / 0.4294\n",
      "5\n",
      "[5/20][0/62]\tLoss_D: 1.0904\tLoss_G: 1.1771\tD(x): 0.8978\tD(G(z)): 0.4311 / 0.4183\n",
      "[5/20][50/62]\tLoss_D: 1.5991\tLoss_G: 1.3866\tD(x): 0.5358\tD(G(z)): 0.3754 / 0.3670\n",
      "6\n",
      "[6/20][0/62]\tLoss_D: 0.8909\tLoss_G: 1.4288\tD(x): 0.9065\tD(G(z)): 0.3534 / 0.3419\n",
      "[6/20][50/62]\tLoss_D: 1.7616\tLoss_G: 1.1923\tD(x): 0.5190\tD(G(z)): 0.4200 / 0.4111\n",
      "7\n",
      "[7/20][0/62]\tLoss_D: 1.0486\tLoss_G: 1.1106\tD(x): 0.8973\tD(G(z)): 0.4322 / 0.4185\n",
      "[7/20][50/62]\tLoss_D: 1.4603\tLoss_G: 1.5185\tD(x): 0.5495\tD(G(z)): 0.3185 / 0.3118\n",
      "8\n",
      "[8/20][0/62]\tLoss_D: 0.8449\tLoss_G: 1.4712\tD(x): 0.9071\tD(G(z)): 0.3402 / 0.3274\n",
      "[8/20][50/62]\tLoss_D: 1.5316\tLoss_G: 1.3746\tD(x): 0.5568\tD(G(z)): 0.3532 / 0.3450\n",
      "9\n",
      "[9/20][0/62]\tLoss_D: 0.8748\tLoss_G: 1.4026\tD(x): 0.9004\tD(G(z)): 0.3456 / 0.3284\n",
      "[9/20][50/62]\tLoss_D: 1.2344\tLoss_G: 1.5856\tD(x): 0.6077\tD(G(z)): 0.2885 / 0.2840\n",
      "10\n",
      "[10/20][0/62]\tLoss_D: 0.7058\tLoss_G: 1.5190\tD(x): 0.9202\tD(G(z)): 0.2981 / 0.2894\n",
      "[10/20][50/62]\tLoss_D: 1.1999\tLoss_G: 1.5490\tD(x): 0.6314\tD(G(z)): 0.3187 / 0.3118\n",
      "11\n",
      "[11/20][0/62]\tLoss_D: 0.8227\tLoss_G: 1.3608\tD(x): 0.9222\tD(G(z)): 0.3603 / 0.3487\n",
      "[11/20][50/62]\tLoss_D: 1.4013\tLoss_G: 1.4507\tD(x): 0.5467\tD(G(z)): 0.3248 / 0.3185\n",
      "12\n",
      "[12/20][0/62]\tLoss_D: 0.9048\tLoss_G: 1.3786\tD(x): 0.9177\tD(G(z)): 0.3884 / 0.3714\n",
      "[12/20][50/62]\tLoss_D: 1.3945\tLoss_G: 1.5202\tD(x): 0.5593\tD(G(z)): 0.3346 / 0.3244\n",
      "13\n",
      "[13/20][0/62]\tLoss_D: 0.7788\tLoss_G: 1.4388\tD(x): 0.9289\tD(G(z)): 0.3408 / 0.3244\n",
      "[13/20][50/62]\tLoss_D: 1.1621\tLoss_G: 1.8513\tD(x): 0.5987\tD(G(z)): 0.2208 / 0.2176\n",
      "14\n",
      "[14/20][0/62]\tLoss_D: 0.6361\tLoss_G: 1.6028\tD(x): 0.9395\tD(G(z)): 0.2599 / 0.2520\n",
      "[14/20][50/62]\tLoss_D: 1.0196\tLoss_G: 1.7740\tD(x): 0.6726\tD(G(z)): 0.2535 / 0.2476\n",
      "15\n",
      "[15/20][0/62]\tLoss_D: 0.6255\tLoss_G: 1.7890\tD(x): 0.9526\tD(G(z)): 0.2607 / 0.2512\n",
      "[15/20][50/62]\tLoss_D: 1.1331\tLoss_G: 1.4593\tD(x): 0.6911\tD(G(z)): 0.3457 / 0.3351\n",
      "16\n",
      "[16/20][0/62]\tLoss_D: 0.6210\tLoss_G: 1.7568\tD(x): 0.9562\tD(G(z)): 0.2594 / 0.2501\n",
      "[16/20][50/62]\tLoss_D: 1.0134\tLoss_G: 1.6537\tD(x): 0.7011\tD(G(z)): 0.3015 / 0.2919\n",
      "17\n",
      "[17/20][0/62]\tLoss_D: 0.6309\tLoss_G: 1.8415\tD(x): 0.9583\tD(G(z)): 0.2507 / 0.2420\n",
      "[17/20][50/62]\tLoss_D: 1.0367\tLoss_G: 1.9393\tD(x): 0.6731\tD(G(z)): 0.2781 / 0.2647\n",
      "18\n",
      "[18/20][0/62]\tLoss_D: 0.7681\tLoss_G: 1.9907\tD(x): 0.9503\tD(G(z)): 0.3442 / 0.3227\n",
      "[18/20][50/62]\tLoss_D: 1.1779\tLoss_G: 1.7474\tD(x): 0.6312\tD(G(z)): 0.3014 / 0.2917\n",
      "19\n",
      "[19/20][0/62]\tLoss_D: 0.7319\tLoss_G: 1.6980\tD(x): 0.9358\tD(G(z)): 0.3090 / 0.2931\n",
      "[19/20][50/62]\tLoss_D: 1.6395\tLoss_G: 1.4922\tD(x): 0.4684\tD(G(z)): 0.3543 / 0.3497\n",
      "560.96875\n"
     ]
    }
   ],
   "source": [
    "img_list = []\n",
    "G_losses = []\n",
    "D_losses = []\n",
    "iters = 0\n",
    "\n",
    "start = time.process_time()\n",
    "print(\"Starting Training Loop...\")\n",
    "# For each epoch\n",
    "for epoch in range(num_epochs):\n",
    "    print(epoch)\n",
    "    # For each batch in the dataloader\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "\n",
    "        ############################\n",
    "        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "        ###########################\n",
    "        ## Train with all-real batch\n",
    "        netD.zero_grad()\n",
    "        \n",
    "        real_cpu = data[0]\n",
    "        #print(real_cpu)\n",
    "        b_size = real_cpu.size(0)\n",
    "        # Format batch\n",
    "        label = torch.full((b_size,), real_label, dtype=torch.float)\n",
    "        # Forward pass real batch through D\n",
    "        output, _ = netD(real_cpu)\n",
    "        output = output.view(-1)\n",
    "        # Calculate loss on all-real batch\n",
    "        errD_real = criterion(output, label)\n",
    "        # Calculate gradients for D in backward pass\n",
    "        errD_real.backward()\n",
    "        D_x = output.mean().item()\n",
    "\n",
    "        ## Train with all-fake batch\n",
    "        # Generate batch of latent vectors\n",
    "        noise = torch.randn(b_size, input_dim)\n",
    "        # Generate fake image batch with G\n",
    "        fake = netG(noise)\n",
    "        label.fill_(fake_label)\n",
    "        # Classify all fake batch with D\n",
    "        output, _ = netD(fake.detach())\n",
    "        output = output.view(-1)\n",
    "        # Calculate D's loss on the all-fake batch\n",
    "        errD_fake = criterion(output, label)\n",
    "        #print(errD_fake)\n",
    "        # Calculate the gradients for this batch, accumulated (summed) with previous gradients\n",
    "        errD_fake.backward()\n",
    "        D_G_z1 = output.mean().item()\n",
    "        # Compute error of D as sum over the fake and the real batches\n",
    "        errD = errD_real + errD_fake\n",
    "        # Update D\n",
    "        optimizerD.step()\n",
    "\n",
    "        ############################\n",
    "        # (2) Update G network: maximize log(D(G(z)))\n",
    "        ###########################\n",
    "        netG.zero_grad()\n",
    "        label.fill_(real_label)  # fake labels are real for generator cost\n",
    "        # Since we just updated D, perform another forward pass of all-fake batch through D\n",
    "        output, _ = netD(fake)\n",
    "        output = output.view(-1)\n",
    "        \n",
    "        # Calculate G's loss based on this output\n",
    "        errG = criterion(output, label)\n",
    "        # Calculate gradients for G\n",
    "        errG.backward()\n",
    "        D_G_z2 = output.mean().item()\n",
    "        # Update G\n",
    "        optimizerG.step()\n",
    "\n",
    "        # Output training stats\n",
    "        if i % 50 == 0:\n",
    "            print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n",
    "                  % (epoch, num_epochs, i, len(train_loader),\n",
    "                     errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
    "\n",
    "        # Save Losses for plotting later\n",
    "        G_losses.append(errG.item())\n",
    "        D_losses.append(errD.item())\n",
    "\n",
    "        # Check how the generator is doing by saving G's output on fixed_noise\n",
    "        if (iters % 500 == 0) or ((epoch == num_epochs-1) and (i == len(train_loader)-1)):\n",
    "            with torch.no_grad():\n",
    "                fake = netG(fixed_noise).detach().cpu()\n",
    "            img_list.append(vutils.make_grid(fake, padding=2, normalize=True))\n",
    "\n",
    "        iters += 1\n",
    "        \n",
    "end = time.process_time()\n",
    "shuttle_gan_train_time = end - start\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "id": "7b0d4481",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test['y'] = y_test\n",
    "myDs=MyTestDataset(x_test)\n",
    "test_loader=torch.utils.data.DataLoader(myDs,batch_size=1,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "id": "7870adf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = netG\n",
    "discriminator = netD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "id": "d8e43d32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "223.640625\n"
     ]
    }
   ],
   "source": [
    "start = time.process_time()\n",
    "losses = []\n",
    "for j,(image,label) in enumerate(test_loader):\n",
    "    z = init.normal_(torch.zeros(1,input_dim),mean=0,std=0.1)\n",
    "    z_optimizer = torch.optim.Adam([z],lr=1e-4)\n",
    "\n",
    "    generator.eval()\n",
    "    gen_fake = generator(z)\n",
    "    loss = Anomaly_score(image,gen_fake)\n",
    "    losses.append(loss.detach().numpy().tolist())\n",
    "    \n",
    "threshold = np.mean(losses) + np.std(losses)\n",
    "anomaly_mask = pd.Series(losses) > threshold\n",
    "preds = anomaly_mask.map(lambda x: 1.0 if x == True else 0.0)\n",
    "preds = preds.tolist()\n",
    "\n",
    "x_test['y'] = y_test\n",
    "x_test['anomaly score'] = losses\n",
    "x_test['prediction'] = preds\n",
    "\n",
    "end = time.process_time()\n",
    "shuttle_gan_test_time = end - start\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "id": "14826181",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[13643,    34],\n",
       "       [ 1030,    23]], dtype=int64)"
      ]
     },
     "execution_count": 411,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(x_test['y'], x_test['prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "id": "b81a4f48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9895958034926132"
      ]
     },
     "execution_count": 412,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fpr, tpr, _ = metrics.roc_curve(x_test['y'], x_test['anomaly score'])\n",
    "shuttle_gan_auc = metrics.auc(fpr, tpr)\n",
    "metrics.auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "id": "a21a66fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      1.00      0.96     13677\n",
      "           1       0.40      0.02      0.04      1053\n",
      "\n",
      "    accuracy                           0.93     14730\n",
      "   macro avg       0.67      0.51      0.50     14730\n",
      "weighted avg       0.89      0.93      0.90     14730\n",
      "\n"
     ]
    }
   ],
   "source": [
    "shuttle_gan_report = classification_report(x_test['y'], x_test['prediction'], target_names = ['0','1'], output_dict = True)\n",
    "print(classification_report(x_test['y'], x_test['prediction'], target_names = ['0','1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "id": "99410a34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.40350877192982454\n",
      "0.02184235517568851\n",
      "0.04144144144144145\n"
     ]
    }
   ],
   "source": [
    "print(shuttle_gan_report['1']['precision'])\n",
    "print(shuttle_gan_report['1']['recall'])\n",
    "print(shuttle_gan_report['1']['f1-score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "id": "b7aa0798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8330589350818651\n"
     ]
    }
   ],
   "source": [
    "precision, recall, thresholds = precision_recall_curve(x_test['y'], x_test['anomaly score'])\n",
    "shuttle_gan_auc_precision_recall = metrics.auc(recall, precision)\n",
    "print(shuttle_gan_auc_precision_recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5ee8ce-7b98-40ba-935f-2a1f91d141c3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d55578",
   "metadata": {},
   "source": [
    "**Dataset source**: http://odds.cs.stonybrook.edu/mnist-dataset/ (data is transformed from .mat to .csv format)\n",
    "\n",
    "Shebuti Rayana (2016). ODDS Library [http://odds.cs.stonybrook.edu]. Stony Brook, NY: Stony Brook University, Department of Computer Science.\n",
    "\n",
    "**Additional sources:**\n",
    "\n",
    "Bandaragoda, Tharindu R., et al. “Efficient Anomaly Detection by Isolation Using Nearest Neighbour Ensemble.” 2014 IEEE International Conference on Data Mining Workshop. IEEE, 2014."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "id": "a092626d-fd14-4efd-848b-04c865df948b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./mnist.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "id": "dce30806-daa4-4424-9d34-5590b12cc572",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(columns = ['Col1','Col4', 'Col7', 'Col22', 'Col27', 'Col29', 'Col38', 'Col41', 'Col51', 'Col53', 'Col54', 'Col61', 'Col62', 'Col71', 'Col73', 'Col79', 'Col87', 'Col88', 'Col89', 'Col90',\n",
    "'Col92', 'Col100'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "id": "a977b2ca-e0c7-4440-8019-ecccfafd0794",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Col2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Col2\n",
       "y      \n",
       "0  6903\n",
       "1   700"
      ]
     },
     "execution_count": 418,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.pivot_table(data,\n",
    "             values = 'Col2',\n",
    "               index = 'y', \n",
    "              aggfunc = 'count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "id": "67c2f6ad-e7a7-41df-81e1-5baf06f853d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7603, 79)"
      ]
     },
     "execution_count": 419,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "id": "d6dc2a2e-c4ab-404c-bf0b-d549f2b7c4ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Col2</th>\n",
       "      <th>Col3</th>\n",
       "      <th>Col5</th>\n",
       "      <th>Col6</th>\n",
       "      <th>Col8</th>\n",
       "      <th>Col9</th>\n",
       "      <th>Col10</th>\n",
       "      <th>Col11</th>\n",
       "      <th>Col12</th>\n",
       "      <th>Col13</th>\n",
       "      <th>...</th>\n",
       "      <th>Col86</th>\n",
       "      <th>Col91</th>\n",
       "      <th>Col93</th>\n",
       "      <th>Col94</th>\n",
       "      <th>Col95</th>\n",
       "      <th>Col96</th>\n",
       "      <th>Col97</th>\n",
       "      <th>Col98</th>\n",
       "      <th>Col99</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-73.804153</td>\n",
       "      <td>198.205963</td>\n",
       "      <td>-13.124617</td>\n",
       "      <td>-1.1501</td>\n",
       "      <td>-0.141633</td>\n",
       "      <td>179.249390</td>\n",
       "      <td>114.661163</td>\n",
       "      <td>-80.736702</td>\n",
       "      <td>130.659348</td>\n",
       "      <td>162.649841</td>\n",
       "      <td>...</td>\n",
       "      <td>-15.392716</td>\n",
       "      <td>188.055649</td>\n",
       "      <td>-4.469967</td>\n",
       "      <td>158.381409</td>\n",
       "      <td>-137.100632</td>\n",
       "      <td>27.131416</td>\n",
       "      <td>-2.274633</td>\n",
       "      <td>-0.00065</td>\n",
       "      <td>-12.351267</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-73.804153</td>\n",
       "      <td>197.205963</td>\n",
       "      <td>-13.124617</td>\n",
       "      <td>-1.1501</td>\n",
       "      <td>-0.141633</td>\n",
       "      <td>179.249390</td>\n",
       "      <td>-44.338833</td>\n",
       "      <td>-80.736702</td>\n",
       "      <td>128.659348</td>\n",
       "      <td>190.649841</td>\n",
       "      <td>...</td>\n",
       "      <td>-15.392716</td>\n",
       "      <td>186.055649</td>\n",
       "      <td>-4.469967</td>\n",
       "      <td>123.381416</td>\n",
       "      <td>-137.100632</td>\n",
       "      <td>157.131409</td>\n",
       "      <td>-2.274633</td>\n",
       "      <td>-0.00065</td>\n",
       "      <td>-12.351267</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-73.804153</td>\n",
       "      <td>-53.794033</td>\n",
       "      <td>-13.124617</td>\n",
       "      <td>-1.1501</td>\n",
       "      <td>-0.141633</td>\n",
       "      <td>-73.750618</td>\n",
       "      <td>-44.338833</td>\n",
       "      <td>170.263306</td>\n",
       "      <td>130.659348</td>\n",
       "      <td>46.649849</td>\n",
       "      <td>...</td>\n",
       "      <td>-15.392716</td>\n",
       "      <td>188.055649</td>\n",
       "      <td>-4.469967</td>\n",
       "      <td>157.381409</td>\n",
       "      <td>-137.100632</td>\n",
       "      <td>-93.868584</td>\n",
       "      <td>-2.274633</td>\n",
       "      <td>-0.00065</td>\n",
       "      <td>-12.351267</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-73.804153</td>\n",
       "      <td>86.205963</td>\n",
       "      <td>-13.124617</td>\n",
       "      <td>-1.1501</td>\n",
       "      <td>-0.141633</td>\n",
       "      <td>76.249382</td>\n",
       "      <td>208.661163</td>\n",
       "      <td>107.263298</td>\n",
       "      <td>130.659348</td>\n",
       "      <td>190.649841</td>\n",
       "      <td>...</td>\n",
       "      <td>-15.392716</td>\n",
       "      <td>188.055649</td>\n",
       "      <td>-4.469967</td>\n",
       "      <td>157.381409</td>\n",
       "      <td>-137.100632</td>\n",
       "      <td>74.131416</td>\n",
       "      <td>-2.274633</td>\n",
       "      <td>-0.00065</td>\n",
       "      <td>-12.351267</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-27.804153</td>\n",
       "      <td>199.205963</td>\n",
       "      <td>-13.124617</td>\n",
       "      <td>-1.1501</td>\n",
       "      <td>-0.141633</td>\n",
       "      <td>179.249390</td>\n",
       "      <td>-44.338833</td>\n",
       "      <td>-80.736702</td>\n",
       "      <td>130.659348</td>\n",
       "      <td>91.649849</td>\n",
       "      <td>...</td>\n",
       "      <td>-15.392716</td>\n",
       "      <td>188.055649</td>\n",
       "      <td>-4.469967</td>\n",
       "      <td>22.381416</td>\n",
       "      <td>-137.100632</td>\n",
       "      <td>159.131409</td>\n",
       "      <td>-2.274633</td>\n",
       "      <td>-0.00065</td>\n",
       "      <td>-12.351267</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 79 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Col2        Col3       Col5    Col6      Col8        Col9       Col10  \\\n",
       "0 -73.804153  198.205963 -13.124617 -1.1501 -0.141633  179.249390  114.661163   \n",
       "1 -73.804153  197.205963 -13.124617 -1.1501 -0.141633  179.249390  -44.338833   \n",
       "2 -73.804153  -53.794033 -13.124617 -1.1501 -0.141633  -73.750618  -44.338833   \n",
       "3 -73.804153   86.205963 -13.124617 -1.1501 -0.141633   76.249382  208.661163   \n",
       "4 -27.804153  199.205963 -13.124617 -1.1501 -0.141633  179.249390  -44.338833   \n",
       "\n",
       "        Col11       Col12       Col13  ...      Col86       Col91     Col93  \\\n",
       "0  -80.736702  130.659348  162.649841  ... -15.392716  188.055649 -4.469967   \n",
       "1  -80.736702  128.659348  190.649841  ... -15.392716  186.055649 -4.469967   \n",
       "2  170.263306  130.659348   46.649849  ... -15.392716  188.055649 -4.469967   \n",
       "3  107.263298  130.659348  190.649841  ... -15.392716  188.055649 -4.469967   \n",
       "4  -80.736702  130.659348   91.649849  ... -15.392716  188.055649 -4.469967   \n",
       "\n",
       "        Col94       Col95       Col96     Col97    Col98      Col99  y  \n",
       "0  158.381409 -137.100632   27.131416 -2.274633 -0.00065 -12.351267  0  \n",
       "1  123.381416 -137.100632  157.131409 -2.274633 -0.00065 -12.351267  0  \n",
       "2  157.381409 -137.100632  -93.868584 -2.274633 -0.00065 -12.351267  0  \n",
       "3  157.381409 -137.100632   74.131416 -2.274633 -0.00065 -12.351267  0  \n",
       "4   22.381416 -137.100632  159.131409 -2.274633 -0.00065 -12.351267  0  \n",
       "\n",
       "[5 rows x 79 columns]"
      ]
     },
     "execution_count": 420,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a978b7e1",
   "metadata": {},
   "source": [
    "## AnoGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "id": "8a8ee37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "batch_size = 512\n",
    "learning_rate = 0.0002\n",
    "input_dim = data.shape[1]-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "id": "d58a8f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    x_train,\n",
    "    x_test,\n",
    "    y_train,\n",
    "    y_test,\n",
    "    indices_train,\n",
    "    indices_test,\n",
    ") = train_test_split(data[data.columns[:-1]], data['y'], data.index, test_size=0.3,stratify=data[['y']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "id": "bb618b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = pd.DataFrame(y_train)\n",
    "x_train = x_train.loc[y_train[y_train.y == 0].index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "id": "a2be1d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "myDs=MyTrainDataset(x_train)\n",
    "train_loader=torch.utils.data.DataLoader(myDs,batch_size=batch_size,shuffle=False, drop_last = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "id": "18a39ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "netG = Generator()\n",
    "netD = Discriminator()\n",
    "\n",
    "# Initialize BCELoss function\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Create batch of latent vectors that we will use to visualize\n",
    "#  the progression of the generator\n",
    "fixed_noise = torch.randn(batch_size, input_dim)\n",
    "\n",
    "# Establish convention for real and fake labels during training\n",
    "real_label = 1.\n",
    "fake_label = 0.\n",
    "\n",
    "# Setup Adam optimizers for both G and D\n",
    "optimizerD = torch.optim.Adam(netD.parameters(), lr=learning_rate, betas=(0.5, 0.999))\n",
    "optimizerG = torch.optim.Adam(netG.parameters(), lr=learning_rate, betas=(0.5, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "id": "2e7943fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training Loop...\n",
      "0\n",
      "[0/20][0/9]\tLoss_D: 17.0741\tLoss_G: 0.5482\tD(x): 0.8133\tD(G(z)): 0.8519 / 0.8512\n",
      "1\n",
      "[1/20][0/9]\tLoss_D: 14.2361\tLoss_G: 0.4072\tD(x): 0.8238\tD(G(z)): 0.8751 / 0.8743\n",
      "2\n",
      "[2/20][0/9]\tLoss_D: 11.9060\tLoss_G: 0.2821\tD(x): 0.8315\tD(G(z)): 0.8942 / 0.8931\n",
      "3\n",
      "[3/20][0/9]\tLoss_D: 11.3974\tLoss_G: 0.3030\tD(x): 0.8363\tD(G(z)): 0.8754 / 0.8743\n",
      "4\n",
      "[4/20][0/9]\tLoss_D: 9.8349\tLoss_G: 0.3401\tD(x): 0.8389\tD(G(z)): 0.8719 / 0.8707\n",
      "5\n",
      "[5/20][0/9]\tLoss_D: 8.4643\tLoss_G: 0.2456\tD(x): 0.8395\tD(G(z)): 0.8947 / 0.8935\n",
      "6\n",
      "[6/20][0/9]\tLoss_D: 8.5878\tLoss_G: 0.2350\tD(x): 0.8386\tD(G(z)): 0.8912 / 0.8899\n",
      "7\n",
      "[7/20][0/9]\tLoss_D: 7.3728\tLoss_G: 0.2212\tD(x): 0.8359\tD(G(z)): 0.9006 / 0.8993\n",
      "8\n",
      "[8/20][0/9]\tLoss_D: 6.2379\tLoss_G: 0.2612\tD(x): 0.8319\tD(G(z)): 0.8815 / 0.8801\n",
      "9\n",
      "[9/20][0/9]\tLoss_D: 6.6016\tLoss_G: 0.3695\tD(x): 0.8263\tD(G(z)): 0.8472 / 0.8456\n",
      "10\n",
      "[10/20][0/9]\tLoss_D: 5.7664\tLoss_G: 0.3056\tD(x): 0.8197\tD(G(z)): 0.8618 / 0.8604\n",
      "11\n",
      "[11/20][0/9]\tLoss_D: 5.6583\tLoss_G: 0.3167\tD(x): 0.8125\tD(G(z)): 0.8591 / 0.8575\n",
      "12\n",
      "[12/20][0/9]\tLoss_D: 5.1849\tLoss_G: 0.3585\tD(x): 0.8044\tD(G(z)): 0.8320 / 0.8302\n",
      "13\n",
      "[13/20][0/9]\tLoss_D: 4.6924\tLoss_G: 0.3120\tD(x): 0.7958\tD(G(z)): 0.8374 / 0.8356\n",
      "14\n",
      "[14/20][0/9]\tLoss_D: 4.7460\tLoss_G: 0.3531\tD(x): 0.7865\tD(G(z)): 0.8269 / 0.8250\n",
      "15\n",
      "[15/20][0/9]\tLoss_D: 4.7618\tLoss_G: 0.3466\tD(x): 0.7768\tD(G(z)): 0.8196 / 0.8176\n",
      "16\n",
      "[16/20][0/9]\tLoss_D: 4.4078\tLoss_G: 0.4305\tD(x): 0.7664\tD(G(z)): 0.7980 / 0.7959\n",
      "17\n",
      "[17/20][0/9]\tLoss_D: 4.3066\tLoss_G: 0.3828\tD(x): 0.7556\tD(G(z)): 0.8042 / 0.8019\n",
      "18\n",
      "[18/20][0/9]\tLoss_D: 3.9475\tLoss_G: 0.4028\tD(x): 0.7442\tD(G(z)): 0.7935 / 0.7909\n",
      "19\n",
      "[19/20][0/9]\tLoss_D: 3.7305\tLoss_G: 0.4332\tD(x): 0.7326\tD(G(z)): 0.7742 / 0.7716\n",
      "75.578125\n"
     ]
    }
   ],
   "source": [
    "img_list = []\n",
    "G_losses = []\n",
    "D_losses = []\n",
    "iters = 0\n",
    "\n",
    "start = time.process_time()\n",
    "print(\"Starting Training Loop...\")\n",
    "# For each epoch\n",
    "for epoch in range(num_epochs):\n",
    "    print(epoch)\n",
    "    # For each batch in the dataloader\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "\n",
    "        ############################\n",
    "        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "        ###########################\n",
    "        ## Train with all-real batch\n",
    "        netD.zero_grad()\n",
    "        \n",
    "        real_cpu = data[0]\n",
    "        #print(real_cpu)\n",
    "        b_size = real_cpu.size(0)\n",
    "        # Format batch\n",
    "        label = torch.full((b_size,), real_label, dtype=torch.float)\n",
    "        # Forward pass real batch through D\n",
    "        output, _ = netD(real_cpu)\n",
    "        output = output.view(-1)\n",
    "        # Calculate loss on all-real batch\n",
    "        errD_real = criterion(output, label)\n",
    "        # Calculate gradients for D in backward pass\n",
    "        errD_real.backward()\n",
    "        D_x = output.mean().item()\n",
    "\n",
    "        ## Train with all-fake batch\n",
    "        # Generate batch of latent vectors\n",
    "        noise = torch.randn(b_size, input_dim)\n",
    "        # Generate fake image batch with G\n",
    "        fake = netG(noise)\n",
    "        label.fill_(fake_label)\n",
    "        # Classify all fake batch with D\n",
    "        output, _ = netD(fake.detach())\n",
    "        output = output.view(-1)\n",
    "        # Calculate D's loss on the all-fake batch\n",
    "        errD_fake = criterion(output, label)\n",
    "        #print(errD_fake)\n",
    "        # Calculate the gradients for this batch, accumulated (summed) with previous gradients\n",
    "        errD_fake.backward()\n",
    "        D_G_z1 = output.mean().item()\n",
    "        # Compute error of D as sum over the fake and the real batches\n",
    "        errD = errD_real + errD_fake\n",
    "        # Update D\n",
    "        optimizerD.step()\n",
    "\n",
    "        ############################\n",
    "        # (2) Update G network: maximize log(D(G(z)))\n",
    "        ###########################\n",
    "        netG.zero_grad()\n",
    "        label.fill_(real_label)  # fake labels are real for generator cost\n",
    "        # Since we just updated D, perform another forward pass of all-fake batch through D\n",
    "        output, _ = netD(fake)\n",
    "        output = output.view(-1)\n",
    "        \n",
    "        # Calculate G's loss based on this output\n",
    "        errG = criterion(output, label)\n",
    "        # Calculate gradients for G\n",
    "        errG.backward()\n",
    "        D_G_z2 = output.mean().item()\n",
    "        # Update G\n",
    "        optimizerG.step()\n",
    "\n",
    "        # Output training stats\n",
    "        if i % 50 == 0:\n",
    "            print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n",
    "                  % (epoch, num_epochs, i, len(train_loader),\n",
    "                     errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
    "\n",
    "        # Save Losses for plotting later\n",
    "        G_losses.append(errG.item())\n",
    "        D_losses.append(errD.item())\n",
    "\n",
    "        # Check how the generator is doing by saving G's output on fixed_noise\n",
    "        if (iters % 500 == 0) or ((epoch == num_epochs-1) and (i == len(train_loader)-1)):\n",
    "            with torch.no_grad():\n",
    "                fake = netG(fixed_noise).detach().cpu()\n",
    "            img_list.append(vutils.make_grid(fake, padding=2, normalize=True))\n",
    "\n",
    "        iters += 1\n",
    "        \n",
    "end = time.process_time()\n",
    "mnist_gan_train_time = end - start\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "id": "d98d202b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test['y'] = y_test\n",
    "myDs=MyTestDataset(x_test)\n",
    "test_loader=torch.utils.data.DataLoader(myDs,batch_size=1,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "id": "c4dcd7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = netG\n",
    "discriminator = netD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "id": "663aa408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40.890625\n"
     ]
    }
   ],
   "source": [
    "start = time.process_time()\n",
    "losses = []\n",
    "for j,(image,label) in enumerate(test_loader):\n",
    "    z = init.normal_(torch.zeros(1,input_dim),mean=0,std=0.1)\n",
    "    z_optimizer = torch.optim.Adam([z],lr=1e-4)\n",
    "\n",
    "    generator.eval()\n",
    "    gen_fake = generator(z)\n",
    "    loss = Anomaly_score(image,gen_fake)\n",
    "    losses.append(loss.detach().numpy().tolist())\n",
    "    \n",
    "threshold = np.mean(losses) + np.std(losses)\n",
    "anomaly_mask = pd.Series(losses) > threshold\n",
    "preds = anomaly_mask.map(lambda x: 1.0 if x == True else 0.0)\n",
    "preds = preds.tolist()\n",
    "\n",
    "x_test['y'] = y_test\n",
    "x_test['anomaly score'] = losses\n",
    "x_test['prediction'] = preds\n",
    "\n",
    "end = time.process_time()\n",
    "mnist_gan_test_time = end - start\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "id": "cd6922dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1703,  368],\n",
       "       [ 199,   11]], dtype=int64)"
      ]
     },
     "execution_count": 430,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(x_test['y'], x_test['prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "id": "86ea5394",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2474075095996873"
      ]
     },
     "execution_count": 431,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fpr, tpr, _ = metrics.roc_curve(x_test['y'], x_test['anomaly score'])\n",
    "mnist_gan_auc = metrics.auc(fpr, tpr)\n",
    "metrics.auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "id": "445053e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.82      0.86      2071\n",
      "           1       0.03      0.05      0.04       210\n",
      "\n",
      "    accuracy                           0.75      2281\n",
      "   macro avg       0.46      0.44      0.45      2281\n",
      "weighted avg       0.82      0.75      0.78      2281\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mnist_gan_report = classification_report(x_test['y'], x_test['prediction'], target_names = ['0','1'], output_dict = True)\n",
    "print(classification_report(x_test['y'], x_test['prediction'], target_names = ['0','1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "id": "6ca79783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.029023746701846966\n",
      "0.05238095238095238\n",
      "0.03735144312393888\n"
     ]
    }
   ],
   "source": [
    "print(mnist_gan_report['1']['precision'])\n",
    "print(mnist_gan_report['1']['recall'])\n",
    "print(mnist_gan_report['1']['f1-score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "id": "bb3a142d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.056654942258299375\n"
     ]
    }
   ],
   "source": [
    "precision, recall, thresholds = precision_recall_curve(x_test['y'], x_test['anomaly score'])\n",
    "mnist_gan_auc_precision_recall = metrics.auc(recall, precision)\n",
    "print(mnist_gan_auc_precision_recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ec8246-a429-4e4e-b3af-68198a4ed70f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## vowels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02dd3508",
   "metadata": {},
   "source": [
    "**Dataset source**: http://odds.cs.stonybrook.edu/japanese-vowels-data/\n",
    "\n",
    "Shebuti Rayana (2016). ODDS Library [http://odds.cs.stonybrook.edu]. Stony Brook, NY: Stony Brook University, Department of Computer Science.\n",
    "\n",
    "**Additional sources:**\n",
    "\n",
    "C. C. Aggarwal and S. Sathe, “Theoretical foundations and algorithms for outlier ensembles.” ACM SIGKDD Explorations Newsletter, vol. 17, no. 1, pp. 24–47, 2015.\n",
    "\n",
    "Saket Sathe and Charu C. Aggarwal. LODES: Local Density meets Spectral Outlier Detection. SIAM Conference on Data Mining, 2016."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "id": "bc358b13-ecf1-4e7b-9c7b-84c08eacc450",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./vowels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "id": "4c61a6b6-97f1-4bee-8df8-c505f0bfc266",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Col1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>1406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Col1\n",
       "y        \n",
       "0.0  1406\n",
       "1.0    50"
      ]
     },
     "execution_count": 436,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.pivot_table(data,\n",
    "             values = 'Col1',\n",
    "               index = 'y', \n",
    "              aggfunc = 'count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "id": "26c84d3e-0b54-474e-a4c1-a297b66a81bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1456, 13)"
      ]
     },
     "execution_count": 437,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "id": "257280ea-de3b-4e4e-aad9-3565b8a2a7a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Col1</th>\n",
       "      <th>Col2</th>\n",
       "      <th>Col3</th>\n",
       "      <th>Col4</th>\n",
       "      <th>Col5</th>\n",
       "      <th>Col6</th>\n",
       "      <th>Col7</th>\n",
       "      <th>Col8</th>\n",
       "      <th>Col9</th>\n",
       "      <th>Col10</th>\n",
       "      <th>Col11</th>\n",
       "      <th>Col12</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.580469</td>\n",
       "      <td>-0.902534</td>\n",
       "      <td>0.617899</td>\n",
       "      <td>-0.997942</td>\n",
       "      <td>-2.463799</td>\n",
       "      <td>-0.846455</td>\n",
       "      <td>2.349849</td>\n",
       "      <td>0.375400</td>\n",
       "      <td>-0.649334</td>\n",
       "      <td>1.604637</td>\n",
       "      <td>-0.623060</td>\n",
       "      <td>-0.383125</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.784375</td>\n",
       "      <td>-1.077366</td>\n",
       "      <td>0.615781</td>\n",
       "      <td>-0.921911</td>\n",
       "      <td>-2.388553</td>\n",
       "      <td>-0.638047</td>\n",
       "      <td>2.106684</td>\n",
       "      <td>0.361018</td>\n",
       "      <td>-0.714317</td>\n",
       "      <td>1.260236</td>\n",
       "      <td>-0.423339</td>\n",
       "      <td>-0.287791</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.791292</td>\n",
       "      <td>-1.086242</td>\n",
       "      <td>0.669773</td>\n",
       "      <td>-0.806112</td>\n",
       "      <td>-2.260781</td>\n",
       "      <td>-0.538491</td>\n",
       "      <td>2.053282</td>\n",
       "      <td>0.266492</td>\n",
       "      <td>-0.842815</td>\n",
       "      <td>1.081797</td>\n",
       "      <td>-0.267201</td>\n",
       "      <td>-0.172203</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.217306</td>\n",
       "      <td>-1.083425</td>\n",
       "      <td>0.855483</td>\n",
       "      <td>-0.724879</td>\n",
       "      <td>-2.155552</td>\n",
       "      <td>-0.101879</td>\n",
       "      <td>1.768597</td>\n",
       "      <td>0.303151</td>\n",
       "      <td>-1.044710</td>\n",
       "      <td>0.655290</td>\n",
       "      <td>0.214298</td>\n",
       "      <td>-0.341840</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.065352</td>\n",
       "      <td>-1.030178</td>\n",
       "      <td>0.773297</td>\n",
       "      <td>-0.452289</td>\n",
       "      <td>-1.955907</td>\n",
       "      <td>0.248205</td>\n",
       "      <td>1.530474</td>\n",
       "      <td>0.253740</td>\n",
       "      <td>-0.968961</td>\n",
       "      <td>-0.208287</td>\n",
       "      <td>0.331578</td>\n",
       "      <td>0.007288</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Col1      Col2      Col3      Col4      Col5      Col6      Col7  \\\n",
       "0  0.580469 -0.902534  0.617899 -0.997942 -2.463799 -0.846455  2.349849   \n",
       "1  0.784375 -1.077366  0.615781 -0.921911 -2.388553 -0.638047  2.106684   \n",
       "2  0.791292 -1.086242  0.669773 -0.806112 -2.260781 -0.538491  2.053282   \n",
       "3  1.217306 -1.083425  0.855483 -0.724879 -2.155552 -0.101879  1.768597   \n",
       "4  1.065352 -1.030178  0.773297 -0.452289 -1.955907  0.248205  1.530474   \n",
       "\n",
       "       Col8      Col9     Col10     Col11     Col12    y  \n",
       "0  0.375400 -0.649334  1.604637 -0.623060 -0.383125  0.0  \n",
       "1  0.361018 -0.714317  1.260236 -0.423339 -0.287791  0.0  \n",
       "2  0.266492 -0.842815  1.081797 -0.267201 -0.172203  0.0  \n",
       "3  0.303151 -1.044710  0.655290  0.214298 -0.341840  0.0  \n",
       "4  0.253740 -0.968961 -0.208287  0.331578  0.007288  0.0  "
      ]
     },
     "execution_count": 438,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea9b85f",
   "metadata": {},
   "source": [
    "## AnoGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "id": "16151b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "batch_size = 512\n",
    "learning_rate = 0.0002\n",
    "input_dim = data.shape[1]-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "id": "af24d201",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    x_train,\n",
    "    x_test,\n",
    "    y_train,\n",
    "    y_test,\n",
    "    indices_train,\n",
    "    indices_test,\n",
    ") = train_test_split(data[data.columns[:-1]], data['y'], data.index, test_size=0.3,stratify=data[['y']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "id": "a9f33ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = pd.DataFrame(y_train)\n",
    "x_train = x_train.loc[y_train[y_train.y == 0].index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "id": "4954b853",
   "metadata": {},
   "outputs": [],
   "source": [
    "myDs=MyTrainDataset(x_train)\n",
    "train_loader=torch.utils.data.DataLoader(myDs,batch_size=batch_size,shuffle=False, drop_last = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "id": "a5ae4973",
   "metadata": {},
   "outputs": [],
   "source": [
    "netG = Generator()\n",
    "netD = Discriminator()\n",
    "\n",
    "# Initialize BCELoss function\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Create batch of latent vectors that we will use to visualize\n",
    "#  the progression of the generator\n",
    "fixed_noise = torch.randn(batch_size, input_dim)\n",
    "\n",
    "# Establish convention for real and fake labels during training\n",
    "real_label = 1.\n",
    "fake_label = 0.\n",
    "\n",
    "# Setup Adam optimizers for both G and D\n",
    "optimizerD = torch.optim.Adam(netD.parameters(), lr=learning_rate, betas=(0.5, 0.999))\n",
    "optimizerG = torch.optim.Adam(netG.parameters(), lr=learning_rate, betas=(0.5, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "id": "de459d41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training Loop...\n",
      "0\n",
      "[0/20][0/1]\tLoss_D: 5.3591\tLoss_G: 4.6144\tD(x): 0.2015\tD(G(z)): 0.2483 / 0.2473\n",
      "1\n",
      "[1/20][0/1]\tLoss_D: 5.2632\tLoss_G: 4.4410\tD(x): 0.2010\tD(G(z)): 0.2486 / 0.2485\n",
      "2\n",
      "[2/20][0/1]\tLoss_D: 5.0895\tLoss_G: 4.2689\tD(x): 0.2007\tD(G(z)): 0.2352 / 0.2351\n",
      "3\n",
      "[3/20][0/1]\tLoss_D: 4.9926\tLoss_G: 4.1917\tD(x): 0.2006\tD(G(z)): 0.2154 / 0.2154\n",
      "4\n",
      "[4/20][0/1]\tLoss_D: 4.9247\tLoss_G: 4.1313\tD(x): 0.2009\tD(G(z)): 0.2191 / 0.2191\n",
      "5\n",
      "[5/20][0/1]\tLoss_D: 4.8285\tLoss_G: 4.0538\tD(x): 0.2013\tD(G(z)): 0.1941 / 0.1946\n",
      "6\n",
      "[6/20][0/1]\tLoss_D: 4.7875\tLoss_G: 3.9843\tD(x): 0.2021\tD(G(z)): 0.2151 / 0.2153\n",
      "7\n",
      "[7/20][0/1]\tLoss_D: 4.7218\tLoss_G: 3.8036\tD(x): 0.2027\tD(G(z)): 0.2084 / 0.2091\n",
      "8\n",
      "[8/20][0/1]\tLoss_D: 4.6836\tLoss_G: 3.7089\tD(x): 0.2035\tD(G(z)): 0.2271 / 0.2276\n",
      "9\n",
      "[9/20][0/1]\tLoss_D: 4.5224\tLoss_G: 3.6834\tD(x): 0.2045\tD(G(z)): 0.1958 / 0.1962\n",
      "10\n",
      "[10/20][0/1]\tLoss_D: 4.5214\tLoss_G: 3.6101\tD(x): 0.2056\tD(G(z)): 0.2151 / 0.2153\n",
      "11\n",
      "[11/20][0/1]\tLoss_D: 4.4345\tLoss_G: 3.4913\tD(x): 0.2064\tD(G(z)): 0.2068 / 0.2072\n",
      "12\n",
      "[12/20][0/1]\tLoss_D: 4.3911\tLoss_G: 3.5150\tD(x): 0.2075\tD(G(z)): 0.2063 / 0.2066\n",
      "13\n",
      "[13/20][0/1]\tLoss_D: 4.3590\tLoss_G: 3.3071\tD(x): 0.2086\tD(G(z)): 0.2260 / 0.2263\n",
      "14\n",
      "[14/20][0/1]\tLoss_D: 4.2657\tLoss_G: 3.3198\tD(x): 0.2100\tD(G(z)): 0.1992 / 0.2003\n",
      "15\n",
      "[15/20][0/1]\tLoss_D: 4.2281\tLoss_G: 3.2835\tD(x): 0.2115\tD(G(z)): 0.2063 / 0.2068\n",
      "16\n",
      "[16/20][0/1]\tLoss_D: 4.1511\tLoss_G: 3.2042\tD(x): 0.2129\tD(G(z)): 0.2013 / 0.2022\n",
      "17\n",
      "[17/20][0/1]\tLoss_D: 4.2101\tLoss_G: 3.1730\tD(x): 0.2146\tD(G(z)): 0.2232 / 0.2233\n",
      "18\n",
      "[18/20][0/1]\tLoss_D: 4.0897\tLoss_G: 3.0920\tD(x): 0.2161\tD(G(z)): 0.2102 / 0.2105\n",
      "19\n",
      "[19/20][0/1]\tLoss_D: 4.0664\tLoss_G: 3.0802\tD(x): 0.2176\tD(G(z)): 0.2221 / 0.2224\n",
      "9.015625\n"
     ]
    }
   ],
   "source": [
    "img_list = []\n",
    "G_losses = []\n",
    "D_losses = []\n",
    "iters = 0\n",
    "\n",
    "start = time.process_time()\n",
    "print(\"Starting Training Loop...\")\n",
    "# For each epoch\n",
    "for epoch in range(num_epochs):\n",
    "    print(epoch)\n",
    "    # For each batch in the dataloader\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "\n",
    "        ############################\n",
    "        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "        ###########################\n",
    "        ## Train with all-real batch\n",
    "        netD.zero_grad()\n",
    "        \n",
    "        real_cpu = data[0]\n",
    "        #print(real_cpu)\n",
    "        b_size = real_cpu.size(0)\n",
    "        # Format batch\n",
    "        label = torch.full((b_size,), real_label, dtype=torch.float)\n",
    "        # Forward pass real batch through D\n",
    "        output, _ = netD(real_cpu)\n",
    "        output = output.view(-1)\n",
    "        # Calculate loss on all-real batch\n",
    "        errD_real = criterion(output, label)\n",
    "        # Calculate gradients for D in backward pass\n",
    "        errD_real.backward()\n",
    "        D_x = output.mean().item()\n",
    "\n",
    "        ## Train with all-fake batch\n",
    "        # Generate batch of latent vectors\n",
    "        noise = torch.randn(b_size, input_dim)\n",
    "        # Generate fake image batch with G\n",
    "        fake = netG(noise)\n",
    "        label.fill_(fake_label)\n",
    "        # Classify all fake batch with D\n",
    "        output, _ = netD(fake.detach())\n",
    "        output = output.view(-1)\n",
    "        # Calculate D's loss on the all-fake batch\n",
    "        errD_fake = criterion(output, label)\n",
    "        #print(errD_fake)\n",
    "        # Calculate the gradients for this batch, accumulated (summed) with previous gradients\n",
    "        errD_fake.backward()\n",
    "        D_G_z1 = output.mean().item()\n",
    "        # Compute error of D as sum over the fake and the real batches\n",
    "        errD = errD_real + errD_fake\n",
    "        # Update D\n",
    "        optimizerD.step()\n",
    "\n",
    "        ############################\n",
    "        # (2) Update G network: maximize log(D(G(z)))\n",
    "        ###########################\n",
    "        netG.zero_grad()\n",
    "        label.fill_(real_label)  # fake labels are real for generator cost\n",
    "        # Since we just updated D, perform another forward pass of all-fake batch through D\n",
    "        output, _ = netD(fake)\n",
    "        output = output.view(-1)\n",
    "        \n",
    "        # Calculate G's loss based on this output\n",
    "        errG = criterion(output, label)\n",
    "        # Calculate gradients for G\n",
    "        errG.backward()\n",
    "        D_G_z2 = output.mean().item()\n",
    "        # Update G\n",
    "        optimizerG.step()\n",
    "\n",
    "        # Output training stats\n",
    "        if i % 50 == 0:\n",
    "            print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n",
    "                  % (epoch, num_epochs, i, len(train_loader),\n",
    "                     errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
    "\n",
    "        # Save Losses for plotting later\n",
    "        G_losses.append(errG.item())\n",
    "        D_losses.append(errD.item())\n",
    "\n",
    "        # Check how the generator is doing by saving G's output on fixed_noise\n",
    "        if (iters % 500 == 0) or ((epoch == num_epochs-1) and (i == len(train_loader)-1)):\n",
    "            with torch.no_grad():\n",
    "                fake = netG(fixed_noise).detach().cpu()\n",
    "            img_list.append(vutils.make_grid(fake, padding=2, normalize=True))\n",
    "\n",
    "        iters += 1\n",
    "        \n",
    "end = time.process_time()\n",
    "vowels_gan_train_time = end - start\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "id": "ef191ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test['y'] = y_test\n",
    "myDs=MyTestDataset(x_test)\n",
    "test_loader=torch.utils.data.DataLoader(myDs,batch_size=1,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "id": "4cdaddc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = netG\n",
    "discriminator = netD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "id": "0c3c6603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.3125\n"
     ]
    }
   ],
   "source": [
    "start = time.process_time()\n",
    "losses = []\n",
    "for j,(image,label) in enumerate(test_loader):\n",
    "    z = init.normal_(torch.zeros(1,input_dim),mean=0,std=0.1)\n",
    "    z_optimizer = torch.optim.Adam([z],lr=1e-4)\n",
    "\n",
    "    generator.eval()\n",
    "    gen_fake = generator(z)\n",
    "    loss = Anomaly_score(image,gen_fake)\n",
    "    losses.append(loss.detach().numpy().tolist())\n",
    "    \n",
    "threshold = np.mean(losses) + np.std(losses)\n",
    "anomaly_mask = pd.Series(losses) > threshold\n",
    "preds = anomaly_mask.map(lambda x: 1.0 if x == True else 0.0)\n",
    "preds = preds.tolist()\n",
    "\n",
    "x_test['y'] = y_test\n",
    "x_test['anomaly score'] = losses\n",
    "x_test['prediction'] = preds\n",
    "\n",
    "end = time.process_time()\n",
    "vowels_gan_test_time = end - start\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "id": "18852974",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[351,  71],\n",
       "       [ 12,   3]], dtype=int64)"
      ]
     },
     "execution_count": 448,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(x_test['y'], x_test['prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "id": "c648a3fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5744075829383886"
      ]
     },
     "execution_count": 449,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fpr, tpr, _ = metrics.roc_curve(x_test['y'], x_test['anomaly score'])\n",
    "vowels_gan_auc = metrics.auc(fpr, tpr)\n",
    "metrics.auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "id": "f598d8d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.83      0.89       422\n",
      "           1       0.04      0.20      0.07        15\n",
      "\n",
      "    accuracy                           0.81       437\n",
      "   macro avg       0.50      0.52      0.48       437\n",
      "weighted avg       0.94      0.81      0.87       437\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vowels_gan_report = classification_report(x_test['y'], x_test['prediction'], target_names = ['0','1'], output_dict = True)\n",
    "print(classification_report(x_test['y'], x_test['prediction'], target_names = ['0','1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "id": "951808cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04054054054054054\n",
      "0.2\n",
      "0.06741573033707865\n"
     ]
    }
   ],
   "source": [
    "print(vowels_gan_report['1']['precision'])\n",
    "print(vowels_gan_report['1']['recall'])\n",
    "print(vowels_gan_report['1']['f1-score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "id": "d6785310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05141399632850409\n"
     ]
    }
   ],
   "source": [
    "precision, recall, thresholds = precision_recall_curve(x_test['y'], x_test['anomaly score'])\n",
    "vowels_gan_auc_precision_recall = metrics.auc(recall, precision)\n",
    "print(vowels_gan_auc_precision_recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e819820-662f-4b7a-95d1-c6eeb0e62876",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Seismic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51807b44",
   "metadata": {},
   "source": [
    "**Dataset source**: http://odds.cs.stonybrook.edu/seismic-dataset/ (data is transformed from .arff to .csv format)\n",
    "\n",
    "Shebuti Rayana (2016). ODDS Library [http://odds.cs.stonybrook.edu]. Stony Brook, NY: Stony Brook University, Department of Computer Science.\n",
    "\n",
    "**Additional sources:**\n",
    "\n",
    "Saket Sathe and Charu C. Aggarwal. LODES: Local Density meets Spectral Outlier Detection. SIAM Conference on Data Mining, 2016."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "id": "9f7497cf-f25d-4d39-b778-abd020ebfa04",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./seismic.csv', sep = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "id": "98583ea0-afb4-40a3-8bc3-1b96002de33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(columns = ['nbumps6','nbumps7','nbumps89'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "id": "d87f1bdc-6c57-4601-8eeb-1bbbfa24a572",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2584, 16)"
      ]
     },
     "execution_count": 455,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "id": "efce2262",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_enc = OneHotEncoder(drop='first').fit_transform(data[['seismic','seismoacoustic','shift','ghazard']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "id": "67579a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_var = pd.DataFrame(drop_enc.toarray())\n",
    "cat_var.columns = ['seismic: b', 'seismoacoustic: b','seismoacoustic: c','shift: W','ghazard: b','ghazard: c']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "id": "b0932a73-7c0f-4a32-9573-d38a7034034e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([data, cat_var], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "id": "5bd4e951-5330-4c88-902f-1a2d184efb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(columns = ['seismic','seismoacoustic','shift','ghazard'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "id": "45ef5b65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>genergy</th>\n",
       "      <th>gpuls</th>\n",
       "      <th>gdenergy</th>\n",
       "      <th>gdpuls</th>\n",
       "      <th>nbumps</th>\n",
       "      <th>nbumps2</th>\n",
       "      <th>nbumps3</th>\n",
       "      <th>nbumps4</th>\n",
       "      <th>nbumps5</th>\n",
       "      <th>energy</th>\n",
       "      <th>maxenergy</th>\n",
       "      <th>class</th>\n",
       "      <th>seismic: b</th>\n",
       "      <th>seismoacoustic: b</th>\n",
       "      <th>seismoacoustic: c</th>\n",
       "      <th>shift: W</th>\n",
       "      <th>ghazard: b</th>\n",
       "      <th>ghazard: c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15180</td>\n",
       "      <td>48</td>\n",
       "      <td>-72</td>\n",
       "      <td>-72</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14720</td>\n",
       "      <td>33</td>\n",
       "      <td>-70</td>\n",
       "      <td>-79</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>2000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8050</td>\n",
       "      <td>30</td>\n",
       "      <td>-81</td>\n",
       "      <td>-78</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>28820</td>\n",
       "      <td>171</td>\n",
       "      <td>-23</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3000</td>\n",
       "      <td>3000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12640</td>\n",
       "      <td>57</td>\n",
       "      <td>-63</td>\n",
       "      <td>-52</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   genergy  gpuls  gdenergy  gdpuls  nbumps  nbumps2  nbumps3  nbumps4  \\\n",
       "0    15180     48       -72     -72       0        0        0        0   \n",
       "1    14720     33       -70     -79       1        0        1        0   \n",
       "2     8050     30       -81     -78       0        0        0        0   \n",
       "3    28820    171       -23      40       1        0        1        0   \n",
       "4    12640     57       -63     -52       0        0        0        0   \n",
       "\n",
       "   nbumps5  energy  maxenergy  class  seismic: b  seismoacoustic: b  \\\n",
       "0        0       0          0      0         0.0                0.0   \n",
       "1        0    2000       2000      0         0.0                0.0   \n",
       "2        0       0          0      0         0.0                0.0   \n",
       "3        0    3000       3000      0         0.0                0.0   \n",
       "4        0       0          0      0         0.0                0.0   \n",
       "\n",
       "   seismoacoustic: c  shift: W  ghazard: b  ghazard: c  \n",
       "0                0.0       0.0         0.0         0.0  \n",
       "1                0.0       0.0         0.0         0.0  \n",
       "2                0.0       0.0         0.0         0.0  \n",
       "3                0.0       0.0         0.0         0.0  \n",
       "4                0.0       0.0         0.0         0.0  "
      ]
     },
     "execution_count": 460,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "id": "18e0f2ec-0b7d-438f-855a-1ce74d7db590",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>genergy</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>170</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       genergy\n",
       "class         \n",
       "0         2414\n",
       "1          170"
      ]
     },
     "execution_count": 461,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.pivot_table(data,\n",
    "             values = 'genergy',\n",
    "               index = 'class', \n",
    "              aggfunc = 'count')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0961d18",
   "metadata": {},
   "source": [
    "## AnoGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "id": "33264654",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "batch_size = 64\n",
    "learning_rate = 0.0002\n",
    "input_dim = data.shape[1]-1\n",
    "\n",
    "input_dim_num = 11\n",
    "input_dim_cat = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "id": "a180fcf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    x_train,\n",
    "    x_test,\n",
    "    y_train,\n",
    "    y_test,\n",
    "    indices_train,\n",
    "    indices_test,\n",
    ") = train_test_split(data[data.columns[:-1]], data['class'], data.index, test_size=0.3,stratify=data[['class']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "id": "53cffb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = pd.DataFrame(y_train)\n",
    "x_train = x_train.loc[y_train[y_train['class'] == 0].index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "id": "38122af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "myDs=MyTrainDataset(x_train)\n",
    "train_loader=torch.utils.data.DataLoader(myDs,batch_size=batch_size,shuffle=False, drop_last = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "id": "5df978ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "netG = Generator()\n",
    "netD = Discriminator()\n",
    "\n",
    "# Initialize BCELoss function\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Create batch of latent vectors that we will use to visualize\n",
    "#  the progression of the generator\n",
    "fixed_noise = torch.randn(batch_size, input_dim)\n",
    "\n",
    "# Establish convention for real and fake labels during training\n",
    "real_label = 1.\n",
    "fake_label = 0.\n",
    "\n",
    "# Setup Adam optimizers for both G and D\n",
    "optimizerD = torch.optim.Adam(netD.parameters(), lr=learning_rate, betas=(0.5, 0.999))\n",
    "optimizerG = torch.optim.Adam(netG.parameters(), lr=learning_rate, betas=(0.5, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "id": "4d9fb7de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training Loop...\n",
      "0\n",
      "[0/20][0/26]\tLoss_D: 6.4474\tLoss_G: 7.6650\tD(x): 0.1302\tD(G(z)): 0.1424 / 0.1410\n",
      "1\n",
      "[1/20][0/26]\tLoss_D: 5.1347\tLoss_G: 6.8697\tD(x): 0.4192\tD(G(z)): 0.1334 / 0.1322\n",
      "2\n",
      "[2/20][0/26]\tLoss_D: 4.5433\tLoss_G: 6.3320\tD(x): 0.6182\tD(G(z)): 0.1418 / 0.1382\n",
      "3\n",
      "[3/20][0/26]\tLoss_D: 4.4931\tLoss_G: 5.5369\tD(x): 0.6946\tD(G(z)): 0.2046 / 0.1990\n",
      "4\n",
      "[4/20][0/26]\tLoss_D: 4.2503\tLoss_G: 5.0859\tD(x): 0.7240\tD(G(z)): 0.1981 / 0.1923\n",
      "5\n",
      "[5/20][0/26]\tLoss_D: 4.2916\tLoss_G: 5.2521\tD(x): 0.7416\tD(G(z)): 0.2500 / 0.2421\n",
      "6\n",
      "[6/20][0/26]\tLoss_D: 4.1722\tLoss_G: 5.5023\tD(x): 0.7548\tD(G(z)): 0.1929 / 0.1885\n",
      "7\n",
      "[7/20][0/26]\tLoss_D: 4.0035\tLoss_G: 4.6586\tD(x): 0.7626\tD(G(z)): 0.1982 / 0.1911\n",
      "8\n",
      "[8/20][0/26]\tLoss_D: 3.8790\tLoss_G: 5.3794\tD(x): 0.7620\tD(G(z)): 0.1698 / 0.1641\n",
      "9\n",
      "[9/20][0/26]\tLoss_D: 3.7364\tLoss_G: 5.1522\tD(x): 0.7660\tD(G(z)): 0.1070 / 0.1040\n",
      "10\n",
      "[10/20][0/26]\tLoss_D: 3.7297\tLoss_G: 4.7930\tD(x): 0.7637\tD(G(z)): 0.1193 / 0.1156\n",
      "11\n",
      "[11/20][0/26]\tLoss_D: 3.5909\tLoss_G: 4.9294\tD(x): 0.7649\tD(G(z)): 0.1349 / 0.1303\n",
      "12\n",
      "[12/20][0/26]\tLoss_D: 3.4201\tLoss_G: 4.6843\tD(x): 0.7628\tD(G(z)): 0.0942 / 0.0908\n",
      "13\n",
      "[13/20][0/26]\tLoss_D: 3.3660\tLoss_G: 4.5097\tD(x): 0.7662\tD(G(z)): 0.1121 / 0.1091\n",
      "14\n",
      "[14/20][0/26]\tLoss_D: 3.2435\tLoss_G: 4.5159\tD(x): 0.7750\tD(G(z)): 0.0922 / 0.0895\n",
      "15\n",
      "[15/20][0/26]\tLoss_D: 3.1951\tLoss_G: 4.9458\tD(x): 0.7847\tD(G(z)): 0.0628 / 0.0621\n",
      "16\n",
      "[16/20][0/26]\tLoss_D: 3.0763\tLoss_G: 4.9828\tD(x): 0.7953\tD(G(z)): 0.0555 / 0.0539\n",
      "17\n",
      "[17/20][0/26]\tLoss_D: 2.9712\tLoss_G: 4.4759\tD(x): 0.8048\tD(G(z)): 0.0630 / 0.0610\n",
      "18\n",
      "[18/20][0/26]\tLoss_D: 2.8771\tLoss_G: 4.8015\tD(x): 0.8125\tD(G(z)): 0.0492 / 0.0471\n",
      "19\n",
      "[19/20][0/26]\tLoss_D: 2.8098\tLoss_G: 3.8652\tD(x): 0.8071\tD(G(z)): 0.0955 / 0.0905\n",
      "90.390625\n"
     ]
    }
   ],
   "source": [
    "img_list = []\n",
    "G_losses = []\n",
    "D_losses = []\n",
    "iters = 0\n",
    "\n",
    "start = time.process_time()\n",
    "print(\"Starting Training Loop...\")\n",
    "# For each epoch\n",
    "for epoch in range(num_epochs):\n",
    "    print(epoch)\n",
    "    # For each batch in the dataloader\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "\n",
    "        ############################\n",
    "        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "        ###########################\n",
    "        ## Train with all-real batch\n",
    "        netD.zero_grad()\n",
    "        \n",
    "        real_cpu = data[0]\n",
    "        #print(real_cpu)\n",
    "        b_size = real_cpu.size(0)\n",
    "        # Format batch\n",
    "        label = torch.full((b_size,), real_label, dtype=torch.float)\n",
    "        # Forward pass real batch through D\n",
    "        output, _ = netD(real_cpu)\n",
    "        output = output.view(-1)\n",
    "        # Calculate loss on all-real batch\n",
    "        errD_real = criterion(output, label)\n",
    "        # Calculate gradients for D in backward pass\n",
    "        errD_real.backward()\n",
    "        D_x = output.mean().item()\n",
    "\n",
    "        ## Train with all-fake batch\n",
    "        # Generate batch of latent vectors\n",
    "        #noise = torch.randn(b_size, input_dim)\n",
    "        noise = torch.cat((torch.randn(b_size, input_dim_num),torch.randint(low=0, high = 1, size = (b_size, input_dim_cat))),-1)\n",
    "        \n",
    "        # Generate fake image batch with G\n",
    "        fake = netG(noise)\n",
    "        label.fill_(fake_label)\n",
    "        # Classify all fake batch with D\n",
    "        output, _ = netD(fake.detach())\n",
    "        output = output.view(-1)\n",
    "        # Calculate D's loss on the all-fake batch\n",
    "        errD_fake = criterion(output, label)\n",
    "        #print(errD_fake)\n",
    "        # Calculate the gradients for this batch, accumulated (summed) with previous gradients\n",
    "        errD_fake.backward()\n",
    "        D_G_z1 = output.mean().item()\n",
    "        # Compute error of D as sum over the fake and the real batches\n",
    "        errD = errD_real + errD_fake\n",
    "        # Update D\n",
    "        optimizerD.step()\n",
    "\n",
    "        ############################\n",
    "        # (2) Update G network: maximize log(D(G(z)))\n",
    "        ###########################\n",
    "        netG.zero_grad()\n",
    "        label.fill_(real_label)  # fake labels are real for generator cost\n",
    "        # Since we just updated D, perform another forward pass of all-fake batch through D\n",
    "        output, _ = netD(fake)\n",
    "        output = output.view(-1)\n",
    "        \n",
    "        # Calculate G's loss based on this output\n",
    "        errG = criterion(output, label)\n",
    "        # Calculate gradients for G\n",
    "        errG.backward()\n",
    "        D_G_z2 = output.mean().item()\n",
    "        # Update G\n",
    "        optimizerG.step()\n",
    "\n",
    "        # Output training stats\n",
    "        if i % 50 == 0:\n",
    "            print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n",
    "                  % (epoch, num_epochs, i, len(train_loader),\n",
    "                     errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
    "\n",
    "        # Save Losses for plotting later\n",
    "        G_losses.append(errG.item())\n",
    "        D_losses.append(errD.item())\n",
    "\n",
    "        # Check how the generator is doing by saving G's output on fixed_noise\n",
    "        if (iters % 500 == 0) or ((epoch == num_epochs-1) and (i == len(train_loader)-1)):\n",
    "            with torch.no_grad():\n",
    "                fake = netG(fixed_noise).detach().cpu()\n",
    "            img_list.append(vutils.make_grid(fake, padding=2, normalize=True))\n",
    "\n",
    "        iters += 1\n",
    "        \n",
    "end = time.process_time()\n",
    "seismic_gan_train_time = end - start\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "id": "8ec08abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test['y'] = y_test\n",
    "myDs=MyTestDataset(x_test)\n",
    "test_loader=torch.utils.data.DataLoader(myDs,batch_size=1,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "id": "bc0bddc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = netG\n",
    "discriminator = netD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "id": "e991dc7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.734375\n"
     ]
    }
   ],
   "source": [
    "start = time.process_time()\n",
    "losses = []\n",
    "for j,(image,label) in enumerate(test_loader):\n",
    "    z = torch.cat((torch.randn(b_size, input_dim_num),torch.randint(low=0, high = 1, size = (b_size, input_dim_cat))),-1)\n",
    "    #z = init.normal_(torch.zeros(1,input_dim),mean=0,std=0.1)\n",
    "    z_optimizer = torch.optim.Adam([z],lr=1e-4)\n",
    "\n",
    "    generator.eval()\n",
    "    gen_fake = generator(z)\n",
    "    loss = Anomaly_score(image,gen_fake)\n",
    "    losses.append(loss.detach().numpy().tolist())\n",
    "    \n",
    "threshold = np.mean(losses) + np.std(losses)\n",
    "anomaly_mask = pd.Series(losses) > threshold\n",
    "preds = anomaly_mask.map(lambda x: 1.0 if x == True else 0.0)\n",
    "preds = preds.tolist()\n",
    "\n",
    "x_test['y'] = y_test\n",
    "x_test['anomaly score'] = losses\n",
    "x_test['prediction'] = preds\n",
    "\n",
    "end = time.process_time()\n",
    "seismic_gan_test_time = end - start\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "id": "4dd4297f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[682,  43],\n",
       "       [ 38,  13]], dtype=int64)"
      ]
     },
     "execution_count": 471,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(x_test['y'], x_test['prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "id": "2f22af9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7625693035835023"
      ]
     },
     "execution_count": 472,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fpr, tpr, _ = metrics.roc_curve(x_test['y'], x_test['anomaly score'])\n",
    "seismic_gan_auc = metrics.auc(fpr, tpr)\n",
    "metrics.auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "id": "eb66e34d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.94      0.94       725\n",
      "           1       0.23      0.25      0.24        51\n",
      "\n",
      "    accuracy                           0.90       776\n",
      "   macro avg       0.59      0.60      0.59       776\n",
      "weighted avg       0.90      0.90      0.90       776\n",
      "\n"
     ]
    }
   ],
   "source": [
    "seismic_gan_report = classification_report(x_test['y'], x_test['prediction'], target_names = ['0','1'], output_dict = True)\n",
    "print(classification_report(x_test['y'], x_test['prediction'], target_names = ['0','1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "id": "afffce3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.23214285714285715\n",
      "0.2549019607843137\n",
      "0.24299065420560748\n"
     ]
    }
   ],
   "source": [
    "print(seismic_gan_report['1']['precision'])\n",
    "print(seismic_gan_report['1']['recall'])\n",
    "print(seismic_gan_report['1']['f1-score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "id": "f4bfbf00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.15982246343679407\n"
     ]
    }
   ],
   "source": [
    "precision, recall, thresholds = precision_recall_curve(x_test['y'], x_test['anomaly score'])\n",
    "seismic_gan_auc_precision_recall = metrics.auc(recall, precision)\n",
    "print(seismic_gan_auc_precision_recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2405a678-b7d5-4ce9-b156-3dd2331e33b7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Musk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41fc33e",
   "metadata": {},
   "source": [
    "**Dataset source**: http://odds.cs.stonybrook.edu/musk-dataset/ (data is transformed from .mat to .csv format)\n",
    "\n",
    "Shebuti Rayana (2016). ODDS Library [http://odds.cs.stonybrook.edu]. Stony Brook, NY: Stony Brook University, Department of Computer Science.\n",
    "\n",
    "**Additional sources:**\n",
    "\n",
    "C. C. Aggarwal and S. Sathe, “Theoretical foundations and algorithms for outlier ensembles.” ACM SIGKDD Explorations Newsletter, vol. 17, no. 1, pp. 24–47, 2015."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "id": "73a9a78a-1612-4d11-8630-d3cc0cf39955",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./musk.csv', sep = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "id": "58a9917b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Col1</th>\n",
       "      <th>Col2</th>\n",
       "      <th>Col3</th>\n",
       "      <th>Col4</th>\n",
       "      <th>Col5</th>\n",
       "      <th>Col6</th>\n",
       "      <th>Col7</th>\n",
       "      <th>Col8</th>\n",
       "      <th>Col9</th>\n",
       "      <th>Col10</th>\n",
       "      <th>...</th>\n",
       "      <th>Col158</th>\n",
       "      <th>Col159</th>\n",
       "      <th>Col160</th>\n",
       "      <th>Col161</th>\n",
       "      <th>Col162</th>\n",
       "      <th>Col163</th>\n",
       "      <th>Col164</th>\n",
       "      <th>Col165</th>\n",
       "      <th>Col166</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>46.0</td>\n",
       "      <td>-108.0</td>\n",
       "      <td>-60.0</td>\n",
       "      <td>-69.0</td>\n",
       "      <td>-117.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>-161.0</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-308.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>-7.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>156.0</td>\n",
       "      <td>-50.0</td>\n",
       "      <td>-112.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>41.0</td>\n",
       "      <td>-188.0</td>\n",
       "      <td>-145.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>-117.0</td>\n",
       "      <td>-6.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>-171.0</td>\n",
       "      <td>-39.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-59.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>136.0</td>\n",
       "      <td>169.0</td>\n",
       "      <td>-61.0</td>\n",
       "      <td>-136.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>46.0</td>\n",
       "      <td>-194.0</td>\n",
       "      <td>-145.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>-117.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>-168.0</td>\n",
       "      <td>-39.0</td>\n",
       "      <td>-22.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-134.0</td>\n",
       "      <td>-154.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>143.0</td>\n",
       "      <td>142.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>-67.0</td>\n",
       "      <td>-145.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>41.0</td>\n",
       "      <td>-188.0</td>\n",
       "      <td>-145.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>-117.0</td>\n",
       "      <td>-7.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>-170.0</td>\n",
       "      <td>-39.0</td>\n",
       "      <td>-99.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-60.0</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>136.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>-60.0</td>\n",
       "      <td>-135.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41.0</td>\n",
       "      <td>-188.0</td>\n",
       "      <td>-145.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>-117.0</td>\n",
       "      <td>-7.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>-170.0</td>\n",
       "      <td>-39.0</td>\n",
       "      <td>-99.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-60.0</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>137.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>-60.0</td>\n",
       "      <td>-135.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 167 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Col1   Col2   Col3  Col4   Col5  Col6  Col7   Col8  Col9  Col10  ...  \\\n",
       "0  46.0 -108.0  -60.0 -69.0 -117.0  49.0  38.0 -161.0  -8.0    5.0  ...   \n",
       "1  41.0 -188.0 -145.0  22.0 -117.0  -6.0  57.0 -171.0 -39.0 -100.0  ...   \n",
       "2  46.0 -194.0 -145.0  28.0 -117.0  73.0  57.0 -168.0 -39.0  -22.0  ...   \n",
       "3  41.0 -188.0 -145.0  22.0 -117.0  -7.0  57.0 -170.0 -39.0  -99.0  ...   \n",
       "4  41.0 -188.0 -145.0  22.0 -117.0  -7.0  57.0 -170.0 -39.0  -99.0  ...   \n",
       "\n",
       "   Col158  Col159  Col160  Col161  Col162  Col163  Col164  Col165  Col166    y  \n",
       "0  -308.0    52.0    -7.0    39.0   126.0   156.0   -50.0  -112.0    96.0  1.0  \n",
       "1   -59.0    -2.0    52.0   103.0   136.0   169.0   -61.0  -136.0    79.0  1.0  \n",
       "2  -134.0  -154.0    57.0   143.0   142.0   165.0   -67.0  -145.0    39.0  1.0  \n",
       "3   -60.0    -4.0    52.0   104.0   136.0   168.0   -60.0  -135.0    80.0  1.0  \n",
       "4   -60.0    -4.0    52.0   104.0   137.0   168.0   -60.0  -135.0    80.0  1.0  \n",
       "\n",
       "[5 rows x 167 columns]"
      ]
     },
     "execution_count": 477,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "id": "4d289c50-09ef-40a0-acd9-5103eaf401c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['y'] = data['y'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "id": "87ef58f9-6a93-4346-8a63-4aeabf235578",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3062, 167)"
      ]
     },
     "execution_count": 479,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "id": "53716d75-2f1b-47c6-9f8a-6f79c81467bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Col1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Col1\n",
       "y      \n",
       "0  2965\n",
       "1    97"
      ]
     },
     "execution_count": 480,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.pivot_table(data,\n",
    "             values = 'Col1',\n",
    "               index = 'y', \n",
    "              aggfunc = 'count')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a555ef9",
   "metadata": {},
   "source": [
    "## AnoGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "id": "0df10a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "batch_size = 64\n",
    "learning_rate = 0.0002\n",
    "input_dim = data.shape[1]-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "id": "55dd4202",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    x_train,\n",
    "    x_test,\n",
    "    y_train,\n",
    "    y_test,\n",
    "    indices_train,\n",
    "    indices_test,\n",
    ") = train_test_split(data[data.columns[:-1]], data['y'], data.index, test_size=0.3,stratify=data[['y']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "id": "32b1a2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = pd.DataFrame(y_train)\n",
    "x_train = x_train.loc[y_train[y_train.y == 0].index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "id": "fe455eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "myDs=MyTrainDataset(x_train)\n",
    "train_loader=torch.utils.data.DataLoader(myDs,batch_size=batch_size,shuffle=False, drop_last = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "id": "37fdfc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "netG = Generator()\n",
    "netD = Discriminator()\n",
    "\n",
    "# Initialize BCELoss function\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Create batch of latent vectors that we will use to visualize\n",
    "#  the progression of the generator\n",
    "fixed_noise = torch.randn(batch_size, input_dim)\n",
    "\n",
    "# Establish convention for real and fake labels during training\n",
    "real_label = 1.\n",
    "fake_label = 0.\n",
    "\n",
    "# Setup Adam optimizers for both G and D\n",
    "optimizerD = torch.optim.Adam(netD.parameters(), lr=learning_rate, betas=(0.5, 0.999))\n",
    "optimizerG = torch.optim.Adam(netG.parameters(), lr=learning_rate, betas=(0.5, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "id": "280301bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training Loop...\n",
      "0\n",
      "[0/20][0/32]\tLoss_D: 6.3082\tLoss_G: 1.7306\tD(x): 0.6849\tD(G(z)): 0.6572 / 0.6555\n",
      "1\n",
      "[1/20][0/32]\tLoss_D: 3.9791\tLoss_G: 1.4699\tD(x): 0.6452\tD(G(z)): 0.6590 / 0.6560\n",
      "2\n",
      "[2/20][0/32]\tLoss_D: 3.5313\tLoss_G: 1.4424\tD(x): 0.6107\tD(G(z)): 0.6064 / 0.6040\n",
      "3\n",
      "[3/20][0/32]\tLoss_D: 3.2272\tLoss_G: 1.7251\tD(x): 0.5848\tD(G(z)): 0.5513 / 0.5492\n",
      "4\n",
      "[4/20][0/32]\tLoss_D: 3.0356\tLoss_G: 1.6170\tD(x): 0.5733\tD(G(z)): 0.5503 / 0.5485\n",
      "5\n",
      "[5/20][0/32]\tLoss_D: 2.7556\tLoss_G: 1.5533\tD(x): 0.5665\tD(G(z)): 0.5336 / 0.5315\n",
      "6\n",
      "[6/20][0/32]\tLoss_D: 2.7218\tLoss_G: 1.7188\tD(x): 0.5667\tD(G(z)): 0.5143 / 0.5124\n",
      "7\n",
      "[7/20][0/32]\tLoss_D: 2.6000\tLoss_G: 1.6838\tD(x): 0.5649\tD(G(z)): 0.5058 / 0.5037\n",
      "8\n",
      "[8/20][0/32]\tLoss_D: 2.2831\tLoss_G: 1.5770\tD(x): 0.5715\tD(G(z)): 0.4668 / 0.4651\n",
      "9\n",
      "[9/20][0/32]\tLoss_D: 1.9546\tLoss_G: 1.5185\tD(x): 0.5722\tD(G(z)): 0.4283 / 0.4265\n",
      "10\n",
      "[10/20][0/32]\tLoss_D: 2.0034\tLoss_G: 1.6714\tD(x): 0.5725\tD(G(z)): 0.4349 / 0.4330\n",
      "11\n",
      "[11/20][0/32]\tLoss_D: 2.0786\tLoss_G: 1.4402\tD(x): 0.5765\tD(G(z)): 0.4501 / 0.4485\n",
      "12\n",
      "[12/20][0/32]\tLoss_D: 2.2799\tLoss_G: 1.5293\tD(x): 0.5777\tD(G(z)): 0.4826 / 0.4805\n",
      "13\n",
      "[13/20][0/32]\tLoss_D: 1.8390\tLoss_G: 1.2625\tD(x): 0.5741\tD(G(z)): 0.4497 / 0.4474\n",
      "14\n",
      "[14/20][0/32]\tLoss_D: 1.9935\tLoss_G: 1.3171\tD(x): 0.5762\tD(G(z)): 0.4735 / 0.4716\n",
      "15\n",
      "[15/20][0/32]\tLoss_D: 1.9764\tLoss_G: 1.2891\tD(x): 0.5735\tD(G(z)): 0.4757 / 0.4735\n",
      "16\n",
      "[16/20][0/32]\tLoss_D: 1.9856\tLoss_G: 1.4125\tD(x): 0.5729\tD(G(z)): 0.4537 / 0.4520\n",
      "17\n",
      "[17/20][0/32]\tLoss_D: 1.9350\tLoss_G: 1.4824\tD(x): 0.5756\tD(G(z)): 0.4287 / 0.4269\n",
      "18\n",
      "[18/20][0/32]\tLoss_D: 1.7825\tLoss_G: 1.2858\tD(x): 0.5697\tD(G(z)): 0.4552 / 0.4527\n",
      "19\n",
      "[19/20][0/32]\tLoss_D: 1.8701\tLoss_G: 1.0604\tD(x): 0.5711\tD(G(z)): 0.4842 / 0.4821\n",
      "127.484375\n"
     ]
    }
   ],
   "source": [
    "img_list = []\n",
    "G_losses = []\n",
    "D_losses = []\n",
    "iters = 0\n",
    "\n",
    "start = time.process_time()\n",
    "print(\"Starting Training Loop...\")\n",
    "# For each epoch\n",
    "for epoch in range(num_epochs):\n",
    "    print(epoch)\n",
    "    # For each batch in the dataloader\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "\n",
    "        ############################\n",
    "        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "        ###########################\n",
    "        ## Train with all-real batch\n",
    "        netD.zero_grad()\n",
    "        \n",
    "        real_cpu = data[0]\n",
    "        #print(real_cpu)\n",
    "        b_size = real_cpu.size(0)\n",
    "        # Format batch\n",
    "        label = torch.full((b_size,), real_label, dtype=torch.float)\n",
    "        # Forward pass real batch through D\n",
    "        output, _ = netD(real_cpu)\n",
    "        output = output.view(-1)\n",
    "        # Calculate loss on all-real batch\n",
    "        errD_real = criterion(output, label)\n",
    "        # Calculate gradients for D in backward pass\n",
    "        errD_real.backward()\n",
    "        D_x = output.mean().item()\n",
    "\n",
    "        ## Train with all-fake batch\n",
    "        # Generate batch of latent vectors\n",
    "        noise = torch.randn(b_size, input_dim)\n",
    "        # Generate fake image batch with G\n",
    "        fake = netG(noise)\n",
    "        label.fill_(fake_label)\n",
    "        # Classify all fake batch with D\n",
    "        output, _ = netD(fake.detach())\n",
    "        output = output.view(-1)\n",
    "        # Calculate D's loss on the all-fake batch\n",
    "        errD_fake = criterion(output, label)\n",
    "        #print(errD_fake)\n",
    "        # Calculate the gradients for this batch, accumulated (summed) with previous gradients\n",
    "        errD_fake.backward()\n",
    "        D_G_z1 = output.mean().item()\n",
    "        # Compute error of D as sum over the fake and the real batches\n",
    "        errD = errD_real + errD_fake\n",
    "        # Update D\n",
    "        optimizerD.step()\n",
    "\n",
    "        ############################\n",
    "        # (2) Update G network: maximize log(D(G(z)))\n",
    "        ###########################\n",
    "        netG.zero_grad()\n",
    "        label.fill_(real_label)  # fake labels are real for generator cost\n",
    "        # Since we just updated D, perform another forward pass of all-fake batch through D\n",
    "        output, _ = netD(fake)\n",
    "        output = output.view(-1)\n",
    "        \n",
    "        # Calculate G's loss based on this output\n",
    "        errG = criterion(output, label)\n",
    "        # Calculate gradients for G\n",
    "        errG.backward()\n",
    "        D_G_z2 = output.mean().item()\n",
    "        # Update G\n",
    "        optimizerG.step()\n",
    "\n",
    "        # Output training stats\n",
    "        if i % 50 == 0:\n",
    "            print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n",
    "                  % (epoch, num_epochs, i, len(train_loader),\n",
    "                     errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
    "\n",
    "        # Save Losses for plotting later\n",
    "        G_losses.append(errG.item())\n",
    "        D_losses.append(errD.item())\n",
    "\n",
    "        # Check how the generator is doing by saving G's output on fixed_noise\n",
    "        if (iters % 500 == 0) or ((epoch == num_epochs-1) and (i == len(train_loader)-1)):\n",
    "            with torch.no_grad():\n",
    "                fake = netG(fixed_noise).detach().cpu()\n",
    "            img_list.append(vutils.make_grid(fake, padding=2, normalize=True))\n",
    "\n",
    "        iters += 1\n",
    "        \n",
    "end = time.process_time()\n",
    "musk_gan_train_time = end - start\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "id": "d51cde1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test['y'] = y_test\n",
    "myDs=MyTestDataset(x_test)\n",
    "test_loader=torch.utils.data.DataLoader(myDs,batch_size=1,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "id": "696ac303",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = netG\n",
    "discriminator = netD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "id": "abb4a1cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.515625\n"
     ]
    }
   ],
   "source": [
    "start = time.process_time()\n",
    "losses = []\n",
    "for j,(image,label) in enumerate(test_loader):\n",
    "    z = init.normal_(torch.zeros(1,input_dim),mean=0,std=0.1)\n",
    "    z_optimizer = torch.optim.Adam([z],lr=1e-4)\n",
    "\n",
    "    generator.eval()\n",
    "    gen_fake = generator(z)\n",
    "    loss = Anomaly_score(image,gen_fake)\n",
    "    losses.append(loss.detach().numpy().tolist())\n",
    "    \n",
    "threshold = np.mean(losses) + np.std(losses)\n",
    "anomaly_mask = pd.Series(losses) > threshold\n",
    "preds = anomaly_mask.map(lambda x: 1.0 if x == True else 0.0)\n",
    "preds = preds.tolist()\n",
    "\n",
    "x_test['y'] = y_test\n",
    "x_test['anomaly score'] = losses\n",
    "x_test['prediction'] = preds\n",
    "\n",
    "end = time.process_time()\n",
    "musk_gan_test_time = end - start\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "id": "3a2d0430",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[744, 146],\n",
       "       [ 20,   9]], dtype=int64)"
      ]
     },
     "execution_count": 490,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(x_test['y'], x_test['prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "id": "bfdf1515",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.803525765207284"
      ]
     },
     "execution_count": 491,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fpr, tpr, _ = metrics.roc_curve(x_test['y'], x_test['anomaly score'])\n",
    "musk_gan_auc = metrics.auc(fpr, tpr)\n",
    "metrics.auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "id": "0f1497c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.84      0.90       890\n",
      "           1       0.06      0.31      0.10        29\n",
      "\n",
      "    accuracy                           0.82       919\n",
      "   macro avg       0.52      0.57      0.50       919\n",
      "weighted avg       0.94      0.82      0.87       919\n",
      "\n"
     ]
    }
   ],
   "source": [
    "musk_gan_report = classification_report(x_test['y'], x_test['prediction'], target_names = ['0','1'], output_dict = True)\n",
    "print(classification_report(x_test['y'], x_test['prediction'], target_names = ['0','1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "id": "3b8d730a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05806451612903226\n",
      "0.3103448275862069\n",
      "0.09782608695652176\n"
     ]
    }
   ],
   "source": [
    "print(musk_gan_report['1']['precision'])\n",
    "print(musk_gan_report['1']['recall'])\n",
    "print(musk_gan_report['1']['f1-score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "id": "7aa16950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06933432232095288\n"
     ]
    }
   ],
   "source": [
    "precision, recall, thresholds = precision_recall_curve(x_test['y'], x_test['anomaly score'])\n",
    "musk_gan_auc_precision_recall = metrics.auc(recall, precision)\n",
    "print(musk_gan_auc_precision_recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2dced57-748e-4374-b5f8-85488e5baf9d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## bank"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755ae4a9",
   "metadata": {},
   "source": [
    "**Dataset source**: https://github.com/GuansongPang/ADRepository-Anomaly-detection-datasets/tree/main/categorical%20data\n",
    "\n",
    "Pang, G., Shen, C., Cao, L., & Hengel, A. V. D. (2021). Deep learning for anomaly detection: A review. ACM Computing Surveys (CSUR), 54(2), 1-38."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "id": "45331943-15fa-4768-a7b0-de9de07366ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./bank.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "id": "3d466cef-6b84-4710-bf4b-4f83fcfa54d7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>job=housemaid</th>\n",
       "      <th>job=services</th>\n",
       "      <th>job=admin.</th>\n",
       "      <th>job=blue-collar</th>\n",
       "      <th>job=technician</th>\n",
       "      <th>job=retired</th>\n",
       "      <th>job=management</th>\n",
       "      <th>job=unemployed</th>\n",
       "      <th>job=self-employed</th>\n",
       "      <th>...</th>\n",
       "      <th>previous</th>\n",
       "      <th>poutcome=nonexistent</th>\n",
       "      <th>poutcome=failure</th>\n",
       "      <th>poutcome=success</th>\n",
       "      <th>emp.var.rate</th>\n",
       "      <th>cons.price.idx</th>\n",
       "      <th>cons.conf.idx</th>\n",
       "      <th>euribor3m</th>\n",
       "      <th>nr.employed</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.209877</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.882307</td>\n",
       "      <td>0.376569</td>\n",
       "      <td>0.980730</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.296296</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.484412</td>\n",
       "      <td>0.615063</td>\n",
       "      <td>0.981183</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.246914</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.698753</td>\n",
       "      <td>0.602510</td>\n",
       "      <td>0.957379</td>\n",
       "      <td>0.859735</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.160494</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.269680</td>\n",
       "      <td>0.192469</td>\n",
       "      <td>0.150759</td>\n",
       "      <td>0.512287</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.530864</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.340608</td>\n",
       "      <td>0.154812</td>\n",
       "      <td>0.174790</td>\n",
       "      <td>0.512287</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 63 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        age  job=housemaid  job=services  job=admin.  job=blue-collar  \\\n",
       "0  0.209877              0             0           0                0   \n",
       "1  0.296296              0             0           1                0   \n",
       "2  0.246914              1             0           0                0   \n",
       "3  0.160494              0             1           0                0   \n",
       "4  0.530864              0             0           0                1   \n",
       "\n",
       "   job=technician  job=retired  job=management  job=unemployed  \\\n",
       "0               0            0               0               0   \n",
       "1               0            0               0               0   \n",
       "2               0            0               0               0   \n",
       "3               0            0               0               0   \n",
       "4               0            0               0               0   \n",
       "\n",
       "   job=self-employed  ...  previous  poutcome=nonexistent  poutcome=failure  \\\n",
       "0                  0  ...  0.000000                     1                 0   \n",
       "1                  0  ...  0.000000                     1                 0   \n",
       "2                  0  ...  0.000000                     1                 0   \n",
       "3                  0  ...  0.142857                     0                 1   \n",
       "4                  0  ...  0.000000                     1                 0   \n",
       "\n",
       "   poutcome=success  emp.var.rate  cons.price.idx  cons.conf.idx  euribor3m  \\\n",
       "0                 0      1.000000        0.882307       0.376569   0.980730   \n",
       "1                 0      1.000000        0.484412       0.615063   0.981183   \n",
       "2                 0      0.937500        0.698753       0.602510   0.957379   \n",
       "3                 0      0.333333        0.269680       0.192469   0.150759   \n",
       "4                 0      0.333333        0.340608       0.154812   0.174790   \n",
       "\n",
       "   nr.employed  class  \n",
       "0     1.000000      0  \n",
       "1     1.000000      0  \n",
       "2     0.859735      0  \n",
       "3     0.512287      0  \n",
       "4     0.512287      1  \n",
       "\n",
       "[5 rows x 63 columns]"
      ]
     },
     "execution_count": 496,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "id": "199d3f42-c83b-405f-ad36-39998fd67eef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>36548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4640</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         age\n",
       "class       \n",
       "0      36548\n",
       "1       4640"
      ]
     },
     "execution_count": 497,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.pivot_table(data,\n",
    "             values = 'age',\n",
    "               index = 'class', \n",
    "              aggfunc = 'count')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe86c2b",
   "metadata": {},
   "source": [
    "## AnoGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "id": "ddecceaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "batch_size = 512\n",
    "learning_rate = 0.0002\n",
    "input_dim = data.shape[1]-1\n",
    "\n",
    "input_dim_cat = 52\n",
    "input_dim_num = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "id": "3227a148",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    x_train,\n",
    "    x_test,\n",
    "    y_train,\n",
    "    y_test,\n",
    "    indices_train,\n",
    "    indices_test,\n",
    ") = train_test_split(data[data.columns[:-1]], data['class'], data.index, test_size=0.3,stratify=data[['class']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "id": "51439c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = pd.DataFrame(y_train)\n",
    "x_train = x_train.loc[y_train[y_train['class'] == 0].index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "id": "f7950284",
   "metadata": {},
   "outputs": [],
   "source": [
    "myDs=MyTrainDataset(x_train)\n",
    "train_loader=torch.utils.data.DataLoader(myDs,batch_size=batch_size,shuffle=False, drop_last = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "id": "c35cc1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "netG = Generator()\n",
    "netD = Discriminator()\n",
    "\n",
    "# Initialize BCELoss function\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Create batch of latent vectors that we will use to visualize\n",
    "#  the progression of the generator\n",
    "fixed_noise = torch.randn(batch_size, input_dim)\n",
    "\n",
    "# Establish convention for real and fake labels during training\n",
    "real_label = 1.\n",
    "fake_label = 0.\n",
    "\n",
    "# Setup Adam optimizers for both G and D\n",
    "optimizerD = torch.optim.Adam(netD.parameters(), lr=learning_rate, betas=(0.5, 0.999))\n",
    "optimizerG = torch.optim.Adam(netG.parameters(), lr=learning_rate, betas=(0.5, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "id": "d96ef11e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training Loop...\n",
      "0\n",
      "[0/20][0/49]\tLoss_D: 9.0326\tLoss_G: 0.6769\tD(x): 0.7988\tD(G(z)): 0.7860 / 0.7850\n",
      "1\n",
      "[1/20][0/49]\tLoss_D: 4.7214\tLoss_G: 0.1594\tD(x): 0.7750\tD(G(z)): 0.8992 / 0.8973\n",
      "2\n",
      "[2/20][0/49]\tLoss_D: 4.0397\tLoss_G: 0.1473\tD(x): 0.7067\tD(G(z)): 0.8900 / 0.8876\n",
      "3\n",
      "[3/20][0/49]\tLoss_D: 3.7829\tLoss_G: 0.1846\tD(x): 0.6181\tD(G(z)): 0.8613 / 0.8576\n",
      "4\n",
      "[4/20][0/49]\tLoss_D: 3.6480\tLoss_G: 0.2172\tD(x): 0.5285\tD(G(z)): 0.8340 / 0.8301\n",
      "5\n",
      "[5/20][0/49]\tLoss_D: 3.6018\tLoss_G: 0.2786\tD(x): 0.4572\tD(G(z)): 0.7975 / 0.7933\n",
      "6\n",
      "[6/20][0/49]\tLoss_D: 3.6678\tLoss_G: 0.2961\tD(x): 0.4097\tD(G(z)): 0.7861 / 0.7807\n",
      "7\n",
      "[7/20][0/49]\tLoss_D: 3.7405\tLoss_G: 0.2979\tD(x): 0.3747\tD(G(z)): 0.7775 / 0.7721\n",
      "8\n",
      "[8/20][0/49]\tLoss_D: 3.7918\tLoss_G: 0.3183\tD(x): 0.3515\tD(G(z)): 0.7667 / 0.7606\n",
      "9\n",
      "[9/20][0/49]\tLoss_D: 3.8693\tLoss_G: 0.2987\tD(x): 0.3344\tD(G(z)): 0.7777 / 0.7711\n",
      "10\n",
      "[10/20][0/49]\tLoss_D: 3.7292\tLoss_G: 0.3631\tD(x): 0.3255\tD(G(z)): 0.7409 / 0.7337\n",
      "11\n",
      "[11/20][0/49]\tLoss_D: 3.7841\tLoss_G: 0.3082\tD(x): 0.3224\tD(G(z)): 0.7707 / 0.7613\n",
      "12\n",
      "[12/20][0/49]\tLoss_D: 3.6872\tLoss_G: 0.3680\tD(x): 0.3173\tD(G(z)): 0.7447 / 0.7374\n",
      "13\n",
      "[13/20][0/49]\tLoss_D: 3.5009\tLoss_G: 0.3841\tD(x): 0.3222\tD(G(z)): 0.7212 / 0.7115\n",
      "14\n",
      "[14/20][0/49]\tLoss_D: 3.7593\tLoss_G: 0.2965\tD(x): 0.3203\tD(G(z)): 0.7754 / 0.7638\n",
      "15\n",
      "[15/20][0/49]\tLoss_D: 3.5936\tLoss_G: 0.3639\tD(x): 0.3187\tD(G(z)): 0.7365 / 0.7250\n",
      "16\n",
      "[16/20][0/49]\tLoss_D: 3.7043\tLoss_G: 0.3712\tD(x): 0.3119\tD(G(z)): 0.7364 / 0.7306\n",
      "17\n",
      "[17/20][0/49]\tLoss_D: 3.6977\tLoss_G: 0.3155\tD(x): 0.3138\tD(G(z)): 0.7620 / 0.7531\n",
      "18\n",
      "[18/20][0/49]\tLoss_D: 3.4224\tLoss_G: 0.4246\tD(x): 0.3194\tD(G(z)): 0.6934 / 0.6844\n",
      "19\n",
      "[19/20][0/49]\tLoss_D: 3.2545\tLoss_G: 0.4941\tD(x): 0.3244\tD(G(z)): 0.6644 / 0.6557\n",
      "428.640625\n"
     ]
    }
   ],
   "source": [
    "img_list = []\n",
    "G_losses = []\n",
    "D_losses = []\n",
    "iters = 0\n",
    "\n",
    "start = time.process_time()\n",
    "print(\"Starting Training Loop...\")\n",
    "# For each epoch\n",
    "for epoch in range(num_epochs):\n",
    "    print(epoch)\n",
    "    # For each batch in the dataloader\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "\n",
    "        ############################\n",
    "        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "        ###########################\n",
    "        ## Train with all-real batch\n",
    "        netD.zero_grad()\n",
    "        \n",
    "        real_cpu = data[0]\n",
    "        #print(real_cpu)\n",
    "        b_size = real_cpu.size(0)\n",
    "        # Format batch\n",
    "        label = torch.full((b_size,), real_label, dtype=torch.float)\n",
    "        # Forward pass real batch through D\n",
    "        output, _ = netD(real_cpu)\n",
    "        output = output.view(-1)\n",
    "        # Calculate loss on all-real batch\n",
    "        errD_real = criterion(output, label)\n",
    "        # Calculate gradients for D in backward pass\n",
    "        errD_real.backward()\n",
    "        D_x = output.mean().item()\n",
    "\n",
    "        ## Train with all-fake batch\n",
    "        # Generate batch of latent vectors\n",
    "        #noise = torch.randn(b_size, input_dim)\n",
    "        noise = torch.cat((torch.randn(b_size, input_dim_num),torch.randint(low=0, high = 1, size = (b_size, input_dim_cat))),-1)\n",
    "        \n",
    "        # Generate fake image batch with G\n",
    "        fake = netG(noise)\n",
    "        label.fill_(fake_label)\n",
    "        # Classify all fake batch with D\n",
    "        output, _ = netD(fake.detach())\n",
    "        output = output.view(-1)\n",
    "        # Calculate D's loss on the all-fake batch\n",
    "        errD_fake = criterion(output, label)\n",
    "        #print(errD_fake)\n",
    "        # Calculate the gradients for this batch, accumulated (summed) with previous gradients\n",
    "        errD_fake.backward()\n",
    "        D_G_z1 = output.mean().item()\n",
    "        # Compute error of D as sum over the fake and the real batches\n",
    "        errD = errD_real + errD_fake\n",
    "        # Update D\n",
    "        optimizerD.step()\n",
    "\n",
    "        ############################\n",
    "        # (2) Update G network: maximize log(D(G(z)))\n",
    "        ###########################\n",
    "        netG.zero_grad()\n",
    "        label.fill_(real_label)  # fake labels are real for generator cost\n",
    "        # Since we just updated D, perform another forward pass of all-fake batch through D\n",
    "        output, _ = netD(fake)\n",
    "        output = output.view(-1)\n",
    "        \n",
    "        # Calculate G's loss based on this output\n",
    "        errG = criterion(output, label)\n",
    "        # Calculate gradients for G\n",
    "        errG.backward()\n",
    "        D_G_z2 = output.mean().item()\n",
    "        # Update G\n",
    "        optimizerG.step()\n",
    "\n",
    "        # Output training stats\n",
    "        if i % 50 == 0:\n",
    "            print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n",
    "                  % (epoch, num_epochs, i, len(train_loader),\n",
    "                     errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
    "\n",
    "        # Save Losses for plotting later\n",
    "        G_losses.append(errG.item())\n",
    "        D_losses.append(errD.item())\n",
    "\n",
    "        # Check how the generator is doing by saving G's output on fixed_noise\n",
    "        if (iters % 500 == 0) or ((epoch == num_epochs-1) and (i == len(train_loader)-1)):\n",
    "            with torch.no_grad():\n",
    "                fake = netG(fixed_noise).detach().cpu()\n",
    "            img_list.append(vutils.make_grid(fake, padding=2, normalize=True))\n",
    "\n",
    "        iters += 1\n",
    "        \n",
    "end = time.process_time()\n",
    "bank_gan_train_time = end - start\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "id": "f2946420",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test['y'] = y_test\n",
    "myDs=MyTestDataset(x_test)\n",
    "test_loader=torch.utils.data.DataLoader(myDs,batch_size=1,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "id": "1dfe8f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = netG\n",
    "discriminator = netD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "id": "07d3147a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1221.09375\n"
     ]
    }
   ],
   "source": [
    "start = time.process_time()\n",
    "losses = []\n",
    "for j,(image,label) in enumerate(test_loader):\n",
    "    z = torch.cat((torch.randn(b_size, input_dim_num),torch.randint(low=0, high = 1, size = (b_size, input_dim_cat))),-1)\n",
    "    #z = init.normal_(torch.zeros(1,input_dim),mean=0,std=0.1)\n",
    "    z_optimizer = torch.optim.Adam([z],lr=1e-4)\n",
    "\n",
    "    generator.eval()\n",
    "    gen_fake = generator(z)\n",
    "    loss = Anomaly_score(image,gen_fake)\n",
    "    losses.append(loss.detach().numpy().tolist())\n",
    "    \n",
    "threshold = np.mean(losses) + np.std(losses)\n",
    "anomaly_mask = pd.Series(losses) > threshold\n",
    "preds = anomaly_mask.map(lambda x: 1.0 if x == True else 0.0)\n",
    "preds = preds.tolist()\n",
    "\n",
    "x_test['y'] = y_test\n",
    "x_test['anomaly score'] = losses\n",
    "x_test['prediction'] = preds\n",
    "\n",
    "end = time.process_time()\n",
    "bank_gan_test_time = end - start\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "id": "e6654c96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[8999, 1966],\n",
       "       [1276,  116]], dtype=int64)"
      ]
     },
     "execution_count": 507,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(x_test['y'], x_test['prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "id": "9dfcea82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.34309283456766826"
      ]
     },
     "execution_count": 508,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fpr, tpr, _ = metrics.roc_curve(x_test['y'], x_test['anomaly score'])\n",
    "bank_gan_auc = metrics.auc(fpr, tpr)\n",
    "metrics.auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "id": "7bf7bda9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.82      0.85     10965\n",
      "           1       0.06      0.08      0.07      1392\n",
      "\n",
      "    accuracy                           0.74     12357\n",
      "   macro avg       0.47      0.45      0.46     12357\n",
      "weighted avg       0.78      0.74      0.76     12357\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bank_gan_report = classification_report(x_test['y'], x_test['prediction'], target_names = ['0','1'], output_dict = True)\n",
    "print(classification_report(x_test['y'], x_test['prediction'], target_names = ['0','1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "id": "f24699dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05571565802113353\n",
      "0.08333333333333333\n",
      "0.06678180771445019\n"
     ]
    }
   ],
   "source": [
    "print(bank_gan_report['1']['precision'])\n",
    "print(bank_gan_report['1']['recall'])\n",
    "print(bank_gan_report['1']['f1-score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "id": "e10a5b3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08204586983883812\n"
     ]
    }
   ],
   "source": [
    "precision, recall, thresholds = precision_recall_curve(x_test['y'], x_test['anomaly score'])\n",
    "bank_gan_auc_precision_recall = metrics.auc(recall, precision)\n",
    "print(bank_gan_auc_precision_recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e924f118",
   "metadata": {},
   "source": [
    "## Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "id": "26731f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "performance = pd.DataFrame(columns = ['F1 score', 'recall', 'precision', 'AUC', 'AUPRC', \n",
    "                                      'Training time','Inference time','Total time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "id": "e98f99e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score_gan = {'arrhythmia':arrhythmia_gan_report['1']['f1-score'],\n",
    "                       'cardio':cardio_gan_report['1']['f1-score'], \n",
    "                        'forestcover':forestcover_gan_report['1']['f1-score'], \n",
    "                       'annthyroid':annthyroid_gan_report['1']['f1-score'],       \n",
    "                        'creditcard':creditcard_gan_report['1']['f1-score'], \n",
    "                       'mammography':mammography_gan_report['1']['f1-score'], \n",
    "                        'shuttle':shuttle_gan_report['1']['f1-score'], \n",
    "                      'mnist':mnist_gan_report['1']['f1-score'], \n",
    "                  'vowels':vowels_gan_report['1']['f1-score'], \n",
    "                  'seismic':seismic_gan_report['1']['f1-score'], \n",
    "                  'musk':musk_gan_report['1']['f1-score'], \n",
    "                  'bank':bank_gan_report['1']['f1-score']}\n",
    "f1_score_gan_df = pd.DataFrame.from_dict(f1_score_gan, orient='index', columns = ['F1 score']).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "id": "76cf7944",
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_gan = {'arrhythmia':arrhythmia_gan_report['1']['recall'],\n",
    "                       'cardio':cardio_gan_report['1']['recall'], \n",
    "                        'forestcover':forestcover_gan_report['1']['recall'], \n",
    "                       'annthyroid':annthyroid_gan_report['1']['recall'],       \n",
    "                        'creditcard':creditcard_gan_report['1']['recall'], \n",
    "                       'mammography':mammography_gan_report['1']['recall'], \n",
    "                        'shuttle':shuttle_gan_report['1']['recall'], \n",
    "                      'mnist':mnist_gan_report['1']['recall'], \n",
    "                  'vowels':vowels_gan_report['1']['recall'], \n",
    "                  'seismic':seismic_gan_report['1']['recall'], \n",
    "                  'musk':musk_gan_report['1']['recall'], \n",
    "                  'bank':bank_gan_report['1']['recall'], }\n",
    "recall_gan_df = pd.DataFrame.from_dict(recall_gan, orient='index', columns = ['Recall']).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "id": "c7852f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_gan = {'arrhythmia':arrhythmia_gan_report['1']['precision'],\n",
    "                       'cardio':cardio_gan_report['1']['precision'], \n",
    "                        'forestcover':forestcover_gan_report['1']['precision'], \n",
    "                       'annthyroid':annthyroid_gan_report['1']['precision'],       \n",
    "                        'creditcard':creditcard_gan_report['1']['precision'], \n",
    "                       'mammography':mammography_gan_report['1']['precision'], \n",
    "                        'shuttle':shuttle_gan_report['1']['precision'], \n",
    "                      'mnist':mnist_gan_report['1']['precision'], \n",
    "                  'vowels':vowels_gan_report['1']['precision'], \n",
    "                  'seismic':seismic_gan_report['1']['precision'], \n",
    "                  'musk':musk_gan_report['1']['precision'], \n",
    "                  'bank':bank_gan_report['1']['precision'], }\n",
    "precision_gan_df = pd.DataFrame.from_dict(precision_gan, orient='index', columns = ['Precision']).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "id": "52fe0f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_gan = {'arrhythmia':arrhythmia_gan_auc,\n",
    "                       'cardio':cardio_gan_auc, \n",
    "                        'forestcover':forestcover_gan_auc, \n",
    "                       'annthyroid':annthyroid_gan_auc,       \n",
    "                        'creditcard':creditcard_gan_auc, \n",
    "                       'mammography':mammography_gan_auc, \n",
    "                        'shuttle':shuttle_gan_auc, \n",
    "                      'mnist':mnist_gan_auc, \n",
    "                  'vowels':vowels_gan_auc, \n",
    "                  'seismic':seismic_gan_auc, \n",
    "                  'musk':musk_gan_auc, \n",
    "                  'bank':bank_gan_auc}\n",
    "auc_gan_df = pd.DataFrame.from_dict(auc_gan, orient='index', columns = ['AUC']).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "id": "ce2218fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "auprc_gan = {'arrhythmia':arrhythmia_gan_auc_precision_recall,\n",
    "                       'cardio':cardio_gan_auc_precision_recall, \n",
    "                        'forestcover':forestcover_gan_auc_precision_recall, \n",
    "                       'annthyroid':annthyroid_gan_auc_precision_recall,       \n",
    "                        'creditcard':creditcard_gan_auc_precision_recall, \n",
    "                       'mammography':mammography_gan_auc_precision_recall, \n",
    "                        'shuttle':shuttle_gan_auc_precision_recall, \n",
    "                      'mnist':mnist_gan_auc_precision_recall, \n",
    "                  'vowels':vowels_gan_auc_precision_recall, \n",
    "                  'seismic':seismic_gan_auc_precision_recall, \n",
    "                  'musk':musk_gan_auc_precision_recall, \n",
    "                  'bank':bank_gan_auc_precision_recall}\n",
    "auprc_gan_df = pd.DataFrame.from_dict(auprc_gan, orient='index', columns = ['AUPRC']).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "id": "cded9e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_time_gan = {'arrhythmia':arrhythmia_gan_train_time,\n",
    "                       'cardio':cardio_gan_train_time, \n",
    "                        'forestcover':forestcover_gan_train_time, \n",
    "                       'annthyroid':annthyroid_gan_train_time,       \n",
    "                        'creditcard': creditcard_gan_train_time, \n",
    "                       'mammography':mammography_gan_train_time, \n",
    "                        'shuttle':shuttle_gan_train_time, \n",
    "                      'mnist':mnist_gan_train_time, \n",
    "                  'vowels':vowels_gan_train_time, \n",
    "                  'seismic':seismic_gan_train_time, \n",
    "                  'musk':musk_gan_train_time, \n",
    "                  'bank':bank_gan_train_time}\n",
    "training_time_gan_df = pd.DataFrame.from_dict(training_time_gan, orient='index', columns = ['Training time']).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "id": "f12dd862",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_time_gan = {'arrhythmia':arrhythmia_gan_test_time,\n",
    "                       'cardio':cardio_gan_test_time, \n",
    "                        'forestcover':forestcover_gan_test_time, \n",
    "                       'annthyroid':annthyroid_gan_test_time,       \n",
    "                        'creditcard':creditcard_gan_test_time, \n",
    "                       'mammography':mammography_gan_test_time, \n",
    "                        'shuttle':shuttle_gan_test_time, \n",
    "                      'mnist':mnist_gan_test_time, \n",
    "                  'vowels':vowels_gan_test_time, \n",
    "                  'seismic':seismic_gan_test_time, \n",
    "                  'musk':musk_gan_test_time, \n",
    "                  'bank':bank_gan_test_time}\n",
    "test_time_gan_df = pd.DataFrame.from_dict(test_time_gan, orient='index', columns = ['Testing time']).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "id": "1786d7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_time_gan = {'arrhythmia':arrhythmia_gan_train_time + arrhythmia_gan_test_time,\n",
    "                       'cardio':cardio_gan_train_time + cardio_gan_test_time, \n",
    "                        'forestcover':forestcover_gan_train_time + forestcover_gan_test_time, \n",
    "                       'annthyroid':annthyroid_gan_train_time + annthyroid_gan_test_time,       \n",
    "                        'creditcard': creditcard_gan_train_time + creditcard_gan_test_time, \n",
    "                       'mammography':mammography_gan_train_time + mammography_gan_test_time, \n",
    "                        'shuttle':shuttle_gan_train_time + shuttle_gan_test_time, \n",
    "                      'mnist':mnist_gan_train_time + mnist_gan_test_time, \n",
    "                  'vowels':vowels_gan_train_time + vowels_gan_test_time, \n",
    "                  'seismic':seismic_gan_train_time + seismic_gan_test_time, \n",
    "                  'musk':musk_gan_train_time + musk_gan_test_time, \n",
    "                  'bank':bank_gan_train_time + bank_gan_test_time}\n",
    "total_time_gan_df = pd.DataFrame.from_dict(total_time_gan, orient='index', columns = ['Total time']).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "id": "2a201ca5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>F1 score</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>AUC</th>\n",
       "      <th>AUPRC</th>\n",
       "      <th>Training time</th>\n",
       "      <th>Testing time</th>\n",
       "      <th>Total time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>arrhythmia</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.593103</td>\n",
       "      <td>0.322384</td>\n",
       "      <td>18.500000</td>\n",
       "      <td>2.484375</td>\n",
       "      <td>20.984375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cardio</td>\n",
       "      <td>0.619048</td>\n",
       "      <td>0.735849</td>\n",
       "      <td>0.534247</td>\n",
       "      <td>0.953229</td>\n",
       "      <td>0.682443</td>\n",
       "      <td>70.203125</td>\n",
       "      <td>10.593750</td>\n",
       "      <td>80.796875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>forestcover</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042668</td>\n",
       "      <td>0.004925</td>\n",
       "      <td>3768.421875</td>\n",
       "      <td>1458.296875</td>\n",
       "      <td>5226.718750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>annthyroid</td>\n",
       "      <td>0.128472</td>\n",
       "      <td>0.231250</td>\n",
       "      <td>0.088942</td>\n",
       "      <td>0.519306</td>\n",
       "      <td>0.083128</td>\n",
       "      <td>95.546875</td>\n",
       "      <td>42.453125</td>\n",
       "      <td>138.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>creditcard</td>\n",
       "      <td>0.008955</td>\n",
       "      <td>0.141892</td>\n",
       "      <td>0.004624</td>\n",
       "      <td>0.776905</td>\n",
       "      <td>0.004687</td>\n",
       "      <td>3683.500000</td>\n",
       "      <td>1392.734375</td>\n",
       "      <td>5076.234375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>mammography</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.133758</td>\n",
       "      <td>0.861314</td>\n",
       "      <td>0.142467</td>\n",
       "      <td>134.578125</td>\n",
       "      <td>54.937500</td>\n",
       "      <td>189.515625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>shuttle</td>\n",
       "      <td>0.041441</td>\n",
       "      <td>0.021842</td>\n",
       "      <td>0.403509</td>\n",
       "      <td>0.989596</td>\n",
       "      <td>0.833059</td>\n",
       "      <td>560.968750</td>\n",
       "      <td>223.640625</td>\n",
       "      <td>784.609375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>mnist</td>\n",
       "      <td>0.037351</td>\n",
       "      <td>0.052381</td>\n",
       "      <td>0.029024</td>\n",
       "      <td>0.247408</td>\n",
       "      <td>0.056655</td>\n",
       "      <td>75.578125</td>\n",
       "      <td>40.890625</td>\n",
       "      <td>116.468750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>vowels</td>\n",
       "      <td>0.067416</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.040541</td>\n",
       "      <td>0.574408</td>\n",
       "      <td>0.051414</td>\n",
       "      <td>9.015625</td>\n",
       "      <td>7.312500</td>\n",
       "      <td>16.328125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>seismic</td>\n",
       "      <td>0.242991</td>\n",
       "      <td>0.254902</td>\n",
       "      <td>0.232143</td>\n",
       "      <td>0.762569</td>\n",
       "      <td>0.159822</td>\n",
       "      <td>90.390625</td>\n",
       "      <td>24.734375</td>\n",
       "      <td>115.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>musk</td>\n",
       "      <td>0.097826</td>\n",
       "      <td>0.310345</td>\n",
       "      <td>0.058065</td>\n",
       "      <td>0.803526</td>\n",
       "      <td>0.069334</td>\n",
       "      <td>127.484375</td>\n",
       "      <td>12.515625</td>\n",
       "      <td>140.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>bank</td>\n",
       "      <td>0.066782</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.055716</td>\n",
       "      <td>0.343093</td>\n",
       "      <td>0.082046</td>\n",
       "      <td>428.640625</td>\n",
       "      <td>1221.093750</td>\n",
       "      <td>1649.734375</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          index  F1 score    Recall  Precision       AUC     AUPRC  \\\n",
       "0    arrhythmia  0.235294  0.200000   0.285714  0.593103  0.322384   \n",
       "1        cardio  0.619048  0.735849   0.534247  0.953229  0.682443   \n",
       "2   forestcover  0.000000  0.000000   0.000000  0.042668  0.004925   \n",
       "3    annthyroid  0.128472  0.231250   0.088942  0.519306  0.083128   \n",
       "4    creditcard  0.008955  0.141892   0.004624  0.776905  0.004687   \n",
       "5   mammography  0.214286  0.538462   0.133758  0.861314  0.142467   \n",
       "6       shuttle  0.041441  0.021842   0.403509  0.989596  0.833059   \n",
       "7         mnist  0.037351  0.052381   0.029024  0.247408  0.056655   \n",
       "8        vowels  0.067416  0.200000   0.040541  0.574408  0.051414   \n",
       "9       seismic  0.242991  0.254902   0.232143  0.762569  0.159822   \n",
       "10         musk  0.097826  0.310345   0.058065  0.803526  0.069334   \n",
       "11         bank  0.066782  0.083333   0.055716  0.343093  0.082046   \n",
       "\n",
       "    Training time  Testing time   Total time  \n",
       "0       18.500000      2.484375    20.984375  \n",
       "1       70.203125     10.593750    80.796875  \n",
       "2     3768.421875   1458.296875  5226.718750  \n",
       "3       95.546875     42.453125   138.000000  \n",
       "4     3683.500000   1392.734375  5076.234375  \n",
       "5      134.578125     54.937500   189.515625  \n",
       "6      560.968750    223.640625   784.609375  \n",
       "7       75.578125     40.890625   116.468750  \n",
       "8        9.015625      7.312500    16.328125  \n",
       "9       90.390625     24.734375   115.125000  \n",
       "10     127.484375     12.515625   140.000000  \n",
       "11     428.640625   1221.093750  1649.734375  "
      ]
     },
     "execution_count": 521,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.merge(pd.merge(pd.merge(pd.merge(pd.merge(pd.merge(pd.merge(f1_score_gan_df, recall_gan_df, how = 'inner'), \n",
    "                                    precision_gan_df, how ='inner'),\n",
    "         auc_gan_df, how = 'inner'), auprc_gan_df, how = 'inner'), training_time_gan_df, how = 'inner'), \n",
    "         test_time_gan_df, how = 'inner'),total_time_gan_df, how = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7b59d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cpu.m80",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m80"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "c7bb90c4ec68b2a8968b0075ab0b1cb7a78770acf7a7acf2e36e903fa05bac64"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
